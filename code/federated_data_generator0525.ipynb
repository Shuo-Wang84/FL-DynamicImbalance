{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exp1实验设置: 10 clients,100 samples per round(space_keep_all)\n",
    "# Balanced : 60(A):60(B),B随机取60个，A1-A9(A1取20，A2-A9各个取5个)\n",
    "# Imbalanced : 108(A):12(B),B随机取108个，A1-A9(A1取4，A2-A9各个取1个)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义数据文件夹路径\n",
    "input_dir = 'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/source'\n",
    "output_dir = 'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/Federated_600/direction_step10/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 读取数据函数\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path).values\n",
    "\n",
    "# 定义文件名和标签\n",
    "labels = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'B']\n",
    "label_to_class = {label: i for i, label in enumerate(labels[:-1])}\n",
    "\n",
    "# 读取所有类别的数据\n",
    "data = {label: load_data(os.path.join(input_dir, f'{label}.csv')) for label in labels}\n",
    "\n",
    "# 抽样并保存函数\n",
    "def sample_and_save(client_id,round_idx, data, a1_samples, a2_a9_samples, b_samples):\n",
    "    sampled_data = []\n",
    "\n",
    "    # 从 A1 中抽取样本\n",
    "    a1_sampled = data['A1'][:a1_samples]\n",
    "    sampled_data.append(np.column_stack((a1_sampled, np.full(len(a1_sampled), 'A1'), np.zeros(len(a1_sampled)))))\n",
    "    data['A1'] = data['A1'][a1_samples:]  # 更新 A1 剩余数据\n",
    "\n",
    "    # 从 A2-A9 中每个抽取样本\n",
    "    for label in labels[1:-1]:\n",
    "        samples = data[label][:a2_a9_samples]\n",
    "        sampled_data.append(np.column_stack((samples, np.full(len(samples), label), np.zeros(len(samples)))))\n",
    "        data[label] = data[label][a2_a9_samples:]  # 更新剩余数据\n",
    "\n",
    "    # 从 B 中抽取样本\n",
    "    b_sampled = data['B'][:b_samples]\n",
    "    sampled_data.append(np.column_stack((b_sampled, np.full(len(b_sampled), 'B'), np.ones(len(b_sampled)))))\n",
    "    data['B'] = data['B'][b_samples:]  # 更新 B 剩余数据\n",
    "\n",
    "    # 组合所有样本并保存为 CSV\n",
    "    round_data = np.vstack(sampled_data)\n",
    "    round_df = pd.DataFrame(round_data, columns=['feature1', 'feature2', 'label', 'class'])\n",
    "    output_dir1 = os.path.join(output_dir,f'client_{client_id}')\n",
    "    os.makedirs(output_dir1, exist_ok=True)\n",
    "    round_df.to_csv(os.path.join(output_dir1, f'round_{round_idx}.csv'), index=False)\n",
    "\n",
    "start_index = 0\n",
    "client_nums = 10\n",
    "# 1230000 360*5*100=180000\n",
    "for r in range(10):\n",
    "    if r % 2 == 0:\n",
    "        for round_idx in range(start_index,start_index+10):\n",
    "            # print('a',round_idx)\n",
    "            # sample_and_save(round_idx, data, a1_samples=4, a2_a9_samples=1, b_samples=108)\n",
    "            for client_id in range(0,int(client_nums/2)):\n",
    "                sample_and_save(client_id=client_id,round_idx=round_idx, data=data, a1_samples=40, a2_a9_samples=10, b_samples=480)\n",
    "            for client_id in range(int(client_nums/2),client_nums):\n",
    "                sample_and_save(client_id=client_id,round_idx=round_idx, data=data, a1_samples=160, a2_a9_samples=40, b_samples=120)\n",
    "        start_index += 10\n",
    "    elif r % 2 == 1:\n",
    "        for round_idx in range(start_index,start_index+10):\n",
    "            for client_id in range(0,int(client_nums/2)):\n",
    "                sample_and_save(client_id=client_id,round_idx=round_idx, data=data, a1_samples=160, a2_a9_samples=40, b_samples=120)\n",
    "            for client_id in range(int(client_nums/2),client_nums):\n",
    "                sample_and_save(client_id=client_id,round_idx=round_idx, data=data, a1_samples=40, a2_a9_samples=10, b_samples=480)\n",
    "            # print('b',round_idx)\n",
    "            # sample_and_save(round_idx, data, a1_samples=36, a2_a9_samples=9, b_samples=12)\n",
    "        start_index += 10\n",
    "\n",
    "print(f'Data sampling and saving completed. Files are saved in {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exp1实验设置: 10 clients,100 samples per round(space_keep_all)\n",
    "# Balanced : 60(A):60(B),B随机取60个，A1-A9(A1取20，A2-A9各个取5个)\n",
    "# Imbalanced : 108(A):12(B),B随机取108个，A1-A9(A1取4，A2-A9各个取1个)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义数据文件夹路径\n",
    "input_dir = 'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/source'\n",
    "output_dir = 'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/Federated/sample_space/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 读取数据函数\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path).values\n",
    "\n",
    "# 定义文件名和标签\n",
    "labels = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'B']\n",
    "label_to_class = {label: i for i, label in enumerate(labels[:-1])}\n",
    "\n",
    "# 读取所有类别的数据\n",
    "data = {label: load_data(os.path.join(input_dir, f'{label}.csv')) for label in labels}\n",
    "\n",
    "def sample_and_save(client_id,round_idx, data, dictx):\n",
    "    #  a1_samples, a2_a9_samples, b_samples\n",
    "    a1_samples = dictx['A1']\n",
    "    b_samples = dictx['B']\n",
    "    # a2_a9_samples_list = [dictx[f'A{i}'] for i in range(2,10)]\n",
    "    sampled_data = []\n",
    "\n",
    "    # 从 A1 中抽取样本\n",
    "    a1_sampled = data['A1'][:a1_samples]\n",
    "    sampled_data.append(np.column_stack((a1_sampled, np.full(len(a1_sampled), 'A1'), np.zeros(len(a1_sampled)))))\n",
    "    data['A1'] = data['A1'][a1_samples:]  # 更新 A1 剩余数据\n",
    "\n",
    "    # 从 A2-A9 中每个抽取样本\n",
    "    for label in labels[1:-1]:\n",
    "        samples_num = dictx[label]\n",
    "        samples = data[label][:samples_num]\n",
    "        sampled_data.append(np.column_stack((samples, np.full(len(samples), label), np.zeros(len(samples)))))\n",
    "        data[label] = data[label][samples_num:]  # 更新剩余数据\n",
    "\n",
    "    # 从 B 中抽取样本\n",
    "    b_sampled = data['B'][:b_samples]\n",
    "    sampled_data.append(np.column_stack((b_sampled, np.full(len(b_sampled), 'B'), np.ones(len(b_sampled)))))\n",
    "    data['B'] = data['B'][b_samples:]  # 更新 B 剩余数据\n",
    "\n",
    "    # 组合所有样本并保存为 CSV\n",
    "    round_data = np.vstack(sampled_data)\n",
    "    round_df = pd.DataFrame(round_data, columns=['feature1', 'feature2', 'label', 'class'])\n",
    "    output_dir1 = os.path.join(output_dir,f'client_{client_id}')\n",
    "    os.makedirs(output_dir1, exist_ok=True)\n",
    "    round_df.to_csv(os.path.join(output_dir1, f'round_{round_idx}.csv'), index=False)\n",
    "# client0（a0,b120）,client1(A1 60,b60),client2(A2 60, b60),....client9(A9 60,B60)\n",
    "start_index = 0\n",
    "client_nums = 10\n",
    "for r in range(100):\n",
    "    samples_dict = {f'A{i+1}': 0 for i in range(9)}\n",
    "    samples_dict['B'] = 120\n",
    "    sample_and_save(client_id=0,round_idx=r, data=data, dictx=samples_dict)\n",
    "    for client_id in range(1,client_nums):\n",
    "        samples_dict = {f'A{i+1}': 0 for i in range(9)}\n",
    "        samples_dict[f'A{client_id}'] = 60\n",
    "        samples_dict['B'] = 60\n",
    "        #print(samples_dict)\n",
    "        sample_and_save(client_id=client_id,round_idx=r, data=data, dictx=samples_dict)\n",
    "print(f'Data sampling and saving completed. Files are saved in {output_dir}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/200], Loss: 0.3946\n",
      "Epoch [161/200], Loss: 0.3944\n",
      "Epoch [171/200], Loss: 0.3942\n",
      "Epoch [181/200], Loss: 0.3940\n",
      "Epoch [191/200], Loss: 0.3938\n",
      "Test Accuracy: 68.30%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5091\n",
      "Epoch [11/200], Loss: 0.3746\n",
      "Epoch [21/200], Loss: 0.3738\n",
      "Epoch [31/200], Loss: 0.3732\n",
      "Epoch [41/200], Loss: 0.3727\n",
      "Epoch [51/200], Loss: 0.3722\n",
      "Epoch [61/200], Loss: 0.3718\n",
      "Epoch [71/200], Loss: 0.3713\n",
      "Epoch [81/200], Loss: 0.3710\n",
      "Epoch [91/200], Loss: 0.3706\n",
      "Epoch [101/200], Loss: 0.3703\n",
      "Epoch [111/200], Loss: 0.3699\n",
      "Epoch [121/200], Loss: 0.3696\n",
      "Epoch [131/200], Loss: 0.3694\n",
      "Epoch [141/200], Loss: 0.3691\n",
      "Epoch [151/200], Loss: 0.3688\n",
      "Epoch [161/200], Loss: 0.3686\n",
      "Epoch [171/200], Loss: 0.3683\n",
      "Epoch [181/200], Loss: 0.3681\n",
      "Epoch [191/200], Loss: 0.3679\n",
      "Test Accuracy: 71.92%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5334\n",
      "Epoch [11/200], Loss: 0.3938\n",
      "Epoch [21/200], Loss: 0.3934\n",
      "Epoch [31/200], Loss: 0.3931\n",
      "Epoch [41/200], Loss: 0.3928\n",
      "Epoch [51/200], Loss: 0.3926\n",
      "Epoch [61/200], Loss: 0.3924\n",
      "Epoch [71/200], Loss: 0.3922\n",
      "Epoch [81/200], Loss: 0.3920\n",
      "Epoch [91/200], Loss: 0.3919\n",
      "Epoch [101/200], Loss: 0.3917\n",
      "Epoch [111/200], Loss: 0.3916\n",
      "Epoch [121/200], Loss: 0.3915\n",
      "Epoch [131/200], Loss: 0.3913\n",
      "Epoch [141/200], Loss: 0.3912\n",
      "Epoch [151/200], Loss: 0.3911\n",
      "Epoch [161/200], Loss: 0.3910\n",
      "Epoch [171/200], Loss: 0.3909\n",
      "Epoch [181/200], Loss: 0.3908\n",
      "Epoch [191/200], Loss: 0.3906\n",
      "Test Accuracy: 69.68%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5144\n",
      "Epoch [11/200], Loss: 0.3658\n",
      "Epoch [21/200], Loss: 0.3647\n",
      "Epoch [31/200], Loss: 0.3640\n",
      "Epoch [41/200], Loss: 0.3635\n",
      "Epoch [51/200], Loss: 0.3630\n",
      "Epoch [61/200], Loss: 0.3626\n",
      "Epoch [71/200], Loss: 0.3623\n",
      "Epoch [81/200], Loss: 0.3619\n",
      "Epoch [91/200], Loss: 0.3616\n",
      "Epoch [101/200], Loss: 0.3613\n",
      "Epoch [111/200], Loss: 0.3610\n",
      "Epoch [121/200], Loss: 0.3607\n",
      "Epoch [131/200], Loss: 0.3604\n",
      "Epoch [141/200], Loss: 0.3602\n",
      "Epoch [151/200], Loss: 0.3599\n",
      "Epoch [161/200], Loss: 0.3597\n",
      "Epoch [171/200], Loss: 0.3595\n",
      "Epoch [181/200], Loss: 0.3593\n",
      "Epoch [191/200], Loss: 0.3591\n",
      "Test Accuracy: 70.16%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5126\n",
      "Epoch [11/200], Loss: 0.3624\n",
      "Epoch [21/200], Loss: 0.3613\n",
      "Epoch [31/200], Loss: 0.3605\n",
      "Epoch [41/200], Loss: 0.3599\n",
      "Epoch [51/200], Loss: 0.3594\n",
      "Epoch [61/200], Loss: 0.3589\n",
      "Epoch [71/200], Loss: 0.3585\n",
      "Epoch [81/200], Loss: 0.3580\n",
      "Epoch [91/200], Loss: 0.3576\n",
      "Epoch [101/200], Loss: 0.3573\n",
      "Epoch [111/200], Loss: 0.3569\n",
      "Epoch [121/200], Loss: 0.3566\n",
      "Epoch [131/200], Loss: 0.3563\n",
      "Epoch [141/200], Loss: 0.3560\n",
      "Epoch [151/200], Loss: 0.3557\n",
      "Epoch [161/200], Loss: 0.3554\n",
      "Epoch [171/200], Loss: 0.3552\n",
      "Epoch [181/200], Loss: 0.3549\n",
      "Epoch [191/200], Loss: 0.3547\n",
      "Test Accuracy: 70.05%\n",
      "Test Accuracy: 63.70%\n",
      "round_89\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5560\n",
      "Epoch [11/200], Loss: 0.4155\n",
      "Epoch [21/200], Loss: 0.4149\n",
      "Epoch [31/200], Loss: 0.4145\n",
      "Epoch [41/200], Loss: 0.4141\n",
      "Epoch [51/200], Loss: 0.4137\n",
      "Epoch [61/200], Loss: 0.4134\n",
      "Epoch [71/200], Loss: 0.4132\n",
      "Epoch [81/200], Loss: 0.4129\n",
      "Epoch [91/200], Loss: 0.4127\n",
      "Epoch [101/200], Loss: 0.4125\n",
      "Epoch [111/200], Loss: 0.4123\n",
      "Epoch [121/200], Loss: 0.4122\n",
      "Epoch [131/200], Loss: 0.4120\n",
      "Epoch [141/200], Loss: 0.4119\n",
      "Epoch [151/200], Loss: 0.4117\n",
      "Epoch [161/200], Loss: 0.4116\n",
      "Epoch [171/200], Loss: 0.4115\n",
      "Epoch [181/200], Loss: 0.4113\n",
      "Epoch [191/200], Loss: 0.4112\n",
      "Test Accuracy: 54.62%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5293\n",
      "Epoch [11/200], Loss: 0.3975\n",
      "Epoch [21/200], Loss: 0.3970\n",
      "Epoch [31/200], Loss: 0.3965\n",
      "Epoch [41/200], Loss: 0.3962\n",
      "Epoch [51/200], Loss: 0.3959\n",
      "Epoch [61/200], Loss: 0.3956\n",
      "Epoch [71/200], Loss: 0.3954\n",
      "Epoch [81/200], Loss: 0.3952\n",
      "Epoch [91/200], Loss: 0.3950\n",
      "Epoch [101/200], Loss: 0.3949\n",
      "Epoch [111/200], Loss: 0.3947\n",
      "Epoch [121/200], Loss: 0.3946\n",
      "Epoch [131/200], Loss: 0.3945\n",
      "Epoch [141/200], Loss: 0.3944\n",
      "Epoch [151/200], Loss: 0.3943\n",
      "Epoch [161/200], Loss: 0.3942\n",
      "Epoch [171/200], Loss: 0.3941\n",
      "Epoch [181/200], Loss: 0.3940\n",
      "Epoch [191/200], Loss: 0.3939\n",
      "Test Accuracy: 53.71%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5664\n",
      "Epoch [11/200], Loss: 0.4098\n",
      "Epoch [21/200], Loss: 0.4093\n",
      "Epoch [31/200], Loss: 0.4090\n",
      "Epoch [41/200], Loss: 0.4087\n",
      "Epoch [51/200], Loss: 0.4084\n",
      "Epoch [61/200], Loss: 0.4081\n",
      "Epoch [71/200], Loss: 0.4078\n",
      "Epoch [81/200], Loss: 0.4076\n",
      "Epoch [91/200], Loss: 0.4074\n",
      "Epoch [101/200], Loss: 0.4072\n",
      "Epoch [111/200], Loss: 0.4070\n",
      "Epoch [121/200], Loss: 0.4068\n",
      "Epoch [131/200], Loss: 0.4066\n",
      "Epoch [141/200], Loss: 0.4065\n",
      "Epoch [151/200], Loss: 0.4063\n",
      "Epoch [161/200], Loss: 0.4062\n",
      "Epoch [171/200], Loss: 0.4061\n",
      "Epoch [181/200], Loss: 0.4060\n",
      "Epoch [191/200], Loss: 0.4058\n",
      "Test Accuracy: 54.64%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5846\n",
      "Epoch [11/200], Loss: 0.4197\n",
      "Epoch [21/200], Loss: 0.4187\n",
      "Epoch [31/200], Loss: 0.4180\n",
      "Epoch [41/200], Loss: 0.4173\n",
      "Epoch [51/200], Loss: 0.4168\n",
      "Epoch [61/200], Loss: 0.4163\n",
      "Epoch [71/200], Loss: 0.4159\n",
      "Epoch [81/200], Loss: 0.4155\n",
      "Epoch [91/200], Loss: 0.4152\n",
      "Epoch [101/200], Loss: 0.4148\n",
      "Epoch [111/200], Loss: 0.4146\n",
      "Epoch [121/200], Loss: 0.4143\n",
      "Epoch [131/200], Loss: 0.4140\n",
      "Epoch [141/200], Loss: 0.4138\n",
      "Epoch [151/200], Loss: 0.4136\n",
      "Epoch [161/200], Loss: 0.4134\n",
      "Epoch [171/200], Loss: 0.4132\n",
      "Epoch [181/200], Loss: 0.4131\n",
      "Epoch [191/200], Loss: 0.4129\n",
      "Test Accuracy: 54.18%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5650\n",
      "Epoch [11/200], Loss: 0.4026\n",
      "Epoch [21/200], Loss: 0.4022\n",
      "Epoch [31/200], Loss: 0.4018\n",
      "Epoch [41/200], Loss: 0.4015\n",
      "Epoch [51/200], Loss: 0.4013\n",
      "Epoch [61/200], Loss: 0.4011\n",
      "Epoch [71/200], Loss: 0.4009\n",
      "Epoch [81/200], Loss: 0.4007\n",
      "Epoch [91/200], Loss: 0.4005\n",
      "Epoch [101/200], Loss: 0.4004\n",
      "Epoch [111/200], Loss: 0.4002\n",
      "Epoch [121/200], Loss: 0.4001\n",
      "Epoch [131/200], Loss: 0.4000\n",
      "Epoch [141/200], Loss: 0.3998\n",
      "Epoch [151/200], Loss: 0.3997\n",
      "Epoch [161/200], Loss: 0.3996\n",
      "Epoch [171/200], Loss: 0.3995\n",
      "Epoch [181/200], Loss: 0.3994\n",
      "Epoch [191/200], Loss: 0.3993\n",
      "Test Accuracy: 54.45%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5335\n",
      "Epoch [11/200], Loss: 0.3953\n",
      "Epoch [21/200], Loss: 0.3948\n",
      "Epoch [31/200], Loss: 0.3946\n",
      "Epoch [41/200], Loss: 0.3943\n",
      "Epoch [51/200], Loss: 0.3942\n",
      "Epoch [61/200], Loss: 0.3940\n",
      "Epoch [71/200], Loss: 0.3938\n",
      "Epoch [81/200], Loss: 0.3937\n",
      "Epoch [91/200], Loss: 0.3935\n",
      "Epoch [101/200], Loss: 0.3934\n",
      "Epoch [111/200], Loss: 0.3933\n",
      "Epoch [121/200], Loss: 0.3932\n",
      "Epoch [131/200], Loss: 0.3931\n",
      "Epoch [141/200], Loss: 0.3930\n",
      "Epoch [151/200], Loss: 0.3929\n",
      "Epoch [161/200], Loss: 0.3928\n",
      "Epoch [171/200], Loss: 0.3927\n",
      "Epoch [181/200], Loss: 0.3926\n",
      "Epoch [191/200], Loss: 0.3925\n",
      "Test Accuracy: 69.43%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5298\n",
      "Epoch [11/200], Loss: 0.3920\n",
      "Epoch [21/200], Loss: 0.3917\n",
      "Epoch [31/200], Loss: 0.3915\n",
      "Epoch [41/200], Loss: 0.3913\n",
      "Epoch [51/200], Loss: 0.3912\n",
      "Epoch [61/200], Loss: 0.3910\n",
      "Epoch [71/200], Loss: 0.3909\n",
      "Epoch [81/200], Loss: 0.3908\n",
      "Epoch [91/200], Loss: 0.3907\n",
      "Epoch [101/200], Loss: 0.3905\n",
      "Epoch [111/200], Loss: 0.3904\n",
      "Epoch [121/200], Loss: 0.3903\n",
      "Epoch [131/200], Loss: 0.3903\n",
      "Epoch [141/200], Loss: 0.3902\n",
      "Epoch [151/200], Loss: 0.3901\n",
      "Epoch [161/200], Loss: 0.3900\n",
      "Epoch [171/200], Loss: 0.3899\n",
      "Epoch [181/200], Loss: 0.3898\n",
      "Epoch [191/200], Loss: 0.3898\n",
      "Test Accuracy: 69.70%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5054\n",
      "Epoch [11/200], Loss: 0.3561\n",
      "Epoch [21/200], Loss: 0.3552\n",
      "Epoch [31/200], Loss: 0.3545\n",
      "Epoch [41/200], Loss: 0.3540\n",
      "Epoch [51/200], Loss: 0.3535\n",
      "Epoch [61/200], Loss: 0.3531\n",
      "Epoch [71/200], Loss: 0.3527\n",
      "Epoch [81/200], Loss: 0.3523\n",
      "Epoch [91/200], Loss: 0.3520\n",
      "Epoch [101/200], Loss: 0.3516\n",
      "Epoch [111/200], Loss: 0.3513\n",
      "Epoch [121/200], Loss: 0.3509\n",
      "Epoch [131/200], Loss: 0.3506\n",
      "Epoch [141/200], Loss: 0.3503\n",
      "Epoch [151/200], Loss: 0.3501\n",
      "Epoch [161/200], Loss: 0.3498\n",
      "Epoch [171/200], Loss: 0.3495\n",
      "Epoch [181/200], Loss: 0.3493\n",
      "Epoch [191/200], Loss: 0.3490\n",
      "Test Accuracy: 70.32%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5210\n",
      "Epoch [11/200], Loss: 0.3781\n",
      "Epoch [21/200], Loss: 0.3776\n",
      "Epoch [31/200], Loss: 0.3771\n",
      "Epoch [41/200], Loss: 0.3767\n",
      "Epoch [51/200], Loss: 0.3764\n",
      "Epoch [61/200], Loss: 0.3761\n",
      "Epoch [71/200], Loss: 0.3758\n",
      "Epoch [81/200], Loss: 0.3755\n",
      "Epoch [91/200], Loss: 0.3752\n",
      "Epoch [101/200], Loss: 0.3750\n",
      "Epoch [111/200], Loss: 0.3748\n",
      "Epoch [121/200], Loss: 0.3746\n",
      "Epoch [131/200], Loss: 0.3744\n",
      "Epoch [141/200], Loss: 0.3742\n",
      "Epoch [151/200], Loss: 0.3740\n",
      "Epoch [161/200], Loss: 0.3738\n",
      "Epoch [171/200], Loss: 0.3736\n",
      "Epoch [181/200], Loss: 0.3735\n",
      "Epoch [191/200], Loss: 0.3733\n",
      "Test Accuracy: 70.64%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5264\n",
      "Epoch [11/200], Loss: 0.3831\n",
      "Epoch [21/200], Loss: 0.3824\n",
      "Epoch [31/200], Loss: 0.3820\n",
      "Epoch [41/200], Loss: 0.3816\n",
      "Epoch [51/200], Loss: 0.3812\n",
      "Epoch [61/200], Loss: 0.3809\n",
      "Epoch [71/200], Loss: 0.3806\n",
      "Epoch [81/200], Loss: 0.3803\n",
      "Epoch [91/200], Loss: 0.3801\n",
      "Epoch [101/200], Loss: 0.3798\n",
      "Epoch [111/200], Loss: 0.3796\n",
      "Epoch [121/200], Loss: 0.3794\n",
      "Epoch [131/200], Loss: 0.3792\n",
      "Epoch [141/200], Loss: 0.3790\n",
      "Epoch [151/200], Loss: 0.3788\n",
      "Epoch [161/200], Loss: 0.3786\n",
      "Epoch [171/200], Loss: 0.3784\n",
      "Epoch [181/200], Loss: 0.3783\n",
      "Epoch [191/200], Loss: 0.3781\n",
      "Test Accuracy: 69.96%\n",
      "Test Accuracy: 63.45%\n",
      "round_90\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5509\n",
      "Epoch [11/200], Loss: 0.4106\n",
      "Epoch [21/200], Loss: 0.4098\n",
      "Epoch [31/200], Loss: 0.4094\n",
      "Epoch [41/200], Loss: 0.4091\n",
      "Epoch [51/200], Loss: 0.4088\n",
      "Epoch [61/200], Loss: 0.4086\n",
      "Epoch [71/200], Loss: 0.4084\n",
      "Epoch [81/200], Loss: 0.4082\n",
      "Epoch [91/200], Loss: 0.4081\n",
      "Epoch [101/200], Loss: 0.4079\n",
      "Epoch [111/200], Loss: 0.4078\n",
      "Epoch [121/200], Loss: 0.4077\n",
      "Epoch [131/200], Loss: 0.4076\n",
      "Epoch [141/200], Loss: 0.4074\n",
      "Epoch [151/200], Loss: 0.4073\n",
      "Epoch [161/200], Loss: 0.4072\n",
      "Epoch [171/200], Loss: 0.4071\n",
      "Epoch [181/200], Loss: 0.4070\n",
      "Epoch [191/200], Loss: 0.4069\n",
      "Test Accuracy: 68.66%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5411\n",
      "Epoch [11/200], Loss: 0.3925\n",
      "Epoch [21/200], Loss: 0.3919\n",
      "Epoch [31/200], Loss: 0.3915\n",
      "Epoch [41/200], Loss: 0.3912\n",
      "Epoch [51/200], Loss: 0.3909\n",
      "Epoch [61/200], Loss: 0.3907\n",
      "Epoch [71/200], Loss: 0.3904\n",
      "Epoch [81/200], Loss: 0.3902\n",
      "Epoch [91/200], Loss: 0.3900\n",
      "Epoch [101/200], Loss: 0.3898\n",
      "Epoch [111/200], Loss: 0.3896\n",
      "Epoch [121/200], Loss: 0.3894\n",
      "Epoch [131/200], Loss: 0.3892\n",
      "Epoch [141/200], Loss: 0.3890\n",
      "Epoch [151/200], Loss: 0.3889\n",
      "Epoch [161/200], Loss: 0.3887\n",
      "Epoch [171/200], Loss: 0.3885\n",
      "Epoch [181/200], Loss: 0.3884\n",
      "Epoch [191/200], Loss: 0.3882\n",
      "Test Accuracy: 69.75%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5199\n",
      "Epoch [11/200], Loss: 0.3662\n",
      "Epoch [21/200], Loss: 0.3653\n",
      "Epoch [31/200], Loss: 0.3647\n",
      "Epoch [41/200], Loss: 0.3643\n",
      "Epoch [51/200], Loss: 0.3639\n",
      "Epoch [61/200], Loss: 0.3635\n",
      "Epoch [71/200], Loss: 0.3632\n",
      "Epoch [81/200], Loss: 0.3629\n",
      "Epoch [91/200], Loss: 0.3627\n",
      "Epoch [101/200], Loss: 0.3624\n",
      "Epoch [111/200], Loss: 0.3622\n",
      "Epoch [121/200], Loss: 0.3620\n",
      "Epoch [131/200], Loss: 0.3617\n",
      "Epoch [141/200], Loss: 0.3615\n",
      "Epoch [151/200], Loss: 0.3613\n",
      "Epoch [161/200], Loss: 0.3612\n",
      "Epoch [171/200], Loss: 0.3610\n",
      "Epoch [181/200], Loss: 0.3608\n",
      "Epoch [191/200], Loss: 0.3606\n",
      "Test Accuracy: 69.23%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5364\n",
      "Epoch [11/200], Loss: 0.3775\n",
      "Epoch [21/200], Loss: 0.3767\n",
      "Epoch [31/200], Loss: 0.3763\n",
      "Epoch [41/200], Loss: 0.3759\n",
      "Epoch [51/200], Loss: 0.3756\n",
      "Epoch [61/200], Loss: 0.3753\n",
      "Epoch [71/200], Loss: 0.3750\n",
      "Epoch [81/200], Loss: 0.3747\n",
      "Epoch [91/200], Loss: 0.3745\n",
      "Epoch [101/200], Loss: 0.3743\n",
      "Epoch [111/200], Loss: 0.3741\n",
      "Epoch [121/200], Loss: 0.3739\n",
      "Epoch [131/200], Loss: 0.3737\n",
      "Epoch [141/200], Loss: 0.3735\n",
      "Epoch [151/200], Loss: 0.3733\n",
      "Epoch [161/200], Loss: 0.3731\n",
      "Epoch [171/200], Loss: 0.3730\n",
      "Epoch [181/200], Loss: 0.3728\n",
      "Epoch [191/200], Loss: 0.3727\n",
      "Test Accuracy: 69.36%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5453\n",
      "Epoch [11/200], Loss: 0.3955\n",
      "Epoch [21/200], Loss: 0.3944\n",
      "Epoch [31/200], Loss: 0.3938\n",
      "Epoch [41/200], Loss: 0.3935\n",
      "Epoch [51/200], Loss: 0.3932\n",
      "Epoch [61/200], Loss: 0.3930\n",
      "Epoch [71/200], Loss: 0.3928\n",
      "Epoch [81/200], Loss: 0.3926\n",
      "Epoch [91/200], Loss: 0.3924\n",
      "Epoch [101/200], Loss: 0.3922\n",
      "Epoch [111/200], Loss: 0.3921\n",
      "Epoch [121/200], Loss: 0.3919\n",
      "Epoch [131/200], Loss: 0.3918\n",
      "Epoch [141/200], Loss: 0.3916\n",
      "Epoch [151/200], Loss: 0.3914\n",
      "Epoch [161/200], Loss: 0.3913\n",
      "Epoch [171/200], Loss: 0.3911\n",
      "Epoch [181/200], Loss: 0.3910\n",
      "Epoch [191/200], Loss: 0.3908\n",
      "Test Accuracy: 69.15%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5759\n",
      "Epoch [11/200], Loss: 0.4152\n",
      "Epoch [21/200], Loss: 0.4143\n",
      "Epoch [31/200], Loss: 0.4137\n",
      "Epoch [41/200], Loss: 0.4132\n",
      "Epoch [51/200], Loss: 0.4128\n",
      "Epoch [61/200], Loss: 0.4125\n",
      "Epoch [71/200], Loss: 0.4122\n",
      "Epoch [81/200], Loss: 0.4120\n",
      "Epoch [91/200], Loss: 0.4117\n",
      "Epoch [101/200], Loss: 0.4115\n",
      "Epoch [111/200], Loss: 0.4113\n",
      "Epoch [121/200], Loss: 0.4111\n",
      "Epoch [131/200], Loss: 0.4109\n",
      "Epoch [141/200], Loss: 0.4107\n",
      "Epoch [151/200], Loss: 0.4106\n",
      "Epoch [161/200], Loss: 0.4104\n",
      "Epoch [171/200], Loss: 0.4103\n",
      "Epoch [181/200], Loss: 0.4101\n",
      "Epoch [191/200], Loss: 0.4100\n",
      "Test Accuracy: 53.99%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5560\n",
      "Epoch [11/200], Loss: 0.4122\n",
      "Epoch [21/200], Loss: 0.4116\n",
      "Epoch [31/200], Loss: 0.4112\n",
      "Epoch [41/200], Loss: 0.4108\n",
      "Epoch [51/200], Loss: 0.4104\n",
      "Epoch [61/200], Loss: 0.4101\n",
      "Epoch [71/200], Loss: 0.4098\n",
      "Epoch [81/200], Loss: 0.4095\n",
      "Epoch [91/200], Loss: 0.4093\n",
      "Epoch [101/200], Loss: 0.4091\n",
      "Epoch [111/200], Loss: 0.4089\n",
      "Epoch [121/200], Loss: 0.4087\n",
      "Epoch [131/200], Loss: 0.4085\n",
      "Epoch [141/200], Loss: 0.4084\n",
      "Epoch [151/200], Loss: 0.4082\n",
      "Epoch [161/200], Loss: 0.4081\n",
      "Epoch [171/200], Loss: 0.4079\n",
      "Epoch [181/200], Loss: 0.4078\n",
      "Epoch [191/200], Loss: 0.4077\n",
      "Test Accuracy: 54.55%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5596\n",
      "Epoch [11/200], Loss: 0.4137\n",
      "Epoch [21/200], Loss: 0.4127\n",
      "Epoch [31/200], Loss: 0.4120\n",
      "Epoch [41/200], Loss: 0.4115\n",
      "Epoch [51/200], Loss: 0.4111\n",
      "Epoch [61/200], Loss: 0.4107\n",
      "Epoch [71/200], Loss: 0.4103\n",
      "Epoch [81/200], Loss: 0.4100\n",
      "Epoch [91/200], Loss: 0.4097\n",
      "Epoch [101/200], Loss: 0.4095\n",
      "Epoch [111/200], Loss: 0.4092\n",
      "Epoch [121/200], Loss: 0.4090\n",
      "Epoch [131/200], Loss: 0.4088\n",
      "Epoch [141/200], Loss: 0.4086\n",
      "Epoch [151/200], Loss: 0.4084\n",
      "Epoch [161/200], Loss: 0.4083\n",
      "Epoch [171/200], Loss: 0.4081\n",
      "Epoch [181/200], Loss: 0.4080\n",
      "Epoch [191/200], Loss: 0.4078\n",
      "Test Accuracy: 53.96%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5801\n",
      "Epoch [11/200], Loss: 0.4215\n",
      "Epoch [21/200], Loss: 0.4208\n",
      "Epoch [31/200], Loss: 0.4203\n",
      "Epoch [41/200], Loss: 0.4199\n",
      "Epoch [51/200], Loss: 0.4195\n",
      "Epoch [61/200], Loss: 0.4192\n",
      "Epoch [71/200], Loss: 0.4189\n",
      "Epoch [81/200], Loss: 0.4187\n",
      "Epoch [91/200], Loss: 0.4184\n",
      "Epoch [101/200], Loss: 0.4182\n",
      "Epoch [111/200], Loss: 0.4180\n",
      "Epoch [121/200], Loss: 0.4178\n",
      "Epoch [131/200], Loss: 0.4176\n",
      "Epoch [141/200], Loss: 0.4174\n",
      "Epoch [151/200], Loss: 0.4173\n",
      "Epoch [161/200], Loss: 0.4171\n",
      "Epoch [171/200], Loss: 0.4170\n",
      "Epoch [181/200], Loss: 0.4168\n",
      "Epoch [191/200], Loss: 0.4167\n",
      "Test Accuracy: 54.23%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5468\n",
      "Epoch [11/200], Loss: 0.4039\n",
      "Epoch [21/200], Loss: 0.4035\n",
      "Epoch [31/200], Loss: 0.4032\n",
      "Epoch [41/200], Loss: 0.4029\n",
      "Epoch [51/200], Loss: 0.4027\n",
      "Epoch [61/200], Loss: 0.4024\n",
      "Epoch [71/200], Loss: 0.4022\n",
      "Epoch [81/200], Loss: 0.4020\n",
      "Epoch [91/200], Loss: 0.4018\n",
      "Epoch [101/200], Loss: 0.4016\n",
      "Epoch [111/200], Loss: 0.4014\n",
      "Epoch [121/200], Loss: 0.4012\n",
      "Epoch [131/200], Loss: 0.4010\n",
      "Epoch [141/200], Loss: 0.4009\n",
      "Epoch [151/200], Loss: 0.4007\n",
      "Epoch [161/200], Loss: 0.4005\n",
      "Epoch [171/200], Loss: 0.4004\n",
      "Epoch [181/200], Loss: 0.4003\n",
      "Epoch [191/200], Loss: 0.4001\n",
      "Test Accuracy: 54.67%\n",
      "Test Accuracy: 63.68%\n",
      "round_91\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5094\n",
      "Epoch [11/200], Loss: 0.3591\n",
      "Epoch [21/200], Loss: 0.3576\n",
      "Epoch [31/200], Loss: 0.3566\n",
      "Epoch [41/200], Loss: 0.3559\n",
      "Epoch [51/200], Loss: 0.3553\n",
      "Epoch [61/200], Loss: 0.3547\n",
      "Epoch [71/200], Loss: 0.3542\n",
      "Epoch [81/200], Loss: 0.3537\n",
      "Epoch [91/200], Loss: 0.3533\n",
      "Epoch [101/200], Loss: 0.3529\n",
      "Epoch [111/200], Loss: 0.3525\n",
      "Epoch [121/200], Loss: 0.3522\n",
      "Epoch [131/200], Loss: 0.3518\n",
      "Epoch [141/200], Loss: 0.3515\n",
      "Epoch [151/200], Loss: 0.3512\n",
      "Epoch [161/200], Loss: 0.3509\n",
      "Epoch [171/200], Loss: 0.3506\n",
      "Epoch [181/200], Loss: 0.3503\n",
      "Epoch [191/200], Loss: 0.3501\n",
      "Test Accuracy: 70.86%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5364\n",
      "Epoch [11/200], Loss: 0.4045\n",
      "Epoch [21/200], Loss: 0.4039\n",
      "Epoch [31/200], Loss: 0.4036\n",
      "Epoch [41/200], Loss: 0.4034\n",
      "Epoch [51/200], Loss: 0.4032\n",
      "Epoch [61/200], Loss: 0.4030\n",
      "Epoch [71/200], Loss: 0.4029\n",
      "Epoch [81/200], Loss: 0.4027\n",
      "Epoch [91/200], Loss: 0.4026\n",
      "Epoch [101/200], Loss: 0.4024\n",
      "Epoch [111/200], Loss: 0.4023\n",
      "Epoch [121/200], Loss: 0.4021\n",
      "Epoch [131/200], Loss: 0.4020\n",
      "Epoch [141/200], Loss: 0.4019\n",
      "Epoch [151/200], Loss: 0.4018\n",
      "Epoch [161/200], Loss: 0.4016\n",
      "Epoch [171/200], Loss: 0.4015\n",
      "Epoch [181/200], Loss: 0.4014\n",
      "Epoch [191/200], Loss: 0.4013\n",
      "Test Accuracy: 69.86%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5269\n",
      "Epoch [11/200], Loss: 0.3948\n",
      "Epoch [21/200], Loss: 0.3938\n",
      "Epoch [31/200], Loss: 0.3931\n",
      "Epoch [41/200], Loss: 0.3926\n",
      "Epoch [51/200], Loss: 0.3922\n",
      "Epoch [61/200], Loss: 0.3919\n",
      "Epoch [71/200], Loss: 0.3916\n",
      "Epoch [81/200], Loss: 0.3913\n",
      "Epoch [91/200], Loss: 0.3910\n",
      "Epoch [101/200], Loss: 0.3907\n",
      "Epoch [111/200], Loss: 0.3905\n",
      "Epoch [121/200], Loss: 0.3903\n",
      "Epoch [131/200], Loss: 0.3901\n",
      "Epoch [141/200], Loss: 0.3899\n",
      "Epoch [151/200], Loss: 0.3897\n",
      "Epoch [161/200], Loss: 0.3896\n",
      "Epoch [171/200], Loss: 0.3894\n",
      "Epoch [181/200], Loss: 0.3892\n",
      "Epoch [191/200], Loss: 0.3891\n",
      "Test Accuracy: 69.77%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5369\n",
      "Epoch [11/200], Loss: 0.3912\n",
      "Epoch [21/200], Loss: 0.3906\n",
      "Epoch [31/200], Loss: 0.3902\n",
      "Epoch [41/200], Loss: 0.3899\n",
      "Epoch [51/200], Loss: 0.3897\n",
      "Epoch [61/200], Loss: 0.3895\n",
      "Epoch [71/200], Loss: 0.3892\n",
      "Epoch [81/200], Loss: 0.3890\n",
      "Epoch [91/200], Loss: 0.3888\n",
      "Epoch [101/200], Loss: 0.3886\n",
      "Epoch [111/200], Loss: 0.3885\n",
      "Epoch [121/200], Loss: 0.3883\n",
      "Epoch [131/200], Loss: 0.3881\n",
      "Epoch [141/200], Loss: 0.3880\n",
      "Epoch [151/200], Loss: 0.3878\n",
      "Epoch [161/200], Loss: 0.3877\n",
      "Epoch [171/200], Loss: 0.3876\n",
      "Epoch [181/200], Loss: 0.3874\n",
      "Epoch [191/200], Loss: 0.3873\n",
      "Test Accuracy: 68.86%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5273\n",
      "Epoch [11/200], Loss: 0.3746\n",
      "Epoch [21/200], Loss: 0.3739\n",
      "Epoch [31/200], Loss: 0.3734\n",
      "Epoch [41/200], Loss: 0.3731\n",
      "Epoch [51/200], Loss: 0.3727\n",
      "Epoch [61/200], Loss: 0.3724\n",
      "Epoch [71/200], Loss: 0.3721\n",
      "Epoch [81/200], Loss: 0.3719\n",
      "Epoch [91/200], Loss: 0.3716\n",
      "Epoch [101/200], Loss: 0.3714\n",
      "Epoch [111/200], Loss: 0.3712\n",
      "Epoch [121/200], Loss: 0.3710\n",
      "Epoch [131/200], Loss: 0.3708\n",
      "Epoch [141/200], Loss: 0.3706\n",
      "Epoch [151/200], Loss: 0.3705\n",
      "Epoch [161/200], Loss: 0.3703\n",
      "Epoch [171/200], Loss: 0.3701\n",
      "Epoch [181/200], Loss: 0.3699\n",
      "Epoch [191/200], Loss: 0.3698\n",
      "Test Accuracy: 69.91%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5475\n",
      "Epoch [11/200], Loss: 0.4038\n",
      "Epoch [21/200], Loss: 0.4032\n",
      "Epoch [31/200], Loss: 0.4027\n",
      "Epoch [41/200], Loss: 0.4023\n",
      "Epoch [51/200], Loss: 0.4020\n",
      "Epoch [61/200], Loss: 0.4017\n",
      "Epoch [71/200], Loss: 0.4015\n",
      "Epoch [81/200], Loss: 0.4013\n",
      "Epoch [91/200], Loss: 0.4011\n",
      "Epoch [101/200], Loss: 0.4009\n",
      "Epoch [111/200], Loss: 0.4007\n",
      "Epoch [121/200], Loss: 0.4006\n",
      "Epoch [131/200], Loss: 0.4004\n",
      "Epoch [141/200], Loss: 0.4003\n",
      "Epoch [151/200], Loss: 0.4002\n",
      "Epoch [161/200], Loss: 0.4000\n",
      "Epoch [171/200], Loss: 0.3999\n",
      "Epoch [181/200], Loss: 0.3998\n",
      "Epoch [191/200], Loss: 0.3996\n",
      "Test Accuracy: 54.27%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5189\n",
      "Epoch [11/200], Loss: 0.3922\n",
      "Epoch [21/200], Loss: 0.3917\n",
      "Epoch [31/200], Loss: 0.3914\n",
      "Epoch [41/200], Loss: 0.3911\n",
      "Epoch [51/200], Loss: 0.3910\n",
      "Epoch [61/200], Loss: 0.3908\n",
      "Epoch [71/200], Loss: 0.3907\n",
      "Epoch [81/200], Loss: 0.3906\n",
      "Epoch [91/200], Loss: 0.3904\n",
      "Epoch [101/200], Loss: 0.3903\n",
      "Epoch [111/200], Loss: 0.3902\n",
      "Epoch [121/200], Loss: 0.3901\n",
      "Epoch [131/200], Loss: 0.3900\n",
      "Epoch [141/200], Loss: 0.3899\n",
      "Epoch [151/200], Loss: 0.3898\n",
      "Epoch [161/200], Loss: 0.3897\n",
      "Epoch [171/200], Loss: 0.3897\n",
      "Epoch [181/200], Loss: 0.3896\n",
      "Epoch [191/200], Loss: 0.3895\n",
      "Test Accuracy: 54.24%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5410\n",
      "Epoch [11/200], Loss: 0.4008\n",
      "Epoch [21/200], Loss: 0.4002\n",
      "Epoch [31/200], Loss: 0.3998\n",
      "Epoch [41/200], Loss: 0.3995\n",
      "Epoch [51/200], Loss: 0.3993\n",
      "Epoch [61/200], Loss: 0.3990\n",
      "Epoch [71/200], Loss: 0.3988\n",
      "Epoch [81/200], Loss: 0.3986\n",
      "Epoch [91/200], Loss: 0.3985\n",
      "Epoch [101/200], Loss: 0.3983\n",
      "Epoch [111/200], Loss: 0.3981\n",
      "Epoch [121/200], Loss: 0.3980\n",
      "Epoch [131/200], Loss: 0.3978\n",
      "Epoch [141/200], Loss: 0.3977\n",
      "Epoch [151/200], Loss: 0.3975\n",
      "Epoch [161/200], Loss: 0.3974\n",
      "Epoch [171/200], Loss: 0.3973\n",
      "Epoch [181/200], Loss: 0.3972\n",
      "Epoch [191/200], Loss: 0.3970\n",
      "Test Accuracy: 54.43%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5234\n",
      "Epoch [11/200], Loss: 0.3814\n",
      "Epoch [21/200], Loss: 0.3810\n",
      "Epoch [31/200], Loss: 0.3807\n",
      "Epoch [41/200], Loss: 0.3805\n",
      "Epoch [51/200], Loss: 0.3803\n",
      "Epoch [61/200], Loss: 0.3802\n",
      "Epoch [71/200], Loss: 0.3800\n",
      "Epoch [81/200], Loss: 0.3799\n",
      "Epoch [91/200], Loss: 0.3797\n",
      "Epoch [101/200], Loss: 0.3796\n",
      "Epoch [111/200], Loss: 0.3795\n",
      "Epoch [121/200], Loss: 0.3793\n",
      "Epoch [131/200], Loss: 0.3792\n",
      "Epoch [141/200], Loss: 0.3791\n",
      "Epoch [151/200], Loss: 0.3790\n",
      "Epoch [161/200], Loss: 0.3789\n",
      "Epoch [171/200], Loss: 0.3788\n",
      "Epoch [181/200], Loss: 0.3787\n",
      "Epoch [191/200], Loss: 0.3786\n",
      "Test Accuracy: 54.19%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5807\n",
      "Epoch [11/200], Loss: 0.4198\n",
      "Epoch [21/200], Loss: 0.4192\n",
      "Epoch [31/200], Loss: 0.4187\n",
      "Epoch [41/200], Loss: 0.4183\n",
      "Epoch [51/200], Loss: 0.4180\n",
      "Epoch [61/200], Loss: 0.4177\n",
      "Epoch [71/200], Loss: 0.4173\n",
      "Epoch [81/200], Loss: 0.4171\n",
      "Epoch [91/200], Loss: 0.4168\n",
      "Epoch [101/200], Loss: 0.4165\n",
      "Epoch [111/200], Loss: 0.4163\n",
      "Epoch [121/200], Loss: 0.4161\n",
      "Epoch [131/200], Loss: 0.4158\n",
      "Epoch [141/200], Loss: 0.4156\n",
      "Epoch [151/200], Loss: 0.4154\n",
      "Epoch [161/200], Loss: 0.4152\n",
      "Epoch [171/200], Loss: 0.4150\n",
      "Epoch [181/200], Loss: 0.4149\n",
      "Epoch [191/200], Loss: 0.4147\n",
      "Test Accuracy: 54.19%\n",
      "Test Accuracy: 64.00%\n",
      "round_92\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5118\n",
      "Epoch [11/200], Loss: 0.3625\n",
      "Epoch [21/200], Loss: 0.3614\n",
      "Epoch [31/200], Loss: 0.3606\n",
      "Epoch [41/200], Loss: 0.3599\n",
      "Epoch [51/200], Loss: 0.3592\n",
      "Epoch [61/200], Loss: 0.3587\n",
      "Epoch [71/200], Loss: 0.3582\n",
      "Epoch [81/200], Loss: 0.3577\n",
      "Epoch [91/200], Loss: 0.3573\n",
      "Epoch [101/200], Loss: 0.3569\n",
      "Epoch [111/200], Loss: 0.3566\n",
      "Epoch [121/200], Loss: 0.3562\n",
      "Epoch [131/200], Loss: 0.3559\n",
      "Epoch [141/200], Loss: 0.3556\n",
      "Epoch [151/200], Loss: 0.3554\n",
      "Epoch [161/200], Loss: 0.3551\n",
      "Epoch [171/200], Loss: 0.3549\n",
      "Epoch [181/200], Loss: 0.3547\n",
      "Epoch [191/200], Loss: 0.3544\n",
      "Test Accuracy: 70.92%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5238\n",
      "Epoch [11/200], Loss: 0.3831\n",
      "Epoch [21/200], Loss: 0.3827\n",
      "Epoch [31/200], Loss: 0.3824\n",
      "Epoch [41/200], Loss: 0.3821\n",
      "Epoch [51/200], Loss: 0.3818\n",
      "Epoch [61/200], Loss: 0.3815\n",
      "Epoch [71/200], Loss: 0.3812\n",
      "Epoch [81/200], Loss: 0.3810\n",
      "Epoch [91/200], Loss: 0.3808\n",
      "Epoch [101/200], Loss: 0.3806\n",
      "Epoch [111/200], Loss: 0.3804\n",
      "Epoch [121/200], Loss: 0.3802\n",
      "Epoch [131/200], Loss: 0.3800\n",
      "Epoch [141/200], Loss: 0.3798\n",
      "Epoch [151/200], Loss: 0.3797\n",
      "Epoch [161/200], Loss: 0.3795\n",
      "Epoch [171/200], Loss: 0.3793\n",
      "Epoch [181/200], Loss: 0.3792\n",
      "Epoch [191/200], Loss: 0.3791\n",
      "Test Accuracy: 70.66%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5184\n",
      "Epoch [11/200], Loss: 0.3673\n",
      "Epoch [21/200], Loss: 0.3665\n",
      "Epoch [31/200], Loss: 0.3659\n",
      "Epoch [41/200], Loss: 0.3654\n",
      "Epoch [51/200], Loss: 0.3649\n",
      "Epoch [61/200], Loss: 0.3645\n",
      "Epoch [71/200], Loss: 0.3642\n",
      "Epoch [81/200], Loss: 0.3638\n",
      "Epoch [91/200], Loss: 0.3635\n",
      "Epoch [101/200], Loss: 0.3632\n",
      "Epoch [111/200], Loss: 0.3629\n",
      "Epoch [121/200], Loss: 0.3626\n",
      "Epoch [131/200], Loss: 0.3623\n",
      "Epoch [141/200], Loss: 0.3620\n",
      "Epoch [151/200], Loss: 0.3618\n",
      "Epoch [161/200], Loss: 0.3615\n",
      "Epoch [171/200], Loss: 0.3613\n",
      "Epoch [181/200], Loss: 0.3611\n",
      "Epoch [191/200], Loss: 0.3608\n",
      "Test Accuracy: 70.52%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5237\n",
      "Epoch [11/200], Loss: 0.3753\n",
      "Epoch [21/200], Loss: 0.3747\n",
      "Epoch [31/200], Loss: 0.3743\n",
      "Epoch [41/200], Loss: 0.3739\n",
      "Epoch [51/200], Loss: 0.3736\n",
      "Epoch [61/200], Loss: 0.3733\n",
      "Epoch [71/200], Loss: 0.3730\n",
      "Epoch [81/200], Loss: 0.3727\n",
      "Epoch [91/200], Loss: 0.3724\n",
      "Epoch [101/200], Loss: 0.3721\n",
      "Epoch [111/200], Loss: 0.3719\n",
      "Epoch [121/200], Loss: 0.3716\n",
      "Epoch [131/200], Loss: 0.3714\n",
      "Epoch [141/200], Loss: 0.3712\n",
      "Epoch [151/200], Loss: 0.3709\n",
      "Epoch [161/200], Loss: 0.3707\n",
      "Epoch [171/200], Loss: 0.3705\n",
      "Epoch [181/200], Loss: 0.3703\n",
      "Epoch [191/200], Loss: 0.3701\n",
      "Test Accuracy: 70.06%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5270\n",
      "Epoch [11/200], Loss: 0.3752\n",
      "Epoch [21/200], Loss: 0.3746\n",
      "Epoch [31/200], Loss: 0.3742\n",
      "Epoch [41/200], Loss: 0.3738\n",
      "Epoch [51/200], Loss: 0.3735\n",
      "Epoch [61/200], Loss: 0.3732\n",
      "Epoch [71/200], Loss: 0.3728\n",
      "Epoch [81/200], Loss: 0.3726\n",
      "Epoch [91/200], Loss: 0.3723\n",
      "Epoch [101/200], Loss: 0.3720\n",
      "Epoch [111/200], Loss: 0.3718\n",
      "Epoch [121/200], Loss: 0.3715\n",
      "Epoch [131/200], Loss: 0.3713\n",
      "Epoch [141/200], Loss: 0.3711\n",
      "Epoch [151/200], Loss: 0.3708\n",
      "Epoch [161/200], Loss: 0.3706\n",
      "Epoch [171/200], Loss: 0.3704\n",
      "Epoch [181/200], Loss: 0.3702\n",
      "Epoch [191/200], Loss: 0.3700\n",
      "Test Accuracy: 69.62%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5577\n",
      "Epoch [11/200], Loss: 0.4057\n",
      "Epoch [21/200], Loss: 0.4053\n",
      "Epoch [31/200], Loss: 0.4050\n",
      "Epoch [41/200], Loss: 0.4047\n",
      "Epoch [51/200], Loss: 0.4044\n",
      "Epoch [61/200], Loss: 0.4042\n",
      "Epoch [71/200], Loss: 0.4040\n",
      "Epoch [81/200], Loss: 0.4038\n",
      "Epoch [91/200], Loss: 0.4036\n",
      "Epoch [101/200], Loss: 0.4035\n",
      "Epoch [111/200], Loss: 0.4033\n",
      "Epoch [121/200], Loss: 0.4031\n",
      "Epoch [131/200], Loss: 0.4030\n",
      "Epoch [141/200], Loss: 0.4028\n",
      "Epoch [151/200], Loss: 0.4027\n",
      "Epoch [161/200], Loss: 0.4026\n",
      "Epoch [171/200], Loss: 0.4025\n",
      "Epoch [181/200], Loss: 0.4023\n",
      "Epoch [191/200], Loss: 0.4022\n",
      "Test Accuracy: 54.37%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5718\n",
      "Epoch [11/200], Loss: 0.4193\n",
      "Epoch [21/200], Loss: 0.4187\n",
      "Epoch [31/200], Loss: 0.4182\n",
      "Epoch [41/200], Loss: 0.4178\n",
      "Epoch [51/200], Loss: 0.4173\n",
      "Epoch [61/200], Loss: 0.4170\n",
      "Epoch [71/200], Loss: 0.4166\n",
      "Epoch [81/200], Loss: 0.4163\n",
      "Epoch [91/200], Loss: 0.4160\n",
      "Epoch [101/200], Loss: 0.4157\n",
      "Epoch [111/200], Loss: 0.4155\n",
      "Epoch [121/200], Loss: 0.4152\n",
      "Epoch [131/200], Loss: 0.4150\n",
      "Epoch [141/200], Loss: 0.4148\n",
      "Epoch [151/200], Loss: 0.4146\n",
      "Epoch [161/200], Loss: 0.4144\n",
      "Epoch [171/200], Loss: 0.4142\n",
      "Epoch [181/200], Loss: 0.4141\n",
      "Epoch [191/200], Loss: 0.4139\n",
      "Test Accuracy: 54.26%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5322\n",
      "Epoch [11/200], Loss: 0.3955\n",
      "Epoch [21/200], Loss: 0.3951\n",
      "Epoch [31/200], Loss: 0.3947\n",
      "Epoch [41/200], Loss: 0.3944\n",
      "Epoch [51/200], Loss: 0.3941\n",
      "Epoch [61/200], Loss: 0.3938\n",
      "Epoch [71/200], Loss: 0.3936\n",
      "Epoch [81/200], Loss: 0.3934\n",
      "Epoch [91/200], Loss: 0.3932\n",
      "Epoch [101/200], Loss: 0.3930\n",
      "Epoch [111/200], Loss: 0.3928\n",
      "Epoch [121/200], Loss: 0.3926\n",
      "Epoch [131/200], Loss: 0.3925\n",
      "Epoch [141/200], Loss: 0.3924\n",
      "Epoch [151/200], Loss: 0.3922\n",
      "Epoch [161/200], Loss: 0.3921\n",
      "Epoch [171/200], Loss: 0.3920\n",
      "Epoch [181/200], Loss: 0.3919\n",
      "Epoch [191/200], Loss: 0.3918\n",
      "Test Accuracy: 54.45%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5304\n",
      "Epoch [11/200], Loss: 0.3950\n",
      "Epoch [21/200], Loss: 0.3945\n",
      "Epoch [31/200], Loss: 0.3942\n",
      "Epoch [41/200], Loss: 0.3939\n",
      "Epoch [51/200], Loss: 0.3936\n",
      "Epoch [61/200], Loss: 0.3934\n",
      "Epoch [71/200], Loss: 0.3932\n",
      "Epoch [81/200], Loss: 0.3931\n",
      "Epoch [91/200], Loss: 0.3929\n",
      "Epoch [101/200], Loss: 0.3927\n",
      "Epoch [111/200], Loss: 0.3926\n",
      "Epoch [121/200], Loss: 0.3924\n",
      "Epoch [131/200], Loss: 0.3923\n",
      "Epoch [141/200], Loss: 0.3922\n",
      "Epoch [151/200], Loss: 0.3920\n",
      "Epoch [161/200], Loss: 0.3919\n",
      "Epoch [171/200], Loss: 0.3918\n",
      "Epoch [181/200], Loss: 0.3917\n",
      "Epoch [191/200], Loss: 0.3916\n",
      "Test Accuracy: 54.44%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5366\n",
      "Epoch [11/200], Loss: 0.3864\n",
      "Epoch [21/200], Loss: 0.3859\n",
      "Epoch [31/200], Loss: 0.3856\n",
      "Epoch [41/200], Loss: 0.3853\n",
      "Epoch [51/200], Loss: 0.3850\n",
      "Epoch [61/200], Loss: 0.3848\n",
      "Epoch [71/200], Loss: 0.3846\n",
      "Epoch [81/200], Loss: 0.3844\n",
      "Epoch [91/200], Loss: 0.3842\n",
      "Epoch [101/200], Loss: 0.3840\n",
      "Epoch [111/200], Loss: 0.3839\n",
      "Epoch [121/200], Loss: 0.3837\n",
      "Epoch [131/200], Loss: 0.3836\n",
      "Epoch [141/200], Loss: 0.3835\n",
      "Epoch [151/200], Loss: 0.3834\n",
      "Epoch [161/200], Loss: 0.3832\n",
      "Epoch [171/200], Loss: 0.3831\n",
      "Epoch [181/200], Loss: 0.3830\n",
      "Epoch [191/200], Loss: 0.3829\n",
      "Test Accuracy: 54.46%\n",
      "Test Accuracy: 64.28%\n",
      "round_93\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5136\n",
      "Epoch [11/200], Loss: 0.3642\n",
      "Epoch [21/200], Loss: 0.3633\n",
      "Epoch [31/200], Loss: 0.3627\n",
      "Epoch [41/200], Loss: 0.3622\n",
      "Epoch [51/200], Loss: 0.3618\n",
      "Epoch [61/200], Loss: 0.3614\n",
      "Epoch [71/200], Loss: 0.3611\n",
      "Epoch [81/200], Loss: 0.3608\n",
      "Epoch [91/200], Loss: 0.3605\n",
      "Epoch [101/200], Loss: 0.3602\n",
      "Epoch [111/200], Loss: 0.3599\n",
      "Epoch [121/200], Loss: 0.3597\n",
      "Epoch [131/200], Loss: 0.3595\n",
      "Epoch [141/200], Loss: 0.3592\n",
      "Epoch [151/200], Loss: 0.3590\n",
      "Epoch [161/200], Loss: 0.3588\n",
      "Epoch [171/200], Loss: 0.3586\n",
      "Epoch [181/200], Loss: 0.3584\n",
      "Epoch [191/200], Loss: 0.3582\n",
      "Test Accuracy: 70.38%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5301\n",
      "Epoch [11/200], Loss: 0.3887\n",
      "Epoch [21/200], Loss: 0.3877\n",
      "Epoch [31/200], Loss: 0.3870\n",
      "Epoch [41/200], Loss: 0.3865\n",
      "Epoch [51/200], Loss: 0.3861\n",
      "Epoch [61/200], Loss: 0.3857\n",
      "Epoch [71/200], Loss: 0.3854\n",
      "Epoch [81/200], Loss: 0.3851\n",
      "Epoch [91/200], Loss: 0.3849\n",
      "Epoch [101/200], Loss: 0.3847\n",
      "Epoch [111/200], Loss: 0.3845\n",
      "Epoch [121/200], Loss: 0.3843\n",
      "Epoch [131/200], Loss: 0.3841\n",
      "Epoch [141/200], Loss: 0.3839\n",
      "Epoch [151/200], Loss: 0.3838\n",
      "Epoch [161/200], Loss: 0.3836\n",
      "Epoch [171/200], Loss: 0.3835\n",
      "Epoch [181/200], Loss: 0.3833\n",
      "Epoch [191/200], Loss: 0.3831\n",
      "Test Accuracy: 70.35%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5185\n",
      "Epoch [11/200], Loss: 0.3760\n",
      "Epoch [21/200], Loss: 0.3756\n",
      "Epoch [31/200], Loss: 0.3753\n",
      "Epoch [41/200], Loss: 0.3751\n",
      "Epoch [51/200], Loss: 0.3748\n",
      "Epoch [61/200], Loss: 0.3746\n",
      "Epoch [71/200], Loss: 0.3743\n",
      "Epoch [81/200], Loss: 0.3741\n",
      "Epoch [91/200], Loss: 0.3739\n",
      "Epoch [101/200], Loss: 0.3737\n",
      "Epoch [111/200], Loss: 0.3735\n",
      "Epoch [121/200], Loss: 0.3733\n",
      "Epoch [131/200], Loss: 0.3731\n",
      "Epoch [141/200], Loss: 0.3729\n",
      "Epoch [151/200], Loss: 0.3728\n",
      "Epoch [161/200], Loss: 0.3726\n",
      "Epoch [171/200], Loss: 0.3724\n",
      "Epoch [181/200], Loss: 0.3723\n",
      "Epoch [191/200], Loss: 0.3721\n",
      "Test Accuracy: 69.93%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5171\n",
      "Epoch [11/200], Loss: 0.3750\n",
      "Epoch [21/200], Loss: 0.3745\n",
      "Epoch [31/200], Loss: 0.3741\n",
      "Epoch [41/200], Loss: 0.3737\n",
      "Epoch [51/200], Loss: 0.3733\n",
      "Epoch [61/200], Loss: 0.3730\n",
      "Epoch [71/200], Loss: 0.3727\n",
      "Epoch [81/200], Loss: 0.3724\n",
      "Epoch [91/200], Loss: 0.3721\n",
      "Epoch [101/200], Loss: 0.3719\n",
      "Epoch [111/200], Loss: 0.3716\n",
      "Epoch [121/200], Loss: 0.3714\n",
      "Epoch [131/200], Loss: 0.3711\n",
      "Epoch [141/200], Loss: 0.3709\n",
      "Epoch [151/200], Loss: 0.3707\n",
      "Epoch [161/200], Loss: 0.3705\n",
      "Epoch [171/200], Loss: 0.3703\n",
      "Epoch [181/200], Loss: 0.3702\n",
      "Epoch [191/200], Loss: 0.3700\n",
      "Test Accuracy: 71.26%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5175\n",
      "Epoch [11/200], Loss: 0.3758\n",
      "Epoch [21/200], Loss: 0.3753\n",
      "Epoch [31/200], Loss: 0.3749\n",
      "Epoch [41/200], Loss: 0.3745\n",
      "Epoch [51/200], Loss: 0.3741\n",
      "Epoch [61/200], Loss: 0.3737\n",
      "Epoch [71/200], Loss: 0.3734\n",
      "Epoch [81/200], Loss: 0.3731\n",
      "Epoch [91/200], Loss: 0.3728\n",
      "Epoch [101/200], Loss: 0.3726\n",
      "Epoch [111/200], Loss: 0.3723\n",
      "Epoch [121/200], Loss: 0.3720\n",
      "Epoch [131/200], Loss: 0.3718\n",
      "Epoch [141/200], Loss: 0.3716\n",
      "Epoch [151/200], Loss: 0.3714\n",
      "Epoch [161/200], Loss: 0.3711\n",
      "Epoch [171/200], Loss: 0.3709\n",
      "Epoch [181/200], Loss: 0.3707\n",
      "Epoch [191/200], Loss: 0.3705\n",
      "Test Accuracy: 70.78%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5388\n",
      "Epoch [11/200], Loss: 0.3939\n",
      "Epoch [21/200], Loss: 0.3934\n",
      "Epoch [31/200], Loss: 0.3930\n",
      "Epoch [41/200], Loss: 0.3927\n",
      "Epoch [51/200], Loss: 0.3924\n",
      "Epoch [61/200], Loss: 0.3921\n",
      "Epoch [71/200], Loss: 0.3918\n",
      "Epoch [81/200], Loss: 0.3916\n",
      "Epoch [91/200], Loss: 0.3914\n",
      "Epoch [101/200], Loss: 0.3912\n",
      "Epoch [111/200], Loss: 0.3911\n",
      "Epoch [121/200], Loss: 0.3909\n",
      "Epoch [131/200], Loss: 0.3907\n",
      "Epoch [141/200], Loss: 0.3906\n",
      "Epoch [151/200], Loss: 0.3905\n",
      "Epoch [161/200], Loss: 0.3903\n",
      "Epoch [171/200], Loss: 0.3902\n",
      "Epoch [181/200], Loss: 0.3901\n",
      "Epoch [191/200], Loss: 0.3900\n",
      "Test Accuracy: 54.28%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5531\n",
      "Epoch [11/200], Loss: 0.4021\n",
      "Epoch [21/200], Loss: 0.4018\n",
      "Epoch [31/200], Loss: 0.4014\n",
      "Epoch [41/200], Loss: 0.4011\n",
      "Epoch [51/200], Loss: 0.4009\n",
      "Epoch [61/200], Loss: 0.4006\n",
      "Epoch [71/200], Loss: 0.4004\n",
      "Epoch [81/200], Loss: 0.4002\n",
      "Epoch [91/200], Loss: 0.4000\n",
      "Epoch [101/200], Loss: 0.3998\n",
      "Epoch [111/200], Loss: 0.3996\n",
      "Epoch [121/200], Loss: 0.3995\n",
      "Epoch [131/200], Loss: 0.3993\n",
      "Epoch [141/200], Loss: 0.3991\n",
      "Epoch [151/200], Loss: 0.3990\n",
      "Epoch [161/200], Loss: 0.3988\n",
      "Epoch [171/200], Loss: 0.3987\n",
      "Epoch [181/200], Loss: 0.3986\n",
      "Epoch [191/200], Loss: 0.3984\n",
      "Test Accuracy: 54.49%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5860\n",
      "Epoch [11/200], Loss: 0.4192\n",
      "Epoch [21/200], Loss: 0.4185\n",
      "Epoch [31/200], Loss: 0.4180\n",
      "Epoch [41/200], Loss: 0.4175\n",
      "Epoch [51/200], Loss: 0.4171\n",
      "Epoch [61/200], Loss: 0.4167\n",
      "Epoch [71/200], Loss: 0.4164\n",
      "Epoch [81/200], Loss: 0.4161\n",
      "Epoch [91/200], Loss: 0.4159\n",
      "Epoch [101/200], Loss: 0.4156\n",
      "Epoch [111/200], Loss: 0.4154\n",
      "Epoch [121/200], Loss: 0.4152\n",
      "Epoch [131/200], Loss: 0.4150\n",
      "Epoch [141/200], Loss: 0.4148\n",
      "Epoch [151/200], Loss: 0.4146\n",
      "Epoch [161/200], Loss: 0.4145\n",
      "Epoch [171/200], Loss: 0.4143\n",
      "Epoch [181/200], Loss: 0.4141\n",
      "Epoch [191/200], Loss: 0.4140\n",
      "Test Accuracy: 54.24%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5555\n",
      "Epoch [11/200], Loss: 0.4158\n",
      "Epoch [21/200], Loss: 0.4149\n",
      "Epoch [31/200], Loss: 0.4141\n",
      "Epoch [41/200], Loss: 0.4135\n",
      "Epoch [51/200], Loss: 0.4129\n",
      "Epoch [61/200], Loss: 0.4125\n",
      "Epoch [71/200], Loss: 0.4120\n",
      "Epoch [81/200], Loss: 0.4117\n",
      "Epoch [91/200], Loss: 0.4113\n",
      "Epoch [101/200], Loss: 0.4110\n",
      "Epoch [111/200], Loss: 0.4107\n",
      "Epoch [121/200], Loss: 0.4104\n",
      "Epoch [131/200], Loss: 0.4101\n",
      "Epoch [141/200], Loss: 0.4099\n",
      "Epoch [151/200], Loss: 0.4097\n",
      "Epoch [161/200], Loss: 0.4095\n",
      "Epoch [171/200], Loss: 0.4093\n",
      "Epoch [181/200], Loss: 0.4091\n",
      "Epoch [191/200], Loss: 0.4089\n",
      "Test Accuracy: 54.31%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5514\n",
      "Epoch [11/200], Loss: 0.4092\n",
      "Epoch [21/200], Loss: 0.4086\n",
      "Epoch [31/200], Loss: 0.4081\n",
      "Epoch [41/200], Loss: 0.4076\n",
      "Epoch [51/200], Loss: 0.4072\n",
      "Epoch [61/200], Loss: 0.4068\n",
      "Epoch [71/200], Loss: 0.4065\n",
      "Epoch [81/200], Loss: 0.4062\n",
      "Epoch [91/200], Loss: 0.4059\n",
      "Epoch [101/200], Loss: 0.4057\n",
      "Epoch [111/200], Loss: 0.4054\n",
      "Epoch [121/200], Loss: 0.4052\n",
      "Epoch [131/200], Loss: 0.4050\n",
      "Epoch [141/200], Loss: 0.4048\n",
      "Epoch [151/200], Loss: 0.4046\n",
      "Epoch [161/200], Loss: 0.4044\n",
      "Epoch [171/200], Loss: 0.4042\n",
      "Epoch [181/200], Loss: 0.4041\n",
      "Epoch [191/200], Loss: 0.4039\n",
      "Test Accuracy: 54.52%\n",
      "Test Accuracy: 64.59%\n",
      "round_94\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5179\n",
      "Epoch [11/200], Loss: 0.3610\n",
      "Epoch [21/200], Loss: 0.3603\n",
      "Epoch [31/200], Loss: 0.3597\n",
      "Epoch [41/200], Loss: 0.3592\n",
      "Epoch [51/200], Loss: 0.3587\n",
      "Epoch [61/200], Loss: 0.3583\n",
      "Epoch [71/200], Loss: 0.3579\n",
      "Epoch [81/200], Loss: 0.3575\n",
      "Epoch [91/200], Loss: 0.3572\n",
      "Epoch [101/200], Loss: 0.3569\n",
      "Epoch [111/200], Loss: 0.3565\n",
      "Epoch [121/200], Loss: 0.3562\n",
      "Epoch [131/200], Loss: 0.3559\n",
      "Epoch [141/200], Loss: 0.3557\n",
      "Epoch [151/200], Loss: 0.3554\n",
      "Epoch [161/200], Loss: 0.3551\n",
      "Epoch [171/200], Loss: 0.3548\n",
      "Epoch [181/200], Loss: 0.3546\n",
      "Epoch [191/200], Loss: 0.3543\n",
      "Test Accuracy: 69.74%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5071\n",
      "Epoch [11/200], Loss: 0.3619\n",
      "Epoch [21/200], Loss: 0.3612\n",
      "Epoch [31/200], Loss: 0.3606\n",
      "Epoch [41/200], Loss: 0.3601\n",
      "Epoch [51/200], Loss: 0.3596\n",
      "Epoch [61/200], Loss: 0.3591\n",
      "Epoch [71/200], Loss: 0.3587\n",
      "Epoch [81/200], Loss: 0.3583\n",
      "Epoch [91/200], Loss: 0.3579\n",
      "Epoch [101/200], Loss: 0.3576\n",
      "Epoch [111/200], Loss: 0.3573\n",
      "Epoch [121/200], Loss: 0.3570\n",
      "Epoch [131/200], Loss: 0.3567\n",
      "Epoch [141/200], Loss: 0.3564\n",
      "Epoch [151/200], Loss: 0.3562\n",
      "Epoch [161/200], Loss: 0.3559\n",
      "Epoch [171/200], Loss: 0.3557\n",
      "Epoch [181/200], Loss: 0.3554\n",
      "Epoch [191/200], Loss: 0.3552\n",
      "Test Accuracy: 71.44%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5207\n",
      "Epoch [11/200], Loss: 0.3577\n",
      "Epoch [21/200], Loss: 0.3570\n",
      "Epoch [31/200], Loss: 0.3565\n",
      "Epoch [41/200], Loss: 0.3561\n",
      "Epoch [51/200], Loss: 0.3558\n",
      "Epoch [61/200], Loss: 0.3555\n",
      "Epoch [71/200], Loss: 0.3552\n",
      "Epoch [81/200], Loss: 0.3549\n",
      "Epoch [91/200], Loss: 0.3546\n",
      "Epoch [101/200], Loss: 0.3544\n",
      "Epoch [111/200], Loss: 0.3541\n",
      "Epoch [121/200], Loss: 0.3539\n",
      "Epoch [131/200], Loss: 0.3537\n",
      "Epoch [141/200], Loss: 0.3534\n",
      "Epoch [151/200], Loss: 0.3532\n",
      "Epoch [161/200], Loss: 0.3530\n",
      "Epoch [171/200], Loss: 0.3528\n",
      "Epoch [181/200], Loss: 0.3526\n",
      "Epoch [191/200], Loss: 0.3524\n",
      "Test Accuracy: 69.15%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5315\n",
      "Epoch [11/200], Loss: 0.3924\n",
      "Epoch [21/200], Loss: 0.3918\n",
      "Epoch [31/200], Loss: 0.3915\n",
      "Epoch [41/200], Loss: 0.3912\n",
      "Epoch [51/200], Loss: 0.3910\n",
      "Epoch [61/200], Loss: 0.3908\n",
      "Epoch [71/200], Loss: 0.3906\n",
      "Epoch [81/200], Loss: 0.3904\n",
      "Epoch [91/200], Loss: 0.3902\n",
      "Epoch [101/200], Loss: 0.3900\n",
      "Epoch [111/200], Loss: 0.3899\n",
      "Epoch [121/200], Loss: 0.3897\n",
      "Epoch [131/200], Loss: 0.3895\n",
      "Epoch [141/200], Loss: 0.3894\n",
      "Epoch [151/200], Loss: 0.3892\n",
      "Epoch [161/200], Loss: 0.3891\n",
      "Epoch [171/200], Loss: 0.3889\n",
      "Epoch [181/200], Loss: 0.3887\n",
      "Epoch [191/200], Loss: 0.3886\n",
      "Test Accuracy: 69.97%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5102\n",
      "Epoch [11/200], Loss: 0.3524\n",
      "Epoch [21/200], Loss: 0.3515\n",
      "Epoch [31/200], Loss: 0.3508\n",
      "Epoch [41/200], Loss: 0.3501\n",
      "Epoch [51/200], Loss: 0.3494\n",
      "Epoch [61/200], Loss: 0.3488\n",
      "Epoch [71/200], Loss: 0.3483\n",
      "Epoch [81/200], Loss: 0.3478\n",
      "Epoch [91/200], Loss: 0.3473\n",
      "Epoch [101/200], Loss: 0.3469\n",
      "Epoch [111/200], Loss: 0.3465\n",
      "Epoch [121/200], Loss: 0.3461\n",
      "Epoch [131/200], Loss: 0.3457\n",
      "Epoch [141/200], Loss: 0.3453\n",
      "Epoch [151/200], Loss: 0.3450\n",
      "Epoch [161/200], Loss: 0.3447\n",
      "Epoch [171/200], Loss: 0.3443\n",
      "Epoch [181/200], Loss: 0.3441\n",
      "Epoch [191/200], Loss: 0.3438\n",
      "Test Accuracy: 70.60%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5510\n",
      "Epoch [11/200], Loss: 0.4094\n",
      "Epoch [21/200], Loss: 0.4086\n",
      "Epoch [31/200], Loss: 0.4081\n",
      "Epoch [41/200], Loss: 0.4077\n",
      "Epoch [51/200], Loss: 0.4073\n",
      "Epoch [61/200], Loss: 0.4070\n",
      "Epoch [71/200], Loss: 0.4067\n",
      "Epoch [81/200], Loss: 0.4065\n",
      "Epoch [91/200], Loss: 0.4062\n",
      "Epoch [101/200], Loss: 0.4060\n",
      "Epoch [111/200], Loss: 0.4058\n",
      "Epoch [121/200], Loss: 0.4056\n",
      "Epoch [131/200], Loss: 0.4054\n",
      "Epoch [141/200], Loss: 0.4052\n",
      "Epoch [151/200], Loss: 0.4050\n",
      "Epoch [161/200], Loss: 0.4049\n",
      "Epoch [171/200], Loss: 0.4047\n",
      "Epoch [181/200], Loss: 0.4046\n",
      "Epoch [191/200], Loss: 0.4044\n",
      "Test Accuracy: 54.05%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5457\n",
      "Epoch [11/200], Loss: 0.4068\n",
      "Epoch [21/200], Loss: 0.4059\n",
      "Epoch [31/200], Loss: 0.4053\n",
      "Epoch [41/200], Loss: 0.4048\n",
      "Epoch [51/200], Loss: 0.4043\n",
      "Epoch [61/200], Loss: 0.4039\n",
      "Epoch [71/200], Loss: 0.4035\n",
      "Epoch [81/200], Loss: 0.4031\n",
      "Epoch [91/200], Loss: 0.4028\n",
      "Epoch [101/200], Loss: 0.4025\n",
      "Epoch [111/200], Loss: 0.4022\n",
      "Epoch [121/200], Loss: 0.4019\n",
      "Epoch [131/200], Loss: 0.4017\n",
      "Epoch [141/200], Loss: 0.4014\n",
      "Epoch [151/200], Loss: 0.4012\n",
      "Epoch [161/200], Loss: 0.4010\n",
      "Epoch [171/200], Loss: 0.4008\n",
      "Epoch [181/200], Loss: 0.4006\n",
      "Epoch [191/200], Loss: 0.4004\n",
      "Test Accuracy: 54.00%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5160\n",
      "Epoch [11/200], Loss: 0.3855\n",
      "Epoch [21/200], Loss: 0.3851\n",
      "Epoch [31/200], Loss: 0.3847\n",
      "Epoch [41/200], Loss: 0.3845\n",
      "Epoch [51/200], Loss: 0.3842\n",
      "Epoch [61/200], Loss: 0.3840\n",
      "Epoch [71/200], Loss: 0.3837\n",
      "Epoch [81/200], Loss: 0.3835\n",
      "Epoch [91/200], Loss: 0.3833\n",
      "Epoch [101/200], Loss: 0.3832\n",
      "Epoch [111/200], Loss: 0.3830\n",
      "Epoch [121/200], Loss: 0.3829\n",
      "Epoch [131/200], Loss: 0.3827\n",
      "Epoch [141/200], Loss: 0.3826\n",
      "Epoch [151/200], Loss: 0.3824\n",
      "Epoch [161/200], Loss: 0.3823\n",
      "Epoch [171/200], Loss: 0.3822\n",
      "Epoch [181/200], Loss: 0.3820\n",
      "Epoch [191/200], Loss: 0.3819\n",
      "Test Accuracy: 53.65%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5394\n",
      "Epoch [11/200], Loss: 0.4024\n",
      "Epoch [21/200], Loss: 0.4019\n",
      "Epoch [31/200], Loss: 0.4015\n",
      "Epoch [41/200], Loss: 0.4012\n",
      "Epoch [51/200], Loss: 0.4009\n",
      "Epoch [61/200], Loss: 0.4006\n",
      "Epoch [71/200], Loss: 0.4004\n",
      "Epoch [81/200], Loss: 0.4002\n",
      "Epoch [91/200], Loss: 0.4000\n",
      "Epoch [101/200], Loss: 0.3998\n",
      "Epoch [111/200], Loss: 0.3996\n",
      "Epoch [121/200], Loss: 0.3995\n",
      "Epoch [131/200], Loss: 0.3993\n",
      "Epoch [141/200], Loss: 0.3992\n",
      "Epoch [151/200], Loss: 0.3990\n",
      "Epoch [161/200], Loss: 0.3989\n",
      "Epoch [171/200], Loss: 0.3988\n",
      "Epoch [181/200], Loss: 0.3987\n",
      "Epoch [191/200], Loss: 0.3986\n",
      "Test Accuracy: 54.52%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5717\n",
      "Epoch [11/200], Loss: 0.4145\n",
      "Epoch [21/200], Loss: 0.4138\n",
      "Epoch [31/200], Loss: 0.4132\n",
      "Epoch [41/200], Loss: 0.4126\n",
      "Epoch [51/200], Loss: 0.4122\n",
      "Epoch [61/200], Loss: 0.4118\n",
      "Epoch [71/200], Loss: 0.4114\n",
      "Epoch [81/200], Loss: 0.4111\n",
      "Epoch [91/200], Loss: 0.4108\n",
      "Epoch [101/200], Loss: 0.4105\n",
      "Epoch [111/200], Loss: 0.4102\n",
      "Epoch [121/200], Loss: 0.4100\n",
      "Epoch [131/200], Loss: 0.4097\n",
      "Epoch [141/200], Loss: 0.4095\n",
      "Epoch [151/200], Loss: 0.4093\n",
      "Epoch [161/200], Loss: 0.4091\n",
      "Epoch [171/200], Loss: 0.4089\n",
      "Epoch [181/200], Loss: 0.4087\n",
      "Epoch [191/200], Loss: 0.4086\n",
      "Test Accuracy: 54.06%\n",
      "Test Accuracy: 65.65%\n",
      "round_95\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5208\n",
      "Epoch [11/200], Loss: 0.3879\n",
      "Epoch [21/200], Loss: 0.3872\n",
      "Epoch [31/200], Loss: 0.3867\n",
      "Epoch [41/200], Loss: 0.3864\n",
      "Epoch [51/200], Loss: 0.3862\n",
      "Epoch [61/200], Loss: 0.3859\n",
      "Epoch [71/200], Loss: 0.3857\n",
      "Epoch [81/200], Loss: 0.3854\n",
      "Epoch [91/200], Loss: 0.3852\n",
      "Epoch [101/200], Loss: 0.3850\n",
      "Epoch [111/200], Loss: 0.3848\n",
      "Epoch [121/200], Loss: 0.3846\n",
      "Epoch [131/200], Loss: 0.3844\n",
      "Epoch [141/200], Loss: 0.3842\n",
      "Epoch [151/200], Loss: 0.3840\n",
      "Epoch [161/200], Loss: 0.3839\n",
      "Epoch [171/200], Loss: 0.3837\n",
      "Epoch [181/200], Loss: 0.3835\n",
      "Epoch [191/200], Loss: 0.3833\n",
      "Test Accuracy: 69.99%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5011\n",
      "Epoch [11/200], Loss: 0.3672\n",
      "Epoch [21/200], Loss: 0.3666\n",
      "Epoch [31/200], Loss: 0.3661\n",
      "Epoch [41/200], Loss: 0.3657\n",
      "Epoch [51/200], Loss: 0.3653\n",
      "Epoch [61/200], Loss: 0.3649\n",
      "Epoch [71/200], Loss: 0.3645\n",
      "Epoch [81/200], Loss: 0.3642\n",
      "Epoch [91/200], Loss: 0.3639\n",
      "Epoch [101/200], Loss: 0.3636\n",
      "Epoch [111/200], Loss: 0.3633\n",
      "Epoch [121/200], Loss: 0.3630\n",
      "Epoch [131/200], Loss: 0.3628\n",
      "Epoch [141/200], Loss: 0.3625\n",
      "Epoch [151/200], Loss: 0.3623\n",
      "Epoch [161/200], Loss: 0.3621\n",
      "Epoch [171/200], Loss: 0.3618\n",
      "Epoch [181/200], Loss: 0.3616\n",
      "Epoch [191/200], Loss: 0.3614\n",
      "Test Accuracy: 71.27%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5166\n",
      "Epoch [11/200], Loss: 0.3840\n",
      "Epoch [21/200], Loss: 0.3834\n",
      "Epoch [31/200], Loss: 0.3829\n",
      "Epoch [41/200], Loss: 0.3826\n",
      "Epoch [51/200], Loss: 0.3822\n",
      "Epoch [61/200], Loss: 0.3819\n",
      "Epoch [71/200], Loss: 0.3816\n",
      "Epoch [81/200], Loss: 0.3814\n",
      "Epoch [91/200], Loss: 0.3811\n",
      "Epoch [101/200], Loss: 0.3809\n",
      "Epoch [111/200], Loss: 0.3807\n",
      "Epoch [121/200], Loss: 0.3805\n",
      "Epoch [131/200], Loss: 0.3803\n",
      "Epoch [141/200], Loss: 0.3801\n",
      "Epoch [151/200], Loss: 0.3799\n",
      "Epoch [161/200], Loss: 0.3797\n",
      "Epoch [171/200], Loss: 0.3795\n",
      "Epoch [181/200], Loss: 0.3793\n",
      "Epoch [191/200], Loss: 0.3792\n",
      "Test Accuracy: 70.85%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5170\n",
      "Epoch [11/200], Loss: 0.3803\n",
      "Epoch [21/200], Loss: 0.3799\n",
      "Epoch [31/200], Loss: 0.3795\n",
      "Epoch [41/200], Loss: 0.3792\n",
      "Epoch [51/200], Loss: 0.3789\n",
      "Epoch [61/200], Loss: 0.3787\n",
      "Epoch [71/200], Loss: 0.3784\n",
      "Epoch [81/200], Loss: 0.3782\n",
      "Epoch [91/200], Loss: 0.3779\n",
      "Epoch [101/200], Loss: 0.3777\n",
      "Epoch [111/200], Loss: 0.3775\n",
      "Epoch [121/200], Loss: 0.3773\n",
      "Epoch [131/200], Loss: 0.3770\n",
      "Epoch [141/200], Loss: 0.3768\n",
      "Epoch [151/200], Loss: 0.3766\n",
      "Epoch [161/200], Loss: 0.3764\n",
      "Epoch [171/200], Loss: 0.3762\n",
      "Epoch [181/200], Loss: 0.3760\n",
      "Epoch [191/200], Loss: 0.3758\n",
      "Test Accuracy: 70.30%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5004\n",
      "Epoch [11/200], Loss: 0.3620\n",
      "Epoch [21/200], Loss: 0.3610\n",
      "Epoch [31/200], Loss: 0.3601\n",
      "Epoch [41/200], Loss: 0.3593\n",
      "Epoch [51/200], Loss: 0.3586\n",
      "Epoch [61/200], Loss: 0.3580\n",
      "Epoch [71/200], Loss: 0.3574\n",
      "Epoch [81/200], Loss: 0.3569\n",
      "Epoch [91/200], Loss: 0.3564\n",
      "Epoch [101/200], Loss: 0.3560\n",
      "Epoch [111/200], Loss: 0.3555\n",
      "Epoch [121/200], Loss: 0.3552\n",
      "Epoch [131/200], Loss: 0.3548\n",
      "Epoch [141/200], Loss: 0.3544\n",
      "Epoch [151/200], Loss: 0.3541\n",
      "Epoch [161/200], Loss: 0.3538\n",
      "Epoch [171/200], Loss: 0.3535\n",
      "Epoch [181/200], Loss: 0.3532\n",
      "Epoch [191/200], Loss: 0.3529\n",
      "Test Accuracy: 71.55%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5785\n",
      "Epoch [11/200], Loss: 0.4168\n",
      "Epoch [21/200], Loss: 0.4162\n",
      "Epoch [31/200], Loss: 0.4158\n",
      "Epoch [41/200], Loss: 0.4154\n",
      "Epoch [51/200], Loss: 0.4151\n",
      "Epoch [61/200], Loss: 0.4148\n",
      "Epoch [71/200], Loss: 0.4146\n",
      "Epoch [81/200], Loss: 0.4144\n",
      "Epoch [91/200], Loss: 0.4141\n",
      "Epoch [101/200], Loss: 0.4139\n",
      "Epoch [111/200], Loss: 0.4137\n",
      "Epoch [121/200], Loss: 0.4135\n",
      "Epoch [131/200], Loss: 0.4133\n",
      "Epoch [141/200], Loss: 0.4132\n",
      "Epoch [151/200], Loss: 0.4130\n",
      "Epoch [161/200], Loss: 0.4128\n",
      "Epoch [171/200], Loss: 0.4127\n",
      "Epoch [181/200], Loss: 0.4125\n",
      "Epoch [191/200], Loss: 0.4124\n",
      "Test Accuracy: 54.23%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5509\n",
      "Epoch [11/200], Loss: 0.3981\n",
      "Epoch [21/200], Loss: 0.3976\n",
      "Epoch [31/200], Loss: 0.3972\n",
      "Epoch [41/200], Loss: 0.3969\n",
      "Epoch [51/200], Loss: 0.3967\n",
      "Epoch [61/200], Loss: 0.3964\n",
      "Epoch [71/200], Loss: 0.3962\n",
      "Epoch [81/200], Loss: 0.3960\n",
      "Epoch [91/200], Loss: 0.3958\n",
      "Epoch [101/200], Loss: 0.3956\n",
      "Epoch [111/200], Loss: 0.3954\n",
      "Epoch [121/200], Loss: 0.3952\n",
      "Epoch [131/200], Loss: 0.3950\n",
      "Epoch [141/200], Loss: 0.3948\n",
      "Epoch [151/200], Loss: 0.3946\n",
      "Epoch [161/200], Loss: 0.3945\n",
      "Epoch [171/200], Loss: 0.3943\n",
      "Epoch [181/200], Loss: 0.3942\n",
      "Epoch [191/200], Loss: 0.3940\n",
      "Test Accuracy: 54.35%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5383\n",
      "Epoch [11/200], Loss: 0.3967\n",
      "Epoch [21/200], Loss: 0.3962\n",
      "Epoch [31/200], Loss: 0.3958\n",
      "Epoch [41/200], Loss: 0.3955\n",
      "Epoch [51/200], Loss: 0.3952\n",
      "Epoch [61/200], Loss: 0.3949\n",
      "Epoch [71/200], Loss: 0.3946\n",
      "Epoch [81/200], Loss: 0.3944\n",
      "Epoch [91/200], Loss: 0.3942\n",
      "Epoch [101/200], Loss: 0.3940\n",
      "Epoch [111/200], Loss: 0.3938\n",
      "Epoch [121/200], Loss: 0.3936\n",
      "Epoch [131/200], Loss: 0.3934\n",
      "Epoch [141/200], Loss: 0.3933\n",
      "Epoch [151/200], Loss: 0.3931\n",
      "Epoch [161/200], Loss: 0.3929\n",
      "Epoch [171/200], Loss: 0.3928\n",
      "Epoch [181/200], Loss: 0.3927\n",
      "Epoch [191/200], Loss: 0.3925\n",
      "Test Accuracy: 54.11%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5934\n",
      "Epoch [11/200], Loss: 0.4166\n",
      "Epoch [21/200], Loss: 0.4157\n",
      "Epoch [31/200], Loss: 0.4151\n",
      "Epoch [41/200], Loss: 0.4146\n",
      "Epoch [51/200], Loss: 0.4142\n",
      "Epoch [61/200], Loss: 0.4139\n",
      "Epoch [71/200], Loss: 0.4135\n",
      "Epoch [81/200], Loss: 0.4132\n",
      "Epoch [91/200], Loss: 0.4129\n",
      "Epoch [101/200], Loss: 0.4127\n",
      "Epoch [111/200], Loss: 0.4124\n",
      "Epoch [121/200], Loss: 0.4121\n",
      "Epoch [131/200], Loss: 0.4119\n",
      "Epoch [141/200], Loss: 0.4117\n",
      "Epoch [151/200], Loss: 0.4115\n",
      "Epoch [161/200], Loss: 0.4113\n",
      "Epoch [171/200], Loss: 0.4111\n",
      "Epoch [181/200], Loss: 0.4109\n",
      "Epoch [191/200], Loss: 0.4107\n",
      "Test Accuracy: 53.89%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5470\n",
      "Epoch [11/200], Loss: 0.3967\n",
      "Epoch [21/200], Loss: 0.3962\n",
      "Epoch [31/200], Loss: 0.3959\n",
      "Epoch [41/200], Loss: 0.3955\n",
      "Epoch [51/200], Loss: 0.3952\n",
      "Epoch [61/200], Loss: 0.3949\n",
      "Epoch [71/200], Loss: 0.3947\n",
      "Epoch [81/200], Loss: 0.3944\n",
      "Epoch [91/200], Loss: 0.3942\n",
      "Epoch [101/200], Loss: 0.3940\n",
      "Epoch [111/200], Loss: 0.3938\n",
      "Epoch [121/200], Loss: 0.3936\n",
      "Epoch [131/200], Loss: 0.3934\n",
      "Epoch [141/200], Loss: 0.3933\n",
      "Epoch [151/200], Loss: 0.3931\n",
      "Epoch [161/200], Loss: 0.3929\n",
      "Epoch [171/200], Loss: 0.3928\n",
      "Epoch [181/200], Loss: 0.3926\n",
      "Epoch [191/200], Loss: 0.3925\n",
      "Test Accuracy: 54.38%\n",
      "Test Accuracy: 65.29%\n",
      "round_96\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5059\n",
      "Epoch [11/200], Loss: 0.3566\n",
      "Epoch [21/200], Loss: 0.3554\n",
      "Epoch [31/200], Loss: 0.3544\n",
      "Epoch [41/200], Loss: 0.3536\n",
      "Epoch [51/200], Loss: 0.3529\n",
      "Epoch [61/200], Loss: 0.3524\n",
      "Epoch [71/200], Loss: 0.3518\n",
      "Epoch [81/200], Loss: 0.3514\n",
      "Epoch [91/200], Loss: 0.3509\n",
      "Epoch [101/200], Loss: 0.3505\n",
      "Epoch [111/200], Loss: 0.3502\n",
      "Epoch [121/200], Loss: 0.3498\n",
      "Epoch [131/200], Loss: 0.3495\n",
      "Epoch [141/200], Loss: 0.3492\n",
      "Epoch [151/200], Loss: 0.3489\n",
      "Epoch [161/200], Loss: 0.3486\n",
      "Epoch [171/200], Loss: 0.3484\n",
      "Epoch [181/200], Loss: 0.3481\n",
      "Epoch [191/200], Loss: 0.3479\n",
      "Test Accuracy: 71.27%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5366\n",
      "Epoch [11/200], Loss: 0.3889\n",
      "Epoch [21/200], Loss: 0.3880\n",
      "Epoch [31/200], Loss: 0.3874\n",
      "Epoch [41/200], Loss: 0.3870\n",
      "Epoch [51/200], Loss: 0.3866\n",
      "Epoch [61/200], Loss: 0.3862\n",
      "Epoch [71/200], Loss: 0.3859\n",
      "Epoch [81/200], Loss: 0.3855\n",
      "Epoch [91/200], Loss: 0.3852\n",
      "Epoch [101/200], Loss: 0.3849\n",
      "Epoch [111/200], Loss: 0.3847\n",
      "Epoch [121/200], Loss: 0.3844\n",
      "Epoch [131/200], Loss: 0.3841\n",
      "Epoch [141/200], Loss: 0.3839\n",
      "Epoch [151/200], Loss: 0.3836\n",
      "Epoch [161/200], Loss: 0.3834\n",
      "Epoch [171/200], Loss: 0.3832\n",
      "Epoch [181/200], Loss: 0.3830\n",
      "Epoch [191/200], Loss: 0.3828\n",
      "Test Accuracy: 69.30%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5122\n",
      "Epoch [11/200], Loss: 0.3702\n",
      "Epoch [21/200], Loss: 0.3694\n",
      "Epoch [31/200], Loss: 0.3688\n",
      "Epoch [41/200], Loss: 0.3683\n",
      "Epoch [51/200], Loss: 0.3679\n",
      "Epoch [61/200], Loss: 0.3675\n",
      "Epoch [71/200], Loss: 0.3671\n",
      "Epoch [81/200], Loss: 0.3668\n",
      "Epoch [91/200], Loss: 0.3665\n",
      "Epoch [101/200], Loss: 0.3662\n",
      "Epoch [111/200], Loss: 0.3660\n",
      "Epoch [121/200], Loss: 0.3657\n",
      "Epoch [131/200], Loss: 0.3654\n",
      "Epoch [141/200], Loss: 0.3652\n",
      "Epoch [151/200], Loss: 0.3650\n",
      "Epoch [161/200], Loss: 0.3647\n",
      "Epoch [171/200], Loss: 0.3645\n",
      "Epoch [181/200], Loss: 0.3642\n",
      "Epoch [191/200], Loss: 0.3640\n",
      "Test Accuracy: 70.94%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5200\n",
      "Epoch [11/200], Loss: 0.3923\n",
      "Epoch [21/200], Loss: 0.3917\n",
      "Epoch [31/200], Loss: 0.3913\n",
      "Epoch [41/200], Loss: 0.3910\n",
      "Epoch [51/200], Loss: 0.3906\n",
      "Epoch [61/200], Loss: 0.3903\n",
      "Epoch [71/200], Loss: 0.3900\n",
      "Epoch [81/200], Loss: 0.3897\n",
      "Epoch [91/200], Loss: 0.3895\n",
      "Epoch [101/200], Loss: 0.3892\n",
      "Epoch [111/200], Loss: 0.3890\n",
      "Epoch [121/200], Loss: 0.3888\n",
      "Epoch [131/200], Loss: 0.3886\n",
      "Epoch [141/200], Loss: 0.3884\n",
      "Epoch [151/200], Loss: 0.3882\n",
      "Epoch [161/200], Loss: 0.3881\n",
      "Epoch [171/200], Loss: 0.3879\n",
      "Epoch [181/200], Loss: 0.3877\n",
      "Epoch [191/200], Loss: 0.3876\n",
      "Test Accuracy: 70.93%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5263\n",
      "Epoch [11/200], Loss: 0.3899\n",
      "Epoch [21/200], Loss: 0.3886\n",
      "Epoch [31/200], Loss: 0.3881\n",
      "Epoch [41/200], Loss: 0.3878\n",
      "Epoch [51/200], Loss: 0.3875\n",
      "Epoch [61/200], Loss: 0.3873\n",
      "Epoch [71/200], Loss: 0.3870\n",
      "Epoch [81/200], Loss: 0.3869\n",
      "Epoch [91/200], Loss: 0.3867\n",
      "Epoch [101/200], Loss: 0.3865\n",
      "Epoch [111/200], Loss: 0.3863\n",
      "Epoch [121/200], Loss: 0.3862\n",
      "Epoch [131/200], Loss: 0.3860\n",
      "Epoch [141/200], Loss: 0.3859\n",
      "Epoch [151/200], Loss: 0.3857\n",
      "Epoch [161/200], Loss: 0.3856\n",
      "Epoch [171/200], Loss: 0.3854\n",
      "Epoch [181/200], Loss: 0.3853\n",
      "Epoch [191/200], Loss: 0.3851\n",
      "Test Accuracy: 69.64%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5822\n",
      "Epoch [11/200], Loss: 0.4154\n",
      "Epoch [21/200], Loss: 0.4143\n",
      "Epoch [31/200], Loss: 0.4137\n",
      "Epoch [41/200], Loss: 0.4132\n",
      "Epoch [51/200], Loss: 0.4127\n",
      "Epoch [61/200], Loss: 0.4124\n",
      "Epoch [71/200], Loss: 0.4120\n",
      "Epoch [81/200], Loss: 0.4117\n",
      "Epoch [91/200], Loss: 0.4114\n",
      "Epoch [101/200], Loss: 0.4111\n",
      "Epoch [111/200], Loss: 0.4108\n",
      "Epoch [121/200], Loss: 0.4105\n",
      "Epoch [131/200], Loss: 0.4103\n",
      "Epoch [141/200], Loss: 0.4100\n",
      "Epoch [151/200], Loss: 0.4098\n",
      "Epoch [161/200], Loss: 0.4096\n",
      "Epoch [171/200], Loss: 0.4094\n",
      "Epoch [181/200], Loss: 0.4092\n",
      "Epoch [191/200], Loss: 0.4090\n",
      "Test Accuracy: 53.69%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5086\n",
      "Epoch [11/200], Loss: 0.3884\n",
      "Epoch [21/200], Loss: 0.3878\n",
      "Epoch [31/200], Loss: 0.3874\n",
      "Epoch [41/200], Loss: 0.3870\n",
      "Epoch [51/200], Loss: 0.3866\n",
      "Epoch [61/200], Loss: 0.3863\n",
      "Epoch [71/200], Loss: 0.3860\n",
      "Epoch [81/200], Loss: 0.3857\n",
      "Epoch [91/200], Loss: 0.3854\n",
      "Epoch [101/200], Loss: 0.3852\n",
      "Epoch [111/200], Loss: 0.3849\n",
      "Epoch [121/200], Loss: 0.3847\n",
      "Epoch [131/200], Loss: 0.3844\n",
      "Epoch [141/200], Loss: 0.3842\n",
      "Epoch [151/200], Loss: 0.3840\n",
      "Epoch [161/200], Loss: 0.3838\n",
      "Epoch [171/200], Loss: 0.3836\n",
      "Epoch [181/200], Loss: 0.3834\n",
      "Epoch [191/200], Loss: 0.3833\n",
      "Test Accuracy: 54.14%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5554\n",
      "Epoch [11/200], Loss: 0.4086\n",
      "Epoch [21/200], Loss: 0.4078\n",
      "Epoch [31/200], Loss: 0.4072\n",
      "Epoch [41/200], Loss: 0.4067\n",
      "Epoch [51/200], Loss: 0.4062\n",
      "Epoch [61/200], Loss: 0.4057\n",
      "Epoch [71/200], Loss: 0.4053\n",
      "Epoch [81/200], Loss: 0.4050\n",
      "Epoch [91/200], Loss: 0.4046\n",
      "Epoch [101/200], Loss: 0.4043\n",
      "Epoch [111/200], Loss: 0.4040\n",
      "Epoch [121/200], Loss: 0.4037\n",
      "Epoch [131/200], Loss: 0.4034\n",
      "Epoch [141/200], Loss: 0.4031\n",
      "Epoch [151/200], Loss: 0.4029\n",
      "Epoch [161/200], Loss: 0.4026\n",
      "Epoch [171/200], Loss: 0.4024\n",
      "Epoch [181/200], Loss: 0.4022\n",
      "Epoch [191/200], Loss: 0.4020\n",
      "Test Accuracy: 54.01%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5243\n",
      "Epoch [11/200], Loss: 0.3907\n",
      "Epoch [21/200], Loss: 0.3902\n",
      "Epoch [31/200], Loss: 0.3898\n",
      "Epoch [41/200], Loss: 0.3896\n",
      "Epoch [51/200], Loss: 0.3893\n",
      "Epoch [61/200], Loss: 0.3891\n",
      "Epoch [71/200], Loss: 0.3889\n",
      "Epoch [81/200], Loss: 0.3888\n",
      "Epoch [91/200], Loss: 0.3886\n",
      "Epoch [101/200], Loss: 0.3884\n",
      "Epoch [111/200], Loss: 0.3883\n",
      "Epoch [121/200], Loss: 0.3881\n",
      "Epoch [131/200], Loss: 0.3880\n",
      "Epoch [141/200], Loss: 0.3878\n",
      "Epoch [151/200], Loss: 0.3877\n",
      "Epoch [161/200], Loss: 0.3875\n",
      "Epoch [171/200], Loss: 0.3874\n",
      "Epoch [181/200], Loss: 0.3873\n",
      "Epoch [191/200], Loss: 0.3871\n",
      "Test Accuracy: 54.40%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5080\n",
      "Epoch [11/200], Loss: 0.3856\n",
      "Epoch [21/200], Loss: 0.3849\n",
      "Epoch [31/200], Loss: 0.3845\n",
      "Epoch [41/200], Loss: 0.3842\n",
      "Epoch [51/200], Loss: 0.3840\n",
      "Epoch [61/200], Loss: 0.3837\n",
      "Epoch [71/200], Loss: 0.3836\n",
      "Epoch [81/200], Loss: 0.3834\n",
      "Epoch [91/200], Loss: 0.3832\n",
      "Epoch [101/200], Loss: 0.3831\n",
      "Epoch [111/200], Loss: 0.3829\n",
      "Epoch [121/200], Loss: 0.3828\n",
      "Epoch [131/200], Loss: 0.3827\n",
      "Epoch [141/200], Loss: 0.3826\n",
      "Epoch [151/200], Loss: 0.3824\n",
      "Epoch [161/200], Loss: 0.3823\n",
      "Epoch [171/200], Loss: 0.3822\n",
      "Epoch [181/200], Loss: 0.3821\n",
      "Epoch [191/200], Loss: 0.3820\n",
      "Test Accuracy: 53.22%\n",
      "Test Accuracy: 66.23%\n",
      "round_97\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5046\n",
      "Epoch [11/200], Loss: 0.3678\n",
      "Epoch [21/200], Loss: 0.3668\n",
      "Epoch [31/200], Loss: 0.3659\n",
      "Epoch [41/200], Loss: 0.3652\n",
      "Epoch [51/200], Loss: 0.3646\n",
      "Epoch [61/200], Loss: 0.3640\n",
      "Epoch [71/200], Loss: 0.3635\n",
      "Epoch [81/200], Loss: 0.3631\n",
      "Epoch [91/200], Loss: 0.3626\n",
      "Epoch [101/200], Loss: 0.3622\n",
      "Epoch [111/200], Loss: 0.3618\n",
      "Epoch [121/200], Loss: 0.3614\n",
      "Epoch [131/200], Loss: 0.3611\n",
      "Epoch [141/200], Loss: 0.3608\n",
      "Epoch [151/200], Loss: 0.3604\n",
      "Epoch [161/200], Loss: 0.3601\n",
      "Epoch [171/200], Loss: 0.3598\n",
      "Epoch [181/200], Loss: 0.3595\n",
      "Epoch [191/200], Loss: 0.3592\n",
      "Test Accuracy: 71.19%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5145\n",
      "Epoch [11/200], Loss: 0.3616\n",
      "Epoch [21/200], Loss: 0.3604\n",
      "Epoch [31/200], Loss: 0.3595\n",
      "Epoch [41/200], Loss: 0.3588\n",
      "Epoch [51/200], Loss: 0.3581\n",
      "Epoch [61/200], Loss: 0.3575\n",
      "Epoch [71/200], Loss: 0.3570\n",
      "Epoch [81/200], Loss: 0.3566\n",
      "Epoch [91/200], Loss: 0.3562\n",
      "Epoch [101/200], Loss: 0.3558\n",
      "Epoch [111/200], Loss: 0.3554\n",
      "Epoch [121/200], Loss: 0.3551\n",
      "Epoch [131/200], Loss: 0.3547\n",
      "Epoch [141/200], Loss: 0.3544\n",
      "Epoch [151/200], Loss: 0.3541\n",
      "Epoch [161/200], Loss: 0.3538\n",
      "Epoch [171/200], Loss: 0.3535\n",
      "Epoch [181/200], Loss: 0.3532\n",
      "Epoch [191/200], Loss: 0.3529\n",
      "Test Accuracy: 70.22%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.4889\n",
      "Epoch [11/200], Loss: 0.3435\n",
      "Epoch [21/200], Loss: 0.3426\n",
      "Epoch [31/200], Loss: 0.3418\n",
      "Epoch [41/200], Loss: 0.3411\n",
      "Epoch [51/200], Loss: 0.3405\n",
      "Epoch [61/200], Loss: 0.3400\n",
      "Epoch [71/200], Loss: 0.3395\n",
      "Epoch [81/200], Loss: 0.3390\n",
      "Epoch [91/200], Loss: 0.3386\n",
      "Epoch [101/200], Loss: 0.3382\n",
      "Epoch [111/200], Loss: 0.3379\n",
      "Epoch [121/200], Loss: 0.3375\n",
      "Epoch [131/200], Loss: 0.3372\n",
      "Epoch [141/200], Loss: 0.3369\n",
      "Epoch [151/200], Loss: 0.3366\n",
      "Epoch [161/200], Loss: 0.3364\n",
      "Epoch [171/200], Loss: 0.3361\n",
      "Epoch [181/200], Loss: 0.3358\n",
      "Epoch [191/200], Loss: 0.3356\n",
      "Test Accuracy: 71.77%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5227\n",
      "Epoch [11/200], Loss: 0.3857\n",
      "Epoch [21/200], Loss: 0.3849\n",
      "Epoch [31/200], Loss: 0.3843\n",
      "Epoch [41/200], Loss: 0.3838\n",
      "Epoch [51/200], Loss: 0.3834\n",
      "Epoch [61/200], Loss: 0.3830\n",
      "Epoch [71/200], Loss: 0.3827\n",
      "Epoch [81/200], Loss: 0.3824\n",
      "Epoch [91/200], Loss: 0.3821\n",
      "Epoch [101/200], Loss: 0.3819\n",
      "Epoch [111/200], Loss: 0.3816\n",
      "Epoch [121/200], Loss: 0.3814\n",
      "Epoch [131/200], Loss: 0.3812\n",
      "Epoch [141/200], Loss: 0.3810\n",
      "Epoch [151/200], Loss: 0.3808\n",
      "Epoch [161/200], Loss: 0.3806\n",
      "Epoch [171/200], Loss: 0.3804\n",
      "Epoch [181/200], Loss: 0.3802\n",
      "Epoch [191/200], Loss: 0.3801\n",
      "Test Accuracy: 70.50%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.5117\n",
      "Epoch [11/200], Loss: 0.3724\n",
      "Epoch [21/200], Loss: 0.3709\n",
      "Epoch [31/200], Loss: 0.3698\n",
      "Epoch [41/200], Loss: 0.3690\n",
      "Epoch [51/200], Loss: 0.3682\n",
      "Epoch [61/200], Loss: 0.3676\n",
      "Epoch [71/200], Loss: 0.3671\n",
      "Epoch [81/200], Loss: 0.3667\n",
      "Epoch [91/200], Loss: 0.3663\n",
      "Epoch [101/200], Loss: 0.3660\n",
      "Epoch [111/200], Loss: 0.3657\n",
      "Epoch [121/200], Loss: 0.3654\n",
      "Epoch [131/200], Loss: 0.3651\n",
      "Epoch [141/200], Loss: 0.3648\n",
      "Epoch [151/200], Loss: 0.3646\n",
      "Epoch [161/200], Loss: 0.3643\n",
      "Epoch [171/200], Loss: 0.3641\n",
      "Epoch [181/200], Loss: 0.3639\n",
      "Epoch [191/200], Loss: 0.3636\n",
      "Test Accuracy: 70.32%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5242\n",
      "Epoch [11/200], Loss: 0.3900\n",
      "Epoch [21/200], Loss: 0.3894\n",
      "Epoch [31/200], Loss: 0.3890\n",
      "Epoch [41/200], Loss: 0.3886\n",
      "Epoch [51/200], Loss: 0.3882\n",
      "Epoch [61/200], Loss: 0.3879\n",
      "Epoch [71/200], Loss: 0.3876\n",
      "Epoch [81/200], Loss: 0.3874\n",
      "Epoch [91/200], Loss: 0.3871\n",
      "Epoch [101/200], Loss: 0.3869\n",
      "Epoch [111/200], Loss: 0.3867\n",
      "Epoch [121/200], Loss: 0.3864\n",
      "Epoch [131/200], Loss: 0.3862\n",
      "Epoch [141/200], Loss: 0.3860\n",
      "Epoch [151/200], Loss: 0.3859\n",
      "Epoch [161/200], Loss: 0.3857\n",
      "Epoch [171/200], Loss: 0.3855\n",
      "Epoch [181/200], Loss: 0.3853\n",
      "Epoch [191/200], Loss: 0.3852\n",
      "Test Accuracy: 53.79%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5339\n",
      "Epoch [11/200], Loss: 0.3925\n",
      "Epoch [21/200], Loss: 0.3921\n",
      "Epoch [31/200], Loss: 0.3917\n",
      "Epoch [41/200], Loss: 0.3913\n",
      "Epoch [51/200], Loss: 0.3910\n",
      "Epoch [61/200], Loss: 0.3907\n",
      "Epoch [71/200], Loss: 0.3904\n",
      "Epoch [81/200], Loss: 0.3902\n",
      "Epoch [91/200], Loss: 0.3900\n",
      "Epoch [101/200], Loss: 0.3898\n",
      "Epoch [111/200], Loss: 0.3896\n",
      "Epoch [121/200], Loss: 0.3895\n",
      "Epoch [131/200], Loss: 0.3893\n",
      "Epoch [141/200], Loss: 0.3891\n",
      "Epoch [151/200], Loss: 0.3890\n",
      "Epoch [161/200], Loss: 0.3889\n",
      "Epoch [171/200], Loss: 0.3887\n",
      "Epoch [181/200], Loss: 0.3886\n",
      "Epoch [191/200], Loss: 0.3884\n",
      "Test Accuracy: 54.31%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5442\n",
      "Epoch [11/200], Loss: 0.3985\n",
      "Epoch [21/200], Loss: 0.3981\n",
      "Epoch [31/200], Loss: 0.3977\n",
      "Epoch [41/200], Loss: 0.3974\n",
      "Epoch [51/200], Loss: 0.3972\n",
      "Epoch [61/200], Loss: 0.3970\n",
      "Epoch [71/200], Loss: 0.3967\n",
      "Epoch [81/200], Loss: 0.3965\n",
      "Epoch [91/200], Loss: 0.3963\n",
      "Epoch [101/200], Loss: 0.3961\n",
      "Epoch [111/200], Loss: 0.3960\n",
      "Epoch [121/200], Loss: 0.3958\n",
      "Epoch [131/200], Loss: 0.3956\n",
      "Epoch [141/200], Loss: 0.3954\n",
      "Epoch [151/200], Loss: 0.3952\n",
      "Epoch [161/200], Loss: 0.3951\n",
      "Epoch [171/200], Loss: 0.3949\n",
      "Epoch [181/200], Loss: 0.3947\n",
      "Epoch [191/200], Loss: 0.3946\n",
      "Test Accuracy: 54.27%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5400\n",
      "Epoch [11/200], Loss: 0.4030\n",
      "Epoch [21/200], Loss: 0.4024\n",
      "Epoch [31/200], Loss: 0.4019\n",
      "Epoch [41/200], Loss: 0.4014\n",
      "Epoch [51/200], Loss: 0.4011\n",
      "Epoch [61/200], Loss: 0.4007\n",
      "Epoch [71/200], Loss: 0.4005\n",
      "Epoch [81/200], Loss: 0.4002\n",
      "Epoch [91/200], Loss: 0.4000\n",
      "Epoch [101/200], Loss: 0.3997\n",
      "Epoch [111/200], Loss: 0.3995\n",
      "Epoch [121/200], Loss: 0.3994\n",
      "Epoch [131/200], Loss: 0.3992\n",
      "Epoch [141/200], Loss: 0.3990\n",
      "Epoch [151/200], Loss: 0.3988\n",
      "Epoch [161/200], Loss: 0.3987\n",
      "Epoch [171/200], Loss: 0.3985\n",
      "Epoch [181/200], Loss: 0.3984\n",
      "Epoch [191/200], Loss: 0.3982\n",
      "Test Accuracy: 54.17%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.4849\n",
      "Epoch [11/200], Loss: 0.3697\n",
      "Epoch [21/200], Loss: 0.3693\n",
      "Epoch [31/200], Loss: 0.3690\n",
      "Epoch [41/200], Loss: 0.3688\n",
      "Epoch [51/200], Loss: 0.3687\n",
      "Epoch [61/200], Loss: 0.3685\n",
      "Epoch [71/200], Loss: 0.3684\n",
      "Epoch [81/200], Loss: 0.3682\n",
      "Epoch [91/200], Loss: 0.3681\n",
      "Epoch [101/200], Loss: 0.3680\n",
      "Epoch [111/200], Loss: 0.3678\n",
      "Epoch [121/200], Loss: 0.3677\n",
      "Epoch [131/200], Loss: 0.3676\n",
      "Epoch [141/200], Loss: 0.3675\n",
      "Epoch [151/200], Loss: 0.3673\n",
      "Epoch [161/200], Loss: 0.3672\n",
      "Epoch [171/200], Loss: 0.3671\n",
      "Epoch [181/200], Loss: 0.3670\n",
      "Epoch [191/200], Loss: 0.3669\n",
      "Test Accuracy: 52.68%\n",
      "Test Accuracy: 67.25%\n",
      "round_98\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5047\n",
      "Epoch [11/200], Loss: 0.3731\n",
      "Epoch [21/200], Loss: 0.3726\n",
      "Epoch [31/200], Loss: 0.3721\n",
      "Epoch [41/200], Loss: 0.3718\n",
      "Epoch [51/200], Loss: 0.3714\n",
      "Epoch [61/200], Loss: 0.3711\n",
      "Epoch [71/200], Loss: 0.3708\n",
      "Epoch [81/200], Loss: 0.3705\n",
      "Epoch [91/200], Loss: 0.3702\n",
      "Epoch [101/200], Loss: 0.3700\n",
      "Epoch [111/200], Loss: 0.3698\n",
      "Epoch [121/200], Loss: 0.3695\n",
      "Epoch [131/200], Loss: 0.3693\n",
      "Epoch [141/200], Loss: 0.3691\n",
      "Epoch [151/200], Loss: 0.3689\n",
      "Epoch [161/200], Loss: 0.3687\n",
      "Epoch [171/200], Loss: 0.3685\n",
      "Epoch [181/200], Loss: 0.3683\n",
      "Epoch [191/200], Loss: 0.3681\n",
      "Test Accuracy: 71.24%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.5151\n",
      "Epoch [11/200], Loss: 0.3899\n",
      "Epoch [21/200], Loss: 0.3895\n",
      "Epoch [31/200], Loss: 0.3891\n",
      "Epoch [41/200], Loss: 0.3887\n",
      "Epoch [51/200], Loss: 0.3883\n",
      "Epoch [61/200], Loss: 0.3880\n",
      "Epoch [71/200], Loss: 0.3876\n",
      "Epoch [81/200], Loss: 0.3873\n",
      "Epoch [91/200], Loss: 0.3869\n",
      "Epoch [101/200], Loss: 0.3864\n",
      "Epoch [111/200], Loss: 0.3860\n",
      "Epoch [121/200], Loss: 0.3857\n",
      "Epoch [131/200], Loss: 0.3855\n",
      "Epoch [141/200], Loss: 0.3852\n",
      "Epoch [151/200], Loss: 0.3850\n",
      "Epoch [161/200], Loss: 0.3848\n",
      "Epoch [171/200], Loss: 0.3846\n",
      "Epoch [181/200], Loss: 0.3843\n",
      "Epoch [191/200], Loss: 0.3841\n",
      "Test Accuracy: 70.77%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.5047\n",
      "Epoch [11/200], Loss: 0.3539\n",
      "Epoch [21/200], Loss: 0.3528\n",
      "Epoch [31/200], Loss: 0.3522\n",
      "Epoch [41/200], Loss: 0.3517\n",
      "Epoch [51/200], Loss: 0.3513\n",
      "Epoch [61/200], Loss: 0.3509\n",
      "Epoch [71/200], Loss: 0.3506\n",
      "Epoch [81/200], Loss: 0.3502\n",
      "Epoch [91/200], Loss: 0.3499\n",
      "Epoch [101/200], Loss: 0.3496\n",
      "Epoch [111/200], Loss: 0.3494\n",
      "Epoch [121/200], Loss: 0.3491\n",
      "Epoch [131/200], Loss: 0.3488\n",
      "Epoch [141/200], Loss: 0.3486\n",
      "Epoch [151/200], Loss: 0.3484\n",
      "Epoch [161/200], Loss: 0.3481\n",
      "Epoch [171/200], Loss: 0.3479\n",
      "Epoch [181/200], Loss: 0.3477\n",
      "Epoch [191/200], Loss: 0.3474\n",
      "Test Accuracy: 70.31%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5233\n",
      "Epoch [11/200], Loss: 0.4084\n",
      "Epoch [21/200], Loss: 0.4075\n",
      "Epoch [31/200], Loss: 0.4069\n",
      "Epoch [41/200], Loss: 0.4064\n",
      "Epoch [51/200], Loss: 0.4060\n",
      "Epoch [61/200], Loss: 0.4056\n",
      "Epoch [71/200], Loss: 0.4052\n",
      "Epoch [81/200], Loss: 0.4049\n",
      "Epoch [91/200], Loss: 0.4046\n",
      "Epoch [101/200], Loss: 0.4043\n",
      "Epoch [111/200], Loss: 0.4040\n",
      "Epoch [121/200], Loss: 0.4038\n",
      "Epoch [131/200], Loss: 0.4035\n",
      "Epoch [141/200], Loss: 0.4033\n",
      "Epoch [151/200], Loss: 0.4030\n",
      "Epoch [161/200], Loss: 0.4028\n",
      "Epoch [171/200], Loss: 0.4025\n",
      "Epoch [181/200], Loss: 0.4023\n",
      "Epoch [191/200], Loss: 0.4021\n",
      "Test Accuracy: 70.61%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.4913\n",
      "Epoch [11/200], Loss: 0.3574\n",
      "Epoch [21/200], Loss: 0.3562\n",
      "Epoch [31/200], Loss: 0.3555\n",
      "Epoch [41/200], Loss: 0.3550\n",
      "Epoch [51/200], Loss: 0.3545\n",
      "Epoch [61/200], Loss: 0.3540\n",
      "Epoch [71/200], Loss: 0.3536\n",
      "Epoch [81/200], Loss: 0.3532\n",
      "Epoch [91/200], Loss: 0.3529\n",
      "Epoch [101/200], Loss: 0.3525\n",
      "Epoch [111/200], Loss: 0.3522\n",
      "Epoch [121/200], Loss: 0.3518\n",
      "Epoch [131/200], Loss: 0.3515\n",
      "Epoch [141/200], Loss: 0.3512\n",
      "Epoch [151/200], Loss: 0.3509\n",
      "Epoch [161/200], Loss: 0.3506\n",
      "Epoch [171/200], Loss: 0.3503\n",
      "Epoch [181/200], Loss: 0.3500\n",
      "Epoch [191/200], Loss: 0.3498\n",
      "Test Accuracy: 70.55%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5567\n",
      "Epoch [11/200], Loss: 0.4006\n",
      "Epoch [21/200], Loss: 0.3998\n",
      "Epoch [31/200], Loss: 0.3992\n",
      "Epoch [41/200], Loss: 0.3986\n",
      "Epoch [51/200], Loss: 0.3982\n",
      "Epoch [61/200], Loss: 0.3978\n",
      "Epoch [71/200], Loss: 0.3975\n",
      "Epoch [81/200], Loss: 0.3972\n",
      "Epoch [91/200], Loss: 0.3970\n",
      "Epoch [101/200], Loss: 0.3968\n",
      "Epoch [111/200], Loss: 0.3966\n",
      "Epoch [121/200], Loss: 0.3964\n",
      "Epoch [131/200], Loss: 0.3962\n",
      "Epoch [141/200], Loss: 0.3960\n",
      "Epoch [151/200], Loss: 0.3958\n",
      "Epoch [161/200], Loss: 0.3956\n",
      "Epoch [171/200], Loss: 0.3955\n",
      "Epoch [181/200], Loss: 0.3953\n",
      "Epoch [191/200], Loss: 0.3951\n",
      "Test Accuracy: 54.13%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5448\n",
      "Epoch [11/200], Loss: 0.3883\n",
      "Epoch [21/200], Loss: 0.3874\n",
      "Epoch [31/200], Loss: 0.3867\n",
      "Epoch [41/200], Loss: 0.3860\n",
      "Epoch [51/200], Loss: 0.3855\n",
      "Epoch [61/200], Loss: 0.3851\n",
      "Epoch [71/200], Loss: 0.3847\n",
      "Epoch [81/200], Loss: 0.3843\n",
      "Epoch [91/200], Loss: 0.3840\n",
      "Epoch [101/200], Loss: 0.3837\n",
      "Epoch [111/200], Loss: 0.3835\n",
      "Epoch [121/200], Loss: 0.3832\n",
      "Epoch [131/200], Loss: 0.3830\n",
      "Epoch [141/200], Loss: 0.3828\n",
      "Epoch [151/200], Loss: 0.3826\n",
      "Epoch [161/200], Loss: 0.3824\n",
      "Epoch [171/200], Loss: 0.3823\n",
      "Epoch [181/200], Loss: 0.3821\n",
      "Epoch [191/200], Loss: 0.3819\n",
      "Test Accuracy: 54.26%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5542\n",
      "Epoch [11/200], Loss: 0.3964\n",
      "Epoch [21/200], Loss: 0.3958\n",
      "Epoch [31/200], Loss: 0.3954\n",
      "Epoch [41/200], Loss: 0.3950\n",
      "Epoch [51/200], Loss: 0.3947\n",
      "Epoch [61/200], Loss: 0.3944\n",
      "Epoch [71/200], Loss: 0.3942\n",
      "Epoch [81/200], Loss: 0.3939\n",
      "Epoch [91/200], Loss: 0.3937\n",
      "Epoch [101/200], Loss: 0.3935\n",
      "Epoch [111/200], Loss: 0.3933\n",
      "Epoch [121/200], Loss: 0.3931\n",
      "Epoch [131/200], Loss: 0.3930\n",
      "Epoch [141/200], Loss: 0.3928\n",
      "Epoch [151/200], Loss: 0.3926\n",
      "Epoch [161/200], Loss: 0.3925\n",
      "Epoch [171/200], Loss: 0.3923\n",
      "Epoch [181/200], Loss: 0.3922\n",
      "Epoch [191/200], Loss: 0.3920\n",
      "Test Accuracy: 54.15%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5394\n",
      "Epoch [11/200], Loss: 0.3927\n",
      "Epoch [21/200], Loss: 0.3922\n",
      "Epoch [31/200], Loss: 0.3918\n",
      "Epoch [41/200], Loss: 0.3914\n",
      "Epoch [51/200], Loss: 0.3911\n",
      "Epoch [61/200], Loss: 0.3909\n",
      "Epoch [71/200], Loss: 0.3906\n",
      "Epoch [81/200], Loss: 0.3904\n",
      "Epoch [91/200], Loss: 0.3902\n",
      "Epoch [101/200], Loss: 0.3899\n",
      "Epoch [111/200], Loss: 0.3897\n",
      "Epoch [121/200], Loss: 0.3895\n",
      "Epoch [131/200], Loss: 0.3893\n",
      "Epoch [141/200], Loss: 0.3891\n",
      "Epoch [151/200], Loss: 0.3889\n",
      "Epoch [161/200], Loss: 0.3887\n",
      "Epoch [171/200], Loss: 0.3885\n",
      "Epoch [181/200], Loss: 0.3883\n",
      "Epoch [191/200], Loss: 0.3881\n",
      "Test Accuracy: 53.79%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.5541\n",
      "Epoch [11/200], Loss: 0.3987\n",
      "Epoch [21/200], Loss: 0.3981\n",
      "Epoch [31/200], Loss: 0.3976\n",
      "Epoch [41/200], Loss: 0.3972\n",
      "Epoch [51/200], Loss: 0.3969\n",
      "Epoch [61/200], Loss: 0.3966\n",
      "Epoch [71/200], Loss: 0.3963\n",
      "Epoch [81/200], Loss: 0.3961\n",
      "Epoch [91/200], Loss: 0.3958\n",
      "Epoch [101/200], Loss: 0.3956\n",
      "Epoch [111/200], Loss: 0.3954\n",
      "Epoch [121/200], Loss: 0.3952\n",
      "Epoch [131/200], Loss: 0.3950\n",
      "Epoch [141/200], Loss: 0.3949\n",
      "Epoch [151/200], Loss: 0.3947\n",
      "Epoch [161/200], Loss: 0.3945\n",
      "Epoch [171/200], Loss: 0.3944\n",
      "Epoch [181/200], Loss: 0.3942\n",
      "Epoch [191/200], Loss: 0.3940\n",
      "Test Accuracy: 54.42%\n",
      "Test Accuracy: 66.82%\n",
      "round_99\n",
      "client_0\n",
      "Epoch [1/200], Loss: 0.5132\n",
      "Epoch [11/200], Loss: 0.3821\n",
      "Epoch [21/200], Loss: 0.3813\n",
      "Epoch [31/200], Loss: 0.3807\n",
      "Epoch [41/200], Loss: 0.3801\n",
      "Epoch [51/200], Loss: 0.3796\n",
      "Epoch [61/200], Loss: 0.3791\n",
      "Epoch [71/200], Loss: 0.3787\n",
      "Epoch [81/200], Loss: 0.3783\n",
      "Epoch [91/200], Loss: 0.3779\n",
      "Epoch [101/200], Loss: 0.3775\n",
      "Epoch [111/200], Loss: 0.3771\n",
      "Epoch [121/200], Loss: 0.3767\n",
      "Epoch [131/200], Loss: 0.3764\n",
      "Epoch [141/200], Loss: 0.3761\n",
      "Epoch [151/200], Loss: 0.3758\n",
      "Epoch [161/200], Loss: 0.3755\n",
      "Epoch [171/200], Loss: 0.3752\n",
      "Epoch [181/200], Loss: 0.3749\n",
      "Epoch [191/200], Loss: 0.3746\n",
      "Test Accuracy: 70.83%\n",
      "client_1\n",
      "Epoch [1/200], Loss: 0.4987\n",
      "Epoch [11/200], Loss: 0.3534\n",
      "Epoch [21/200], Loss: 0.3523\n",
      "Epoch [31/200], Loss: 0.3514\n",
      "Epoch [41/200], Loss: 0.3506\n",
      "Epoch [51/200], Loss: 0.3499\n",
      "Epoch [61/200], Loss: 0.3493\n",
      "Epoch [71/200], Loss: 0.3488\n",
      "Epoch [81/200], Loss: 0.3482\n",
      "Epoch [91/200], Loss: 0.3478\n",
      "Epoch [101/200], Loss: 0.3473\n",
      "Epoch [111/200], Loss: 0.3469\n",
      "Epoch [121/200], Loss: 0.3464\n",
      "Epoch [131/200], Loss: 0.3460\n",
      "Epoch [141/200], Loss: 0.3457\n",
      "Epoch [151/200], Loss: 0.3453\n",
      "Epoch [161/200], Loss: 0.3449\n",
      "Epoch [171/200], Loss: 0.3446\n",
      "Epoch [181/200], Loss: 0.3443\n",
      "Epoch [191/200], Loss: 0.3440\n",
      "Test Accuracy: 71.53%\n",
      "client_2\n",
      "Epoch [1/200], Loss: 0.4991\n",
      "Epoch [11/200], Loss: 0.3604\n",
      "Epoch [21/200], Loss: 0.3595\n",
      "Epoch [31/200], Loss: 0.3588\n",
      "Epoch [41/200], Loss: 0.3583\n",
      "Epoch [51/200], Loss: 0.3578\n",
      "Epoch [61/200], Loss: 0.3573\n",
      "Epoch [71/200], Loss: 0.3569\n",
      "Epoch [81/200], Loss: 0.3565\n",
      "Epoch [91/200], Loss: 0.3562\n",
      "Epoch [101/200], Loss: 0.3558\n",
      "Epoch [111/200], Loss: 0.3555\n",
      "Epoch [121/200], Loss: 0.3552\n",
      "Epoch [131/200], Loss: 0.3549\n",
      "Epoch [141/200], Loss: 0.3546\n",
      "Epoch [151/200], Loss: 0.3543\n",
      "Epoch [161/200], Loss: 0.3540\n",
      "Epoch [171/200], Loss: 0.3537\n",
      "Epoch [181/200], Loss: 0.3534\n",
      "Epoch [191/200], Loss: 0.3531\n",
      "Test Accuracy: 71.36%\n",
      "client_3\n",
      "Epoch [1/200], Loss: 0.5212\n",
      "Epoch [11/200], Loss: 0.3748\n",
      "Epoch [21/200], Loss: 0.3740\n",
      "Epoch [31/200], Loss: 0.3734\n",
      "Epoch [41/200], Loss: 0.3728\n",
      "Epoch [51/200], Loss: 0.3724\n",
      "Epoch [61/200], Loss: 0.3719\n",
      "Epoch [71/200], Loss: 0.3715\n",
      "Epoch [81/200], Loss: 0.3712\n",
      "Epoch [91/200], Loss: 0.3708\n",
      "Epoch [101/200], Loss: 0.3704\n",
      "Epoch [111/200], Loss: 0.3701\n",
      "Epoch [121/200], Loss: 0.3698\n",
      "Epoch [131/200], Loss: 0.3695\n",
      "Epoch [141/200], Loss: 0.3692\n",
      "Epoch [151/200], Loss: 0.3689\n",
      "Epoch [161/200], Loss: 0.3686\n",
      "Epoch [171/200], Loss: 0.3683\n",
      "Epoch [181/200], Loss: 0.3680\n",
      "Epoch [191/200], Loss: 0.3677\n",
      "Test Accuracy: 71.18%\n",
      "client_4\n",
      "Epoch [1/200], Loss: 0.4944\n",
      "Epoch [11/200], Loss: 0.3364\n",
      "Epoch [21/200], Loss: 0.3351\n",
      "Epoch [31/200], Loss: 0.3340\n",
      "Epoch [41/200], Loss: 0.3332\n",
      "Epoch [51/200], Loss: 0.3324\n",
      "Epoch [61/200], Loss: 0.3318\n",
      "Epoch [71/200], Loss: 0.3312\n",
      "Epoch [81/200], Loss: 0.3306\n",
      "Epoch [91/200], Loss: 0.3301\n",
      "Epoch [101/200], Loss: 0.3296\n",
      "Epoch [111/200], Loss: 0.3291\n",
      "Epoch [121/200], Loss: 0.3287\n",
      "Epoch [131/200], Loss: 0.3283\n",
      "Epoch [141/200], Loss: 0.3279\n",
      "Epoch [151/200], Loss: 0.3275\n",
      "Epoch [161/200], Loss: 0.3271\n",
      "Epoch [171/200], Loss: 0.3267\n",
      "Epoch [181/200], Loss: 0.3263\n",
      "Epoch [191/200], Loss: 0.3260\n",
      "Test Accuracy: 71.17%\n",
      "client_5\n",
      "Epoch [1/200], Loss: 0.5199\n",
      "Epoch [11/200], Loss: 0.3887\n",
      "Epoch [21/200], Loss: 0.3882\n",
      "Epoch [31/200], Loss: 0.3877\n",
      "Epoch [41/200], Loss: 0.3874\n",
      "Epoch [51/200], Loss: 0.3870\n",
      "Epoch [61/200], Loss: 0.3867\n",
      "Epoch [71/200], Loss: 0.3865\n",
      "Epoch [81/200], Loss: 0.3862\n",
      "Epoch [91/200], Loss: 0.3860\n",
      "Epoch [101/200], Loss: 0.3858\n",
      "Epoch [111/200], Loss: 0.3856\n",
      "Epoch [121/200], Loss: 0.3854\n",
      "Epoch [131/200], Loss: 0.3853\n",
      "Epoch [141/200], Loss: 0.3851\n",
      "Epoch [151/200], Loss: 0.3850\n",
      "Epoch [161/200], Loss: 0.3849\n",
      "Epoch [171/200], Loss: 0.3847\n",
      "Epoch [181/200], Loss: 0.3846\n",
      "Epoch [191/200], Loss: 0.3845\n",
      "Test Accuracy: 54.04%\n",
      "client_6\n",
      "Epoch [1/200], Loss: 0.5260\n",
      "Epoch [11/200], Loss: 0.3844\n",
      "Epoch [21/200], Loss: 0.3836\n",
      "Epoch [31/200], Loss: 0.3830\n",
      "Epoch [41/200], Loss: 0.3824\n",
      "Epoch [51/200], Loss: 0.3818\n",
      "Epoch [61/200], Loss: 0.3813\n",
      "Epoch [71/200], Loss: 0.3809\n",
      "Epoch [81/200], Loss: 0.3805\n",
      "Epoch [91/200], Loss: 0.3802\n",
      "Epoch [101/200], Loss: 0.3799\n",
      "Epoch [111/200], Loss: 0.3796\n",
      "Epoch [121/200], Loss: 0.3793\n",
      "Epoch [131/200], Loss: 0.3791\n",
      "Epoch [141/200], Loss: 0.3788\n",
      "Epoch [151/200], Loss: 0.3786\n",
      "Epoch [161/200], Loss: 0.3784\n",
      "Epoch [171/200], Loss: 0.3782\n",
      "Epoch [181/200], Loss: 0.3780\n",
      "Epoch [191/200], Loss: 0.3778\n",
      "Test Accuracy: 53.37%\n",
      "client_7\n",
      "Epoch [1/200], Loss: 0.5194\n",
      "Epoch [11/200], Loss: 0.3886\n",
      "Epoch [21/200], Loss: 0.3880\n",
      "Epoch [31/200], Loss: 0.3875\n",
      "Epoch [41/200], Loss: 0.3871\n",
      "Epoch [51/200], Loss: 0.3867\n",
      "Epoch [61/200], Loss: 0.3864\n",
      "Epoch [71/200], Loss: 0.3861\n",
      "Epoch [81/200], Loss: 0.3858\n",
      "Epoch [91/200], Loss: 0.3856\n",
      "Epoch [101/200], Loss: 0.3853\n",
      "Epoch [111/200], Loss: 0.3851\n",
      "Epoch [121/200], Loss: 0.3848\n",
      "Epoch [131/200], Loss: 0.3846\n",
      "Epoch [141/200], Loss: 0.3844\n",
      "Epoch [151/200], Loss: 0.3842\n",
      "Epoch [161/200], Loss: 0.3840\n",
      "Epoch [171/200], Loss: 0.3837\n",
      "Epoch [181/200], Loss: 0.3835\n",
      "Epoch [191/200], Loss: 0.3833\n",
      "Test Accuracy: 54.06%\n",
      "client_8\n",
      "Epoch [1/200], Loss: 0.5331\n",
      "Epoch [11/200], Loss: 0.3840\n",
      "Epoch [21/200], Loss: 0.3830\n",
      "Epoch [31/200], Loss: 0.3824\n",
      "Epoch [41/200], Loss: 0.3819\n",
      "Epoch [51/200], Loss: 0.3814\n",
      "Epoch [61/200], Loss: 0.3811\n",
      "Epoch [71/200], Loss: 0.3807\n",
      "Epoch [81/200], Loss: 0.3804\n",
      "Epoch [91/200], Loss: 0.3801\n",
      "Epoch [101/200], Loss: 0.3799\n",
      "Epoch [111/200], Loss: 0.3796\n",
      "Epoch [121/200], Loss: 0.3793\n",
      "Epoch [131/200], Loss: 0.3791\n",
      "Epoch [141/200], Loss: 0.3789\n",
      "Epoch [151/200], Loss: 0.3786\n",
      "Epoch [161/200], Loss: 0.3784\n",
      "Epoch [171/200], Loss: 0.3782\n",
      "Epoch [181/200], Loss: 0.3780\n",
      "Epoch [191/200], Loss: 0.3778\n",
      "Test Accuracy: 53.90%\n",
      "client_9\n",
      "Epoch [1/200], Loss: 0.4966\n",
      "Epoch [11/200], Loss: 0.3724\n",
      "Epoch [21/200], Loss: 0.3719\n",
      "Epoch [31/200], Loss: 0.3716\n",
      "Epoch [41/200], Loss: 0.3713\n",
      "Epoch [51/200], Loss: 0.3710\n",
      "Epoch [61/200], Loss: 0.3707\n",
      "Epoch [71/200], Loss: 0.3705\n",
      "Epoch [81/200], Loss: 0.3703\n",
      "Epoch [91/200], Loss: 0.3701\n",
      "Epoch [101/200], Loss: 0.3699\n",
      "Epoch [111/200], Loss: 0.3697\n",
      "Epoch [121/200], Loss: 0.3696\n",
      "Epoch [131/200], Loss: 0.3694\n",
      "Epoch [141/200], Loss: 0.3693\n",
      "Epoch [151/200], Loss: 0.3691\n",
      "Epoch [161/200], Loss: 0.3690\n",
      "Epoch [171/200], Loss: 0.3688\n",
      "Epoch [181/200], Loss: 0.3687\n",
      "Epoch [191/200], Loss: 0.3686\n",
      "Test Accuracy: 53.87%\n",
      "Test Accuracy: 67.93%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.fc4 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "def save_metrics(title,rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def read_data_return_tensor(dataset_path, round_id,client_id):\n",
    "    open_file_path = os.path.join(dataset_path, f'client_{client_id}/round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=None)\n",
    "\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data.iloc[1:])\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :2].astype(float).values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].astype(float).astype(int).values  # 目标变量\n",
    "\n",
    "    # 将特征和目标变量转换为 PyTorch 张量\n",
    "    X_train_tensor = torch.tensor(raw_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(raw_y, dtype=torch.long)  # 标签张量\n",
    "\n",
    "    return X_train_tensor, y_train_tensor\n",
    "def read_test_data(dataset_path):\n",
    "    # open_file_path = os.path.join(dataset_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(dataset_path, header=None)\n",
    "\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data.iloc[1:])\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :2].astype(float).values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].astype(float).astype(int).values  # 目标变量\n",
    "\n",
    "    # 将特征和目标变量转换为 PyTorch 张量\n",
    "    X_train_tensor = torch.tensor(raw_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(raw_y, dtype=torch.long)  # 标签张量\n",
    "\n",
    "    return X_train_tensor, y_train_tensor\n",
    "def train_worker(input_model,X_train_tensor, y_train_tensor, num_epochs,client_id,round_id,save_path):\n",
    "    foder = os.path.join(save_path,f'client_{client_id}/round_{round_id}')\n",
    "    os.makedirs(foder, exist_ok=True)\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        # if epoch % 2 == 0:\n",
    "        #     model.eval()\n",
    "        #     save_model(copy.deepcopy(model),id=epoch,save_model_path = foder)\n",
    "\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "def save_model(model,id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'epoch_{id}.pt')\n",
    "    torch.save(model,model_path)\n",
    "# def share_params(model):\n",
    "#     params = model.state_dict()\n",
    "#     return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "    # return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def runFederatedDataStream(dataset_name):\n",
    "    rounds = 100\n",
    "    client_nums = 10\n",
    "    input_size = 2\n",
    "    hidden_size = 50\n",
    "    output_size = 2\n",
    "    num_epochs = 200\n",
    "    global_model = MLP4(input_size, hidden_size, output_size)\n",
    "    clients_models = [MLP4(input_size, hidden_size, output_size) for _ in range(client_nums)]\n",
    "\n",
    "    save_model_path = f'E:/FedStream/models/2Dim_9A1B/Federated_600/{dataset_name}_{num_epochs}_mlp4'\n",
    "    save_metrics_path = f'E:/FedStream/metrics/2Dim_9A1B/Federated_600/{dataset_name}_{num_epochs}_mlp4'\n",
    "    os.makedirs(save_metrics_path , exist_ok=True)\n",
    "    read_data_path = f'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/Federated_600/{dataset_name}'\n",
    "    test_path = 'E:/FedStream/data_set_syn/Synthetic0310/2Dim_9A1B/test/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "    para = {}\n",
    "    for update in range(rounds):\n",
    "        print(f'round_{update}')\n",
    "        local_params_list = []\n",
    "        for client_id in range(client_nums):\n",
    "            print(f'client_{client_id}')\n",
    "            X_train_local,y_train_local= read_data_return_tensor(read_data_path,round_id=update,client_id=client_id)\n",
    "            losses = train_worker(clients_models[client_id],X_train_local,y_train_local,num_epochs=num_epochs,client_id=client_id,round_id=update,save_path=save_model_path)\n",
    "            local_params_list.append(share_params(clients_models[client_id]))\n",
    "            local_metrics = test(copy.deepcopy(clients_models[client_id]),X_test_tensor,y_test_tensor)\n",
    "            save_metrics(title=f\"client_{client_id}_metrics\",rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "        aggregated_params = aggregate_params(local_params_list)\n",
    "    \n",
    "        gm = copy.deepcopy(global_model)\n",
    "        if update==0 or update==1 or update == 30 or update == 50:\n",
    "            para[f'{update}']= aggregated_params\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "        foder = os.path.join(save_model_path,f'global_model')\n",
    "        os.makedirs(foder, exist_ok=True)\n",
    "        save_gm_path = os.path.join(foder,f'round_{update}.pt')\n",
    "        torch.save(gm,save_gm_path)\n",
    "        global_metrics = test(gm,X_test_tensor,y_test_tensor)\n",
    "        save_metrics(title=f\"global_metrics\", rounds=update, metrics=global_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "    return para\n",
    "# dataset_name='sample_space'\n",
    "# Federated_180/direction_step10/\n",
    "dataset_name='direction_step10'\n",
    "para = runFederatedDataStream(dataset_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc1.weight': tensor([[-0.0941, -0.0502],\n",
      "        [ 0.0839,  0.1771],\n",
      "        [ 0.0786,  0.0402],\n",
      "        [-0.1010,  0.0164],\n",
      "        [-0.0846,  0.0678],\n",
      "        [ 0.1583,  0.2954],\n",
      "        [ 0.1220,  0.0080],\n",
      "        [-0.2725, -0.2369],\n",
      "        [ 0.1013,  0.0353],\n",
      "        [ 0.0989, -0.0372],\n",
      "        [-0.0846, -0.0482],\n",
      "        [ 0.2537,  0.1293],\n",
      "        [-0.1480, -0.1581],\n",
      "        [-0.0069, -0.2284],\n",
      "        [ 0.1576,  0.0945],\n",
      "        [ 0.0234, -0.2758],\n",
      "        [-0.0671, -0.0346],\n",
      "        [ 0.3184, -0.1017],\n",
      "        [ 0.2932, -0.0248],\n",
      "        [-0.0075,  0.1546],\n",
      "        [ 0.0659,  0.1669],\n",
      "        [-0.0754,  0.0517],\n",
      "        [ 0.2269, -0.2135],\n",
      "        [ 0.0025, -0.0625],\n",
      "        [ 0.1994,  0.0671],\n",
      "        [-0.1752, -0.1841],\n",
      "        [ 0.1129, -0.0969],\n",
      "        [ 0.1733, -0.0447],\n",
      "        [-0.1179,  0.0789],\n",
      "        [ 0.3524, -0.1611],\n",
      "        [ 0.0244,  0.1774],\n",
      "        [ 0.0487,  0.1807],\n",
      "        [ 0.0667, -0.0789],\n",
      "        [ 0.0775, -0.1181],\n",
      "        [-0.0415, -0.2219],\n",
      "        [-0.0521,  0.1696],\n",
      "        [-0.0319, -0.0381],\n",
      "        [ 0.0996, -0.1026],\n",
      "        [-0.0906,  0.0500],\n",
      "        [-0.1718,  0.1307],\n",
      "        [-0.0935,  0.1695],\n",
      "        [ 0.1474, -0.0946],\n",
      "        [-0.1581,  0.0006],\n",
      "        [-0.1732,  0.0950],\n",
      "        [-0.0224,  0.0811],\n",
      "        [-0.0995, -0.0634],\n",
      "        [ 0.3266,  0.1897],\n",
      "        [ 0.0877,  0.3411],\n",
      "        [-0.1449, -0.1426],\n",
      "        [ 0.2500,  0.1681],\n",
      "        [ 0.0132,  0.1721],\n",
      "        [ 0.1521, -0.0251],\n",
      "        [ 0.2248,  0.0258],\n",
      "        [-0.0673,  0.1341],\n",
      "        [-0.1840,  0.0945],\n",
      "        [-0.0619,  0.0167],\n",
      "        [-0.2519, -0.0849],\n",
      "        [-0.0511,  0.0245],\n",
      "        [ 0.0299, -0.1157],\n",
      "        [ 0.1944, -0.0434],\n",
      "        [ 0.0757,  0.1162],\n",
      "        [-0.1656,  0.0451],\n",
      "        [ 0.1381,  0.1114],\n",
      "        [ 0.0230,  0.0850],\n",
      "        [-0.0161, -0.2571],\n",
      "        [-0.0167,  0.1790],\n",
      "        [ 0.1545,  0.0029],\n",
      "        [-0.1110, -0.0252],\n",
      "        [ 0.0031,  0.0422],\n",
      "        [-0.1413,  0.1677],\n",
      "        [-0.3802, -0.0102],\n",
      "        [-0.1346,  0.2551],\n",
      "        [-0.1169, -0.0159],\n",
      "        [-0.3720, -0.2305],\n",
      "        [-0.1704, -0.0458],\n",
      "        [-0.0749, -0.0613],\n",
      "        [-0.0658, -0.0374],\n",
      "        [ 0.1113,  0.0914],\n",
      "        [-0.2251,  0.0434],\n",
      "        [-0.0789,  0.2521],\n",
      "        [ 0.2869,  0.1225],\n",
      "        [ 0.1889, -0.0174],\n",
      "        [ 0.0621, -0.0095],\n",
      "        [ 0.0971, -0.0432],\n",
      "        [-0.1187,  0.0404],\n",
      "        [-0.1634,  0.1314],\n",
      "        [-0.1832,  0.2122],\n",
      "        [-0.0309,  0.0145],\n",
      "        [-0.0967,  0.1506],\n",
      "        [ 0.1417, -0.0994],\n",
      "        [-0.0206, -0.0594],\n",
      "        [ 0.0146,  0.0370],\n",
      "        [ 0.1562, -0.0907],\n",
      "        [ 0.1215,  0.1107],\n",
      "        [ 0.1049, -0.0064],\n",
      "        [ 0.0399, -0.1853],\n",
      "        [-0.2542,  0.2647],\n",
      "        [ 0.1270, -0.0076],\n",
      "        [-0.0030,  0.2875],\n",
      "        [ 0.2119, -0.0622]]), 'fc1.bias': tensor([-0.2500,  0.0068, -0.0639,  0.1147,  0.1399,  0.0770, -0.0628, -0.0692,\n",
      "        -0.1320,  0.1769, -0.0268,  0.1523, -0.0042,  0.2343,  0.1059,  0.0132,\n",
      "         0.0957,  0.1460,  0.0540,  0.0371,  0.0712,  0.1086,  0.0403, -0.1294,\n",
      "         0.0396,  0.0030, -0.0416, -0.0510,  0.1133, -0.0879,  0.2009, -0.0298,\n",
      "        -0.1837,  0.0659,  0.0065,  0.0314, -0.1626,  0.0664, -0.1887,  0.0989,\n",
      "        -0.0544, -0.0399,  0.1109,  0.1042, -0.1505,  0.0031,  0.0234,  0.1206,\n",
      "         0.1422, -0.0768,  0.0155,  0.1308,  0.0607,  0.0847, -0.1624,  0.1174,\n",
      "        -0.1715, -0.0254, -0.0085,  0.1126, -0.2491,  0.0388,  0.0411, -0.1094,\n",
      "         0.1908, -0.0276, -0.0507, -0.0845, -0.1583,  0.0177,  0.0550, -0.1731,\n",
      "         0.1698, -0.0135,  0.0798,  0.0314, -0.1476, -0.1531,  0.0888,  0.2080,\n",
      "        -0.0417,  0.0083, -0.1619,  0.0244, -0.0349, -0.1685,  0.0443,  0.1812,\n",
      "         0.0053,  0.0293, -0.1519,  0.0649,  0.0551, -0.0677, -0.1491, -0.2804,\n",
      "        -0.0309, -0.2079,  0.3016, -0.2315]), 'fc2.weight': tensor([[ 9.2183e-04,  3.1206e-02, -5.5734e-03,  ...,  6.2910e-03,\n",
      "         -2.7572e-02,  2.2306e-02],\n",
      "        [-1.0786e-02, -1.2398e-02, -1.1620e-02,  ...,  9.0372e-03,\n",
      "          5.4773e-03, -1.4910e-02],\n",
      "        [ 2.3595e-02,  1.4427e-02,  9.8547e-03,  ...,  7.4194e-03,\n",
      "          1.3685e-02,  1.8949e-02],\n",
      "        ...,\n",
      "        [-1.9413e-02,  2.2796e-02, -1.7981e-02,  ..., -9.4602e-03,\n",
      "         -4.6082e-02,  8.2877e-03],\n",
      "        [ 2.4592e-03,  2.1860e-02, -2.7645e-02,  ..., -5.5120e-03,\n",
      "         -2.7783e-03, -2.2653e-02],\n",
      "        [ 4.0112e-03,  2.1644e-02,  2.7216e-02,  ...,  1.9611e-02,\n",
      "         -6.1817e-06, -1.6634e-02]]), 'fc2.bias': tensor([ 0.0005, -0.0028, -0.0110, -0.0217,  0.0017,  0.0042,  0.0040, -0.0374,\n",
      "         0.0078,  0.0314,  0.0318,  0.0101, -0.0154,  0.0132,  0.0126, -0.0285,\n",
      "        -0.0190, -0.0038,  0.0249,  0.0023,  0.0168,  0.0330, -0.0037,  0.0240,\n",
      "        -0.0192,  0.0191, -0.0135,  0.0348, -0.0041,  0.0221,  0.0087, -0.0027,\n",
      "         0.0192, -0.0138,  0.0413, -0.0129, -0.0076, -0.0006,  0.0007, -0.0102,\n",
      "        -0.0104, -0.0099,  0.0091, -0.0485, -0.0175,  0.0004, -0.0146, -0.0110,\n",
      "         0.0052,  0.0220, -0.0305, -0.0281, -0.0156, -0.0302,  0.0195,  0.0315,\n",
      "        -0.0442, -0.0115, -0.0144,  0.0085, -0.0192, -0.0227, -0.0419,  0.0237,\n",
      "        -0.0063,  0.0203, -0.0179,  0.0259,  0.0041, -0.0104, -0.0230,  0.0035,\n",
      "        -0.0278,  0.0062,  0.0208, -0.0113,  0.0125, -0.0030, -0.0333, -0.0024,\n",
      "         0.0145, -0.0213,  0.0216, -0.0238, -0.0110, -0.0211,  0.0090,  0.0046,\n",
      "        -0.0069,  0.0145,  0.0160, -0.0049,  0.0058,  0.0185,  0.0121, -0.0204,\n",
      "        -0.0034,  0.0189,  0.0162, -0.0194]), 'fc3.weight': tensor([[-0.0015, -0.0020,  0.0003,  ..., -0.0247, -0.0482, -0.0042],\n",
      "        [ 0.0183, -0.0129,  0.0293,  ..., -0.0243, -0.0048, -0.0006],\n",
      "        [-0.0001,  0.0006, -0.0086,  ...,  0.0251,  0.0085, -0.0216],\n",
      "        ...,\n",
      "        [-0.0127, -0.0033,  0.0158,  ..., -0.0185,  0.0108, -0.0026],\n",
      "        [-0.0409,  0.0166,  0.0127,  ...,  0.0074, -0.0022, -0.0051],\n",
      "        [-0.0093, -0.0043,  0.0083,  ...,  0.0116,  0.0056, -0.0075]]), 'fc3.bias': tensor([ 0.0002,  0.0152,  0.0312,  0.0048,  0.0606, -0.0021, -0.0074, -0.0297,\n",
      "        -0.0194,  0.0310, -0.0019, -0.0232, -0.0078,  0.0218,  0.0099, -0.0151,\n",
      "         0.0060,  0.0081,  0.0309,  0.0055, -0.0302, -0.0401,  0.0161, -0.0168,\n",
      "        -0.0152,  0.0064,  0.0070, -0.0098,  0.0027, -0.0124, -0.0200, -0.0065,\n",
      "        -0.0150,  0.0031,  0.0003, -0.0175, -0.0042,  0.0010, -0.0201,  0.0175,\n",
      "         0.0585, -0.0128,  0.0174,  0.0151,  0.0058,  0.0035,  0.0058,  0.0344,\n",
      "         0.0096, -0.0220,  0.0191,  0.0260,  0.0033,  0.0247, -0.0137, -0.0011,\n",
      "        -0.0194,  0.0050, -0.0095, -0.0067,  0.0085,  0.0176, -0.0013,  0.0139,\n",
      "         0.0060, -0.0212,  0.0321,  0.0173, -0.0209, -0.0168, -0.0160,  0.0188,\n",
      "        -0.0089, -0.0452,  0.0322, -0.0316, -0.0003,  0.0006,  0.0086, -0.0029,\n",
      "         0.0013,  0.0009, -0.0072, -0.0069,  0.0018, -0.0266,  0.0014, -0.0175,\n",
      "         0.0142,  0.0008, -0.0203,  0.0104, -0.0184,  0.0085, -0.0320,  0.0235,\n",
      "         0.0284,  0.0024, -0.0074, -0.0106]), 'fc4.weight': tensor([[-0.0112, -0.0218,  0.0174,  ...,  0.0370, -0.0016,  0.0156],\n",
      "        [ 0.0167,  0.0055,  0.0086,  ..., -0.0251,  0.0400, -0.0173],\n",
      "        [ 0.0158,  0.0099, -0.0267,  ...,  0.0048,  0.0351,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0186, -0.0206,  ...,  0.0177, -0.0180,  0.0199],\n",
      "        [ 0.0079, -0.0137,  0.0057,  ..., -0.0111,  0.0065,  0.0057],\n",
      "        [-0.0004, -0.0199,  0.0008,  ...,  0.0097, -0.0470, -0.0020]]), 'fc4.bias': tensor([ 0.0044,  0.0173,  0.0244,  0.0195,  0.0090, -0.0102, -0.0003, -0.0122,\n",
      "         0.0046,  0.0259, -0.0126,  0.0229, -0.0060,  0.0153, -0.0066,  0.0332,\n",
      "        -0.0165, -0.0006,  0.0492,  0.0544,  0.0128,  0.0034, -0.0156, -0.0466,\n",
      "        -0.0069,  0.0135,  0.0339,  0.0089, -0.0239,  0.0389, -0.0090,  0.0328,\n",
      "        -0.0056,  0.0471, -0.0091,  0.0426, -0.0040,  0.0559,  0.0443, -0.0042,\n",
      "         0.0094, -0.0125,  0.0351,  0.0235,  0.0229,  0.0374,  0.0016, -0.0030,\n",
      "        -0.0320,  0.0318]), 'fc5.weight': tensor([[ 0.0159,  0.0306,  0.0239,  0.0067, -0.0169,  0.0085,  0.0141, -0.0039,\n",
      "         -0.0442,  0.0189, -0.0003,  0.0053, -0.0581,  0.0158,  0.0069,  0.0166,\n",
      "         -0.0040,  0.0285,  0.0228,  0.0056, -0.0281, -0.0250,  0.0059,  0.0217,\n",
      "          0.0030,  0.0248,  0.0495,  0.0035,  0.0098,  0.0152, -0.0318, -0.0315,\n",
      "          0.0410, -0.0376,  0.0358,  0.0097, -0.0032, -0.0307,  0.0214, -0.0677,\n",
      "          0.0165, -0.0031,  0.0005,  0.0038, -0.0370, -0.0340, -0.0008, -0.0480,\n",
      "          0.0095,  0.0097],\n",
      "        [-0.0168,  0.0170, -0.0292, -0.0203,  0.0320,  0.0030,  0.0248, -0.0125,\n",
      "         -0.0520, -0.0415, -0.0121,  0.0292, -0.0254, -0.0238, -0.0384,  0.0059,\n",
      "         -0.0219, -0.0041,  0.0426, -0.0897, -0.0563, -0.0387, -0.0173,  0.0572,\n",
      "         -0.0164, -0.0143, -0.0322,  0.0133, -0.0219, -0.0387, -0.0368,  0.0157,\n",
      "          0.0019,  0.0470, -0.0040, -0.0217,  0.0792, -0.0230, -0.0754, -0.0113,\n",
      "         -0.0422,  0.0221, -0.0016,  0.0662,  0.0317,  0.0383,  0.0291,  0.0289,\n",
      "          0.0416,  0.0502]]), 'fc5.bias': tensor([-0.0139, -0.0051])}\n"
     ]
    }
   ],
   "source": [
    "# clients_modelx = [MLP4(2, 10, 2) for _ in range(10)]\n",
    "#\n",
    "# # 打印每个模型的内存地址\n",
    "# for i, model in enumerate(clients_modelx):\n",
    "#     print(f'Address of client model {i}: {id(model)}')\n",
    "\n",
    "print(para['1'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc1.weight': tensor([[-9.4109e-02, -5.0184e-02],\n",
      "        [ 8.4218e-02,  1.7760e-01],\n",
      "        [ 8.0057e-02,  4.1848e-02],\n",
      "        [-1.0194e-01,  1.6583e-02],\n",
      "        [-8.4368e-02,  6.7145e-02],\n",
      "        [ 1.5864e-01,  2.9677e-01],\n",
      "        [ 1.2150e-01,  8.3214e-03],\n",
      "        [-2.7230e-01, -2.3715e-01],\n",
      "        [ 1.0124e-01,  3.5261e-02],\n",
      "        [ 9.9431e-02, -3.6483e-02],\n",
      "        [-8.3909e-02, -4.8231e-02],\n",
      "        [ 2.5448e-01,  1.2851e-01],\n",
      "        [-1.4846e-01, -1.5812e-01],\n",
      "        [-6.5620e-03, -2.2906e-01],\n",
      "        [ 1.5732e-01,  9.3475e-02],\n",
      "        [ 2.3491e-02, -2.7733e-01],\n",
      "        [-6.8014e-02, -3.4375e-02],\n",
      "        [ 3.2143e-01, -1.0230e-01],\n",
      "        [ 2.9627e-01, -2.4690e-02],\n",
      "        [-7.3497e-03,  1.5566e-01],\n",
      "        [ 6.5637e-02,  1.6631e-01],\n",
      "        [-7.3942e-02,  5.1380e-02],\n",
      "        [ 2.2778e-01, -2.1423e-01],\n",
      "        [ 2.4930e-03, -6.2492e-02],\n",
      "        [ 2.0194e-01,  6.6081e-02],\n",
      "        [-1.7572e-01, -1.8422e-01],\n",
      "        [ 1.1356e-01, -9.7816e-02],\n",
      "        [ 1.7229e-01, -4.4866e-02],\n",
      "        [-1.1736e-01,  7.7569e-02],\n",
      "        [ 3.5412e-01, -1.6163e-01],\n",
      "        [ 2.5123e-02,  1.7821e-01],\n",
      "        [ 4.9737e-02,  1.7891e-01],\n",
      "        [ 6.6681e-02, -7.8926e-02],\n",
      "        [ 7.8330e-02, -1.1971e-01],\n",
      "        [-4.1306e-02, -2.2312e-01],\n",
      "        [-5.2143e-02,  1.6948e-01],\n",
      "        [-3.1901e-02, -3.8051e-02],\n",
      "        [ 9.9587e-02, -1.0255e-01],\n",
      "        [-9.0593e-02,  5.0022e-02],\n",
      "        [-1.7281e-01,  1.3104e-01],\n",
      "        [-9.3570e-02,  1.6936e-01],\n",
      "        [ 1.4852e-01, -9.4728e-02],\n",
      "        [-1.5970e-01,  2.6321e-04],\n",
      "        [-1.7496e-01,  9.5267e-02],\n",
      "        [-2.2385e-02,  8.1105e-02],\n",
      "        [-9.9610e-02, -6.3944e-02],\n",
      "        [ 3.2454e-01,  1.9099e-01],\n",
      "        [ 8.7727e-02,  3.4338e-01],\n",
      "        [-1.4540e-01, -1.4193e-01],\n",
      "        [ 2.5072e-01,  1.6854e-01],\n",
      "        [ 1.3636e-02,  1.7442e-01],\n",
      "        [ 1.5337e-01, -2.5162e-02],\n",
      "        [ 2.2570e-01,  2.5616e-02],\n",
      "        [-6.8170e-02,  1.3533e-01],\n",
      "        [-1.8247e-01,  9.2758e-02],\n",
      "        [-6.1700e-02,  1.6775e-02],\n",
      "        [-2.5360e-01, -8.6959e-02],\n",
      "        [-5.1383e-02,  2.4795e-02],\n",
      "        [ 3.0157e-02, -1.1634e-01],\n",
      "        [ 1.9473e-01, -4.3242e-02],\n",
      "        [ 7.5688e-02,  1.1624e-01],\n",
      "        [-1.6677e-01,  4.4757e-02],\n",
      "        [ 1.3898e-01,  1.1176e-01],\n",
      "        [ 2.2979e-02,  8.4994e-02],\n",
      "        [-1.6823e-02, -2.5734e-01],\n",
      "        [-1.6180e-02,  1.8063e-01],\n",
      "        [ 1.5722e-01,  1.9175e-03],\n",
      "        [-1.1185e-01, -2.5771e-02],\n",
      "        [ 3.0852e-03,  4.2190e-02],\n",
      "        [-1.4104e-01,  1.6717e-01],\n",
      "        [-3.8302e-01, -9.5818e-03],\n",
      "        [-1.3617e-01,  2.5625e-01],\n",
      "        [-1.1682e-01, -1.5844e-02],\n",
      "        [-3.7331e-01, -2.2999e-01],\n",
      "        [-1.6928e-01, -4.6603e-02],\n",
      "        [-7.6198e-02, -6.1429e-02],\n",
      "        [-6.5829e-02, -3.7396e-02],\n",
      "        [ 1.1151e-01,  9.1546e-02],\n",
      "        [-2.3017e-01,  4.3485e-02],\n",
      "        [-8.1156e-02,  2.5315e-01],\n",
      "        [ 2.8962e-01,  1.2228e-01],\n",
      "        [ 1.9154e-01, -1.7440e-02],\n",
      "        [ 6.2089e-02, -9.4946e-03],\n",
      "        [ 9.7652e-02, -4.3329e-02],\n",
      "        [-1.1852e-01,  4.0178e-02],\n",
      "        [-1.6302e-01,  1.3099e-01],\n",
      "        [-1.8512e-01,  2.1385e-01],\n",
      "        [-3.1731e-02,  1.5676e-02],\n",
      "        [-9.6665e-02,  1.4974e-01],\n",
      "        [ 1.4062e-01, -9.9184e-02],\n",
      "        [-2.0645e-02, -5.9365e-02],\n",
      "        [ 1.5748e-02,  3.6379e-02],\n",
      "        [ 1.5801e-01, -9.1346e-02],\n",
      "        [ 1.2247e-01,  1.1171e-01],\n",
      "        [ 1.0492e-01, -6.4231e-03],\n",
      "        [ 3.9865e-02, -1.8531e-01],\n",
      "        [-2.5447e-01,  2.6539e-01],\n",
      "        [ 1.2701e-01, -7.6378e-03],\n",
      "        [-2.6168e-03,  2.8756e-01],\n",
      "        [ 2.1165e-01, -6.1946e-02]]), 'fc1.bias': tensor([-0.2500,  0.0067, -0.0621,  0.1126,  0.1430,  0.0778, -0.0635, -0.0689,\n",
      "        -0.1320,  0.1777, -0.0274,  0.1541, -0.0043,  0.2368,  0.1048,  0.0141,\n",
      "         0.0960,  0.1467,  0.0542,  0.0377,  0.0708,  0.1091,  0.0412, -0.1294,\n",
      "         0.0358,  0.0034, -0.0409, -0.0521,  0.1141, -0.0867,  0.2013, -0.0311,\n",
      "        -0.1837,  0.0665,  0.0052,  0.0300, -0.1626,  0.0700, -0.1887,  0.1003,\n",
      "        -0.0544, -0.0394,  0.1132,  0.1055, -0.1505,  0.0036,  0.0243,  0.1201,\n",
      "         0.1438, -0.0764,  0.0162,  0.1341,  0.0605,  0.0859, -0.1645,  0.1206,\n",
      "        -0.1694, -0.0253, -0.0082,  0.1138, -0.2491,  0.0393,  0.0429, -0.1094,\n",
      "         0.1936, -0.0264, -0.0471, -0.0836, -0.1583,  0.0183,  0.0540, -0.1718,\n",
      "         0.1703, -0.0125,  0.0798,  0.0328, -0.1476, -0.1528,  0.0881,  0.2098,\n",
      "        -0.0394,  0.0091, -0.1619,  0.0219, -0.0352, -0.1689,  0.0482,  0.1829,\n",
      "         0.0061,  0.0282, -0.1519,  0.0673,  0.0557, -0.0669, -0.1491, -0.2804,\n",
      "        -0.0319, -0.2079,  0.3066, -0.2318]), 'fc2.weight': tensor([[ 0.0009,  0.0313, -0.0055,  ...,  0.0063, -0.0274,  0.0223],\n",
      "        [-0.0108, -0.0124, -0.0116,  ...,  0.0090,  0.0055, -0.0149],\n",
      "        [ 0.0236,  0.0151,  0.0099,  ...,  0.0074,  0.0120,  0.0189],\n",
      "        ...,\n",
      "        [-0.0194,  0.0228, -0.0180,  ..., -0.0095, -0.0461,  0.0083],\n",
      "        [ 0.0025,  0.0224, -0.0276,  ..., -0.0055, -0.0001, -0.0226],\n",
      "        [ 0.0040,  0.0233,  0.0275,  ...,  0.0196,  0.0016, -0.0166]]), 'fc2.bias': tensor([ 0.0015, -0.0011, -0.0180, -0.0217,  0.0085,  0.0062,  0.0094, -0.0304,\n",
      "         0.0187,  0.0364,  0.0454,  0.0099, -0.0154,  0.0089,  0.0222, -0.0285,\n",
      "        -0.0190,  0.0047,  0.0361,  0.0035,  0.0484,  0.0440, -0.0008,  0.0310,\n",
      "        -0.0229,  0.0256, -0.0064,  0.0641, -0.0074,  0.0216,  0.0080, -0.0027,\n",
      "         0.0129, -0.0138,  0.0637, -0.0138, -0.0017,  0.0002,  0.0051, -0.0102,\n",
      "        -0.0150, -0.0106,  0.0067, -0.0485,  0.0056,  0.0099, -0.0183, -0.0036,\n",
      "         0.0046,  0.0258, -0.0329, -0.0232, -0.0152, -0.0302,  0.0139,  0.0610,\n",
      "        -0.0442, -0.0122, -0.0144,  0.0151, -0.0192, -0.0156, -0.0419,  0.0400,\n",
      "        -0.0041,  0.0351, -0.0179,  0.0304,  0.0063, -0.0114, -0.0276, -0.0062,\n",
      "        -0.0278,  0.0022,  0.0138, -0.0092,  0.0340,  0.0150, -0.0356, -0.0067,\n",
      "         0.0131, -0.0213,  0.0211, -0.0238, -0.0152, -0.0190,  0.0260,  0.0208,\n",
      "        -0.0097,  0.0319,  0.0241,  0.0059,  0.0037,  0.0340,  0.0032, -0.0204,\n",
      "        -0.0060,  0.0190,  0.0252, -0.0128]), 'fc3.weight': tensor([[-0.0009, -0.0017,  0.0024,  ..., -0.0246, -0.0462, -0.0002],\n",
      "        [ 0.0183, -0.0129,  0.0293,  ..., -0.0243, -0.0041, -0.0005],\n",
      "        [ 0.0014,  0.0013, -0.0030,  ...,  0.0252,  0.0140, -0.0107],\n",
      "        ...,\n",
      "        [-0.0114, -0.0027,  0.0204,  ..., -0.0184,  0.0141,  0.0060],\n",
      "        [-0.0409,  0.0166,  0.0127,  ...,  0.0074, -0.0022, -0.0051],\n",
      "        [-0.0093, -0.0043,  0.0083,  ...,  0.0116,  0.0056, -0.0075]]), 'fc3.bias': tensor([ 0.0327,  0.0445,  0.1193,  0.0174,  0.2762,  0.0277, -0.0074, -0.0297,\n",
      "        -0.0194,  0.0906,  0.0144, -0.0232, -0.0078,  0.0532,  0.0507, -0.0151,\n",
      "         0.0183,  0.0243,  0.1213,  0.0571, -0.0302, -0.0401,  0.0999, -0.0168,\n",
      "        -0.0152,  0.0064,  0.1003,  0.0018,  0.0027, -0.0124, -0.0200, -0.0065,\n",
      "        -0.0150,  0.0593,  0.0330, -0.0175, -0.0042, -0.0074, -0.0201,  0.0893,\n",
      "         0.2406, -0.0128,  0.0237,  0.0580,  0.0467,  0.0282,  0.0699,  0.1382,\n",
      "         0.0863, -0.0220,  0.0499,  0.1273,  0.0759,  0.0685, -0.0139,  0.0053,\n",
      "        -0.0194,  0.0040, -0.0095, -0.0067,  0.0346,  0.1142, -0.0042,  0.0870,\n",
      "         0.0232, -0.0212,  0.1584,  0.0543, -0.0209, -0.0168, -0.0160,  0.0763,\n",
      "        -0.0089, -0.0452,  0.1766, -0.0316, -0.0003, -0.0006,  0.0544,  0.0025,\n",
      "         0.0051, -0.0032, -0.0072, -0.0069,  0.0015, -0.0266,  0.0722, -0.0175,\n",
      "         0.0671,  0.0120, -0.0203,  0.0059, -0.0184,  0.0045, -0.0320,  0.0758,\n",
      "         0.1159,  0.0245, -0.0074, -0.0106]), 'fc4.weight': tensor([[-0.0133, -0.0197,  0.0143,  ...,  0.0314, -0.0016,  0.0156],\n",
      "        [ 0.0175,  0.0088,  0.0144,  ..., -0.0257,  0.0400, -0.0173],\n",
      "        [ 0.0130,  0.0139, -0.0296,  ..., -0.0034,  0.0351,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0186, -0.0206,  ...,  0.0177, -0.0180,  0.0199],\n",
      "        [ 0.0079, -0.0137,  0.0057,  ..., -0.0111,  0.0065,  0.0057],\n",
      "        [ 0.0065, -0.0161,  0.0229,  ...,  0.0208, -0.0470, -0.0020]]), 'fc4.bias': tensor([ 0.0662,  0.1478,  0.1489,  0.1611,  0.1198, -0.0102,  0.0285, -0.0122,\n",
      "         0.0189,  0.1375, -0.0126,  0.1387, -0.0060,  0.1085, -0.0006,  0.2069,\n",
      "        -0.0165, -0.0006,  0.3325,  0.3409,  0.0681,  0.0305, -0.0156, -0.0466,\n",
      "        -0.0069,  0.0838,  0.2115,  0.0600, -0.0239,  0.2769, -0.0114,  0.2786,\n",
      "        -0.0056,  0.3703, -0.0114,  0.2422, -0.0040,  0.3847,  0.2685, -0.0145,\n",
      "         0.1133, -0.0125,  0.2680,  0.1907,  0.1689,  0.2491,  0.0016, -0.0030,\n",
      "        -0.0320,  0.2403]), 'fc5.weight': tensor([[ 1.2038e-01,  6.7353e-02,  1.8193e-01,  8.5696e-02, -1.8313e-01,\n",
      "          8.4969e-03, -2.7771e-02, -3.9345e-03, -1.9915e-02,  2.2850e-01,\n",
      "         -2.5923e-04, -6.9852e-02, -5.8111e-02,  1.4311e-01,  9.4242e-03,\n",
      "          4.5295e-02, -3.9930e-03,  2.8468e-02, -4.5240e-02,  3.0894e-01,\n",
      "          5.4286e-02,  3.1713e-02,  5.9050e-03,  2.1688e-02,  2.9803e-03,\n",
      "          1.5771e-01,  2.9946e-01, -2.7185e-02,  9.8207e-03,  1.8381e-01,\n",
      "         -3.2343e-02, -1.9551e-01,  4.1012e-02, -3.2075e-01,  3.5827e-02,\n",
      "          1.0389e-01, -3.2316e-03, -8.2713e-02,  3.2683e-01, -6.7645e-02,\n",
      "          1.8860e-01, -3.0786e-03, -8.1099e-03, -2.1255e-01, -2.5926e-01,\n",
      "         -2.9464e-01, -7.5413e-04, -4.8018e-02,  9.4549e-03, -1.3548e-01],\n",
      "        [-1.2127e-01, -1.9776e-02, -1.8719e-01, -9.9226e-02,  1.9821e-01,\n",
      "          2.9774e-03,  6.6595e-02, -1.2483e-02, -7.6328e-02, -2.5111e-01,\n",
      "         -1.2079e-02,  1.0431e-01, -2.5369e-02, -1.5110e-01, -4.0912e-02,\n",
      "         -2.2790e-02, -2.1892e-02, -4.0861e-03,  1.1061e-01, -3.9304e-01,\n",
      "         -1.3862e-01, -9.5488e-02, -1.7255e-02,  5.7234e-02, -1.6435e-02,\n",
      "         -1.4721e-01, -2.8224e-01,  4.3930e-02, -2.1873e-02, -2.0728e-01,\n",
      "         -3.6243e-02,  1.7965e-01,  1.9125e-03,  3.3012e-01, -4.0423e-03,\n",
      "         -1.1585e-01,  7.9184e-02,  2.8997e-02, -3.8085e-01, -1.1367e-02,\n",
      "         -2.1436e-01,  2.2090e-02,  6.9803e-03,  2.8257e-01,  2.5394e-01,\n",
      "          2.9900e-01,  2.9052e-02,  2.8928e-02,  4.1648e-02,  1.9536e-01]]), 'fc5.bias': tensor([-0.0518,  0.0328])}\n"
     ]
    }
   ],
   "source": [
    "print(para['30'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc1.weight': tensor([[-0.0941, -0.0502],\n",
      "        [ 0.0732,  0.2044],\n",
      "        [ 0.0878,  0.0545],\n",
      "        [-0.1068,  0.0172],\n",
      "        [-0.0888,  0.0632],\n",
      "        [ 0.1528,  0.3299],\n",
      "        [ 0.1220,  0.0109],\n",
      "        [-0.2790, -0.2463],\n",
      "        [ 0.1011,  0.0352],\n",
      "        [ 0.1051, -0.0361],\n",
      "        [-0.0865, -0.0501],\n",
      "        [ 0.2663,  0.1302],\n",
      "        [-0.1529, -0.1670],\n",
      "        [-0.0051, -0.2371],\n",
      "        [ 0.1615,  0.0951],\n",
      "        [ 0.0244, -0.3272],\n",
      "        [-0.0755, -0.0378],\n",
      "        [ 0.3570, -0.1114],\n",
      "        [ 0.3408, -0.0248],\n",
      "        [-0.0059,  0.1755],\n",
      "        [ 0.0666,  0.1680],\n",
      "        [-0.0677,  0.0497],\n",
      "        [ 0.2377, -0.2298],\n",
      "        [ 0.0025, -0.0625],\n",
      "        [ 0.2351,  0.0543],\n",
      "        [-0.1842, -0.1947],\n",
      "        [ 0.1221, -0.1094],\n",
      "        [ 0.1821, -0.0452],\n",
      "        [-0.1198,  0.0763],\n",
      "        [ 0.3946, -0.1584],\n",
      "        [ 0.0330,  0.1846],\n",
      "        [ 0.0509,  0.1800],\n",
      "        [ 0.0667, -0.0789],\n",
      "        [ 0.0885, -0.1336],\n",
      "        [-0.0418, -0.2593],\n",
      "        [-0.0501,  0.1877],\n",
      "        [-0.0319, -0.0381],\n",
      "        [ 0.1028, -0.1106],\n",
      "        [-0.0906,  0.0500],\n",
      "        [-0.1864,  0.1423],\n",
      "        [-0.0960,  0.1785],\n",
      "        [ 0.1648, -0.0969],\n",
      "        [-0.1814, -0.0022],\n",
      "        [-0.1915,  0.1017],\n",
      "        [-0.0224,  0.0811],\n",
      "        [-0.1159, -0.0586],\n",
      "        [ 0.3411,  0.1838],\n",
      "        [ 0.0708,  0.3942],\n",
      "        [-0.1569, -0.1485],\n",
      "        [ 0.2642,  0.1736],\n",
      "        [ 0.0139,  0.2038],\n",
      "        [ 0.1649, -0.0267],\n",
      "        [ 0.2510,  0.0184],\n",
      "        [-0.0775,  0.1456],\n",
      "        [-0.1796,  0.0897],\n",
      "        [-0.0640,  0.0147],\n",
      "        [-0.2660, -0.1045],\n",
      "        [-0.0553,  0.0282],\n",
      "        [ 0.0302, -0.1364],\n",
      "        [ 0.2040, -0.0429],\n",
      "        [ 0.0757,  0.1162],\n",
      "        [-0.1865,  0.0372],\n",
      "        [ 0.1487,  0.1142],\n",
      "        [ 0.0230,  0.0850],\n",
      "        [-0.0191, -0.2682],\n",
      "        [-0.0141,  0.2079],\n",
      "        [ 0.1781, -0.0037],\n",
      "        [-0.1177, -0.0325],\n",
      "        [ 0.0031,  0.0422],\n",
      "        [-0.1393,  0.1726],\n",
      "        [-0.4431, -0.0115],\n",
      "        [-0.1523,  0.2665],\n",
      "        [-0.1252, -0.0166],\n",
      "        [-0.4255, -0.2071],\n",
      "        [-0.1750, -0.0527],\n",
      "        [-0.0862, -0.0662],\n",
      "        [-0.0658, -0.0374],\n",
      "        [ 0.1125,  0.0925],\n",
      "        [-0.2874,  0.0299],\n",
      "        [-0.0993,  0.2630],\n",
      "        [ 0.3343,  0.1059],\n",
      "        [ 0.2215, -0.0230],\n",
      "        [ 0.0621, -0.0095],\n",
      "        [ 0.1121, -0.0402],\n",
      "        [-0.1249,  0.0380],\n",
      "        [-0.1681,  0.1366],\n",
      "        [-0.2013,  0.2232],\n",
      "        [-0.0348,  0.0175],\n",
      "        [-0.0873,  0.1627],\n",
      "        [ 0.1438, -0.1001],\n",
      "        [-0.0206, -0.0594],\n",
      "        [ 0.0191,  0.0355],\n",
      "        [ 0.1833, -0.0957],\n",
      "        [ 0.1305,  0.1212],\n",
      "        [ 0.1049, -0.0064],\n",
      "        [ 0.0399, -0.1853],\n",
      "        [-0.2600,  0.2806],\n",
      "        [ 0.1270, -0.0076],\n",
      "        [-0.0011,  0.2927],\n",
      "        [ 0.2109, -0.0613]]), 'fc1.bias': tensor([-0.2500,  0.0005, -0.0544,  0.1117,  0.1481,  0.0535, -0.0625, -0.0786,\n",
      "        -0.1321,  0.1827, -0.0253,  0.1581, -0.0049,  0.2421,  0.1048,  0.0130,\n",
      "         0.0962,  0.1255,  0.0308,  0.0215,  0.0715,  0.1109,  0.0551, -0.1294,\n",
      "         0.0090,  0.0047, -0.0425, -0.0411,  0.1182, -0.0526,  0.2074, -0.0297,\n",
      "        -0.1837,  0.0614,  0.0040,  0.0112, -0.1626,  0.0739, -0.1887,  0.0942,\n",
      "        -0.0473, -0.0315,  0.1035,  0.0991, -0.1505,  0.0035,  0.0251,  0.0625,\n",
      "         0.1473, -0.0725,  0.0056,  0.1422,  0.0382,  0.0805, -0.1683,  0.1262,\n",
      "        -0.1551, -0.0216, -0.0010,  0.1189, -0.2491,  0.0230,  0.0471, -0.1094,\n",
      "         0.1978, -0.0189, -0.0276, -0.0763, -0.1583,  0.0208,  0.0358, -0.1625,\n",
      "         0.1735, -0.0010,  0.0711,  0.0339, -0.1476, -0.1517,  0.0447,  0.2048,\n",
      "        -0.0184,  0.0035, -0.1619,  0.0086, -0.0285, -0.1623,  0.0602,  0.1895,\n",
      "         0.0057,  0.0282, -0.1519,  0.0702,  0.0437, -0.0620, -0.1491, -0.2804,\n",
      "        -0.0461, -0.2079,  0.3164, -0.2326]), 'fc2.weight': tensor([[ 0.0009,  0.0339, -0.0047,  ...,  0.0063, -0.0268,  0.0224],\n",
      "        [-0.0108, -0.0121, -0.0116,  ...,  0.0090,  0.0050, -0.0149],\n",
      "        [ 0.0236,  0.0192,  0.0098,  ...,  0.0074,  0.0070,  0.0189],\n",
      "        ...,\n",
      "        [-0.0194,  0.0228, -0.0180,  ..., -0.0095, -0.0459,  0.0083],\n",
      "        [ 0.0025,  0.0282, -0.0262,  ..., -0.0055,  0.0012, -0.0225],\n",
      "        [ 0.0040,  0.0455,  0.0332,  ...,  0.0196, -0.0046, -0.0160]]), 'fc2.bias': tensor([ 0.0035, -0.0011, -0.0268, -0.0217,  0.0217,  0.0099,  0.0061, -0.0370,\n",
      "         0.0324,  0.0492,  0.0628,  0.0098, -0.0154, -0.0058,  0.0322, -0.0285,\n",
      "        -0.0190, -0.0022,  0.0873, -0.0018,  0.0753,  0.0600, -0.0074,  0.0383,\n",
      "        -0.0235,  0.0775, -0.0065,  0.0918, -0.0121,  0.0226,  0.0002, -0.0027,\n",
      "         0.0032, -0.0138,  0.0963, -0.0141, -0.0092, -0.0040,  0.0125, -0.0102,\n",
      "        -0.0193, -0.0113,  0.0027, -0.0485,  0.0301,  0.0022, -0.0205, -0.0044,\n",
      "         0.0012,  0.0372, -0.0219, -0.0167, -0.0092, -0.0302,  0.0145,  0.0974,\n",
      "        -0.0442, -0.0155, -0.0144,  0.0381, -0.0192, -0.0105, -0.0419,  0.0539,\n",
      "        -0.0130,  0.0573, -0.0179,  0.0380,  0.0260, -0.0085, -0.0283, -0.0082,\n",
      "        -0.0278, -0.0056,  0.0012, -0.0155,  0.0549,  0.0257, -0.0363, -0.0101,\n",
      "         0.0121, -0.0213,  0.0170, -0.0238, -0.0189, -0.0124,  0.0406,  0.0281,\n",
      "        -0.0089,  0.0475,  0.0254,  0.0103,  0.0020,  0.0448, -0.0086, -0.0204,\n",
      "        -0.0012,  0.0210,  0.0325, -0.0198]), 'fc3.weight': tensor([[ 0.0021, -0.0004,  0.0123,  ..., -0.0244, -0.0398,  0.0207],\n",
      "        [ 0.0177, -0.0132,  0.0269,  ..., -0.0244, -0.0047, -0.0050],\n",
      "        [ 0.0085,  0.0045,  0.0208,  ...,  0.0257,  0.0297,  0.0393],\n",
      "        ...,\n",
      "        [-0.0050,  0.0003,  0.0422,  ..., -0.0179,  0.0270,  0.0512],\n",
      "        [-0.0409,  0.0166,  0.0127,  ...,  0.0074, -0.0022, -0.0051],\n",
      "        [-0.0093, -0.0043,  0.0083,  ...,  0.0116,  0.0056, -0.0075]]), 'fc3.bias': tensor([ 4.2763e-02,  6.3872e-02,  1.5303e-01,  5.5825e-02,  4.0312e-01,\n",
      "         2.5534e-02, -7.4201e-03, -2.9652e-02, -1.9450e-02,  1.2367e-01,\n",
      "        -1.2005e-03, -2.3236e-02, -7.8351e-03,  7.2284e-02,  5.5904e-02,\n",
      "        -1.3085e-02,  2.9998e-02,  8.7362e-02,  1.7145e-01,  6.8426e-02,\n",
      "        -3.0198e-02, -4.0095e-02,  1.4945e-01, -1.6814e-02, -1.5217e-02,\n",
      "         6.3541e-03,  1.6658e-01,  2.0487e-04,  2.7086e-03, -1.2424e-02,\n",
      "        -2.0038e-02, -6.4717e-03, -1.5030e-02,  8.8546e-02,  1.1256e-01,\n",
      "        -1.7472e-02, -4.1514e-03, -1.8114e-02, -2.6372e-02,  1.2829e-01,\n",
      "         3.4944e-01, -6.8796e-03,  7.2652e-03,  7.0074e-02,  6.5954e-02,\n",
      "         9.3513e-02,  1.0227e-01,  2.0297e-01,  1.3057e-01, -2.1975e-02,\n",
      "         6.9140e-02,  1.9044e-01,  1.1034e-01,  8.9115e-02, -2.4868e-02,\n",
      "         6.1678e-03, -1.9431e-02,  3.9738e-03, -9.5430e-03, -6.7396e-03,\n",
      "         1.1528e-01,  1.6754e-01, -6.0263e-03,  1.1764e-01,  5.2737e-02,\n",
      "        -2.1169e-02,  2.1330e-01,  6.7454e-02, -2.0922e-02, -1.6809e-02,\n",
      "        -1.5984e-02,  1.1112e-01, -8.8652e-03, -4.5167e-02,  2.5332e-01,\n",
      "        -3.1577e-02, -2.8257e-04, -2.1780e-03,  6.3518e-02,  1.5032e-03,\n",
      "         1.3827e-03,  4.4954e-03, -7.1675e-03, -6.8551e-03,  1.5413e-03,\n",
      "        -2.6603e-02,  1.1563e-01, -1.7489e-02,  8.2997e-02,  4.1417e-02,\n",
      "        -2.0324e-02, -1.9966e-04, -1.8417e-02,  6.9410e-03, -3.2042e-02,\n",
      "         9.4382e-02,  1.5996e-01,  1.7269e-02, -7.3988e-03, -1.0554e-02]), 'fc4.weight': tensor([[-0.0197, -0.0169,  0.0013,  ...,  0.0178, -0.0016,  0.0156],\n",
      "        [ 0.0161,  0.0109,  0.0128,  ..., -0.0301,  0.0400, -0.0173],\n",
      "        [ 0.0036,  0.0183, -0.0485,  ..., -0.0239,  0.0351,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0187, -0.0201,  ...,  0.0180, -0.0180,  0.0199],\n",
      "        [ 0.0079, -0.0137,  0.0057,  ..., -0.0111,  0.0065,  0.0057],\n",
      "        [ 0.0174, -0.0174,  0.0481,  ...,  0.0409, -0.0470, -0.0020]]), 'fc4.bias': tensor([ 0.0898,  0.1750,  0.1886,  0.1958,  0.1136, -0.0102,  0.0281, -0.0122,\n",
      "         0.0252,  0.1821, -0.0156,  0.1454, -0.0053,  0.1406,  0.0046,  0.2360,\n",
      "        -0.0114, -0.0006,  0.3666,  0.4245,  0.0871,  0.0503, -0.0156, -0.0466,\n",
      "        -0.0069,  0.1120,  0.2714,  0.0637, -0.0319,  0.3372, -0.0114,  0.2970,\n",
      "        -0.0072,  0.3832, -0.0177,  0.2836, -0.0040,  0.4275,  0.3429, -0.0183,\n",
      "         0.1524, -0.0125,  0.3042,  0.1882,  0.1598,  0.2453,  0.0016, -0.0019,\n",
      "        -0.0320,  0.2527]), 'fc5.weight': tensor([[ 0.1660,  0.0894,  0.2473,  0.1234, -0.2436,  0.0085, -0.0465, -0.0039,\n",
      "         -0.0091,  0.3270, -0.0006, -0.0857, -0.0582,  0.1988,  0.0195,  0.0700,\n",
      "         -0.0012,  0.0285, -0.0491,  0.4493,  0.0848,  0.0632,  0.0059,  0.0217,\n",
      "          0.0030,  0.2193,  0.4031, -0.0346,  0.0097,  0.2652, -0.0369, -0.2470,\n",
      "          0.0410, -0.4072,  0.0358,  0.1558, -0.0032, -0.0887,  0.4593, -0.0676,\n",
      "          0.2539, -0.0031, -0.0022, -0.2899, -0.3304, -0.3901, -0.0008, -0.0481,\n",
      "          0.0095, -0.1816],\n",
      "        [-0.1669, -0.0419, -0.2526, -0.1369,  0.2587,  0.0030,  0.0854, -0.0125,\n",
      "         -0.0872, -0.3496, -0.0118,  0.1202, -0.0253, -0.2068, -0.0509, -0.0475,\n",
      "         -0.0247, -0.0041,  0.1144, -0.5334, -0.1691, -0.1269, -0.0173,  0.0572,\n",
      "         -0.0164, -0.2088, -0.3859,  0.0513, -0.0217, -0.2886, -0.0317,  0.2311,\n",
      "          0.0019,  0.4166, -0.0040, -0.1678,  0.0792,  0.0350, -0.5133, -0.0114,\n",
      "         -0.2797,  0.0221,  0.0011,  0.3599,  0.3250,  0.3944,  0.0291,  0.0290,\n",
      "          0.0416,  0.2415]]), 'fc5.bias': tensor([-0.0037, -0.0154])}\n"
     ]
    }
   ],
   "source": [
    "print(para['50'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc1.weight': tensor([[-0.0941, -0.0502],\n",
      "        [ 0.0839,  0.1771],\n",
      "        [ 0.0786,  0.0402],\n",
      "        [-0.1010,  0.0164],\n",
      "        [-0.0846,  0.0678],\n",
      "        [ 0.1583,  0.2954],\n",
      "        [ 0.1220,  0.0080],\n",
      "        [-0.2725, -0.2369],\n",
      "        [ 0.1013,  0.0353],\n",
      "        [ 0.0989, -0.0372],\n",
      "        [-0.0847, -0.0482],\n",
      "        [ 0.2537,  0.1293],\n",
      "        [-0.1480, -0.1581],\n",
      "        [-0.0069, -0.2284],\n",
      "        [ 0.1576,  0.0945],\n",
      "        [ 0.0234, -0.2758],\n",
      "        [-0.0671, -0.0346],\n",
      "        [ 0.3184, -0.1017],\n",
      "        [ 0.2932, -0.0248],\n",
      "        [-0.0075,  0.1546],\n",
      "        [ 0.0659,  0.1669],\n",
      "        [-0.0754,  0.0517],\n",
      "        [ 0.2269, -0.2135],\n",
      "        [ 0.0025, -0.0625],\n",
      "        [ 0.1994,  0.0671],\n",
      "        [-0.1752, -0.1841],\n",
      "        [ 0.1129, -0.0969],\n",
      "        [ 0.1733, -0.0447],\n",
      "        [-0.1179,  0.0789],\n",
      "        [ 0.3524, -0.1611],\n",
      "        [ 0.0244,  0.1774],\n",
      "        [ 0.0487,  0.1807],\n",
      "        [ 0.0667, -0.0789],\n",
      "        [ 0.0775, -0.1181],\n",
      "        [-0.0415, -0.2219],\n",
      "        [-0.0521,  0.1696],\n",
      "        [-0.0319, -0.0381],\n",
      "        [ 0.0996, -0.1026],\n",
      "        [-0.0906,  0.0500],\n",
      "        [-0.1718,  0.1307],\n",
      "        [-0.0935,  0.1695],\n",
      "        [ 0.1474, -0.0946],\n",
      "        [-0.1581,  0.0006],\n",
      "        [-0.1732,  0.0950],\n",
      "        [-0.0224,  0.0811],\n",
      "        [-0.0995, -0.0634],\n",
      "        [ 0.3266,  0.1897],\n",
      "        [ 0.0877,  0.3411],\n",
      "        [-0.1449, -0.1426],\n",
      "        [ 0.2500,  0.1681],\n",
      "        [ 0.0132,  0.1721],\n",
      "        [ 0.1521, -0.0251],\n",
      "        [ 0.2248,  0.0258],\n",
      "        [-0.0673,  0.1341],\n",
      "        [-0.1840,  0.0945],\n",
      "        [-0.0619,  0.0167],\n",
      "        [-0.2519, -0.0849],\n",
      "        [-0.0511,  0.0245],\n",
      "        [ 0.0299, -0.1157],\n",
      "        [ 0.1944, -0.0434],\n",
      "        [ 0.0757,  0.1162],\n",
      "        [-0.1656,  0.0451],\n",
      "        [ 0.1381,  0.1114],\n",
      "        [ 0.0230,  0.0850],\n",
      "        [-0.0161, -0.2571],\n",
      "        [-0.0167,  0.1790],\n",
      "        [ 0.1545,  0.0029],\n",
      "        [-0.1110, -0.0252],\n",
      "        [ 0.0031,  0.0422],\n",
      "        [-0.1413,  0.1677],\n",
      "        [-0.3802, -0.0102],\n",
      "        [-0.1346,  0.2551],\n",
      "        [-0.1169, -0.0159],\n",
      "        [-0.3720, -0.2305],\n",
      "        [-0.1704, -0.0458],\n",
      "        [-0.0749, -0.0613],\n",
      "        [-0.0658, -0.0374],\n",
      "        [ 0.1113,  0.0913],\n",
      "        [-0.2251,  0.0434],\n",
      "        [-0.0789,  0.2521],\n",
      "        [ 0.2869,  0.1225],\n",
      "        [ 0.1889, -0.0174],\n",
      "        [ 0.0621, -0.0095],\n",
      "        [ 0.0971, -0.0432],\n",
      "        [-0.1187,  0.0404],\n",
      "        [-0.1634,  0.1314],\n",
      "        [-0.1832,  0.2122],\n",
      "        [-0.0309,  0.0145],\n",
      "        [-0.0967,  0.1506],\n",
      "        [ 0.1417, -0.0994],\n",
      "        [-0.0206, -0.0594],\n",
      "        [ 0.0145,  0.0370],\n",
      "        [ 0.1562, -0.0907],\n",
      "        [ 0.1215,  0.1107],\n",
      "        [ 0.1049, -0.0064],\n",
      "        [ 0.0399, -0.1853],\n",
      "        [-0.2542,  0.2647],\n",
      "        [ 0.1270, -0.0076],\n",
      "        [-0.0030,  0.2875],\n",
      "        [ 0.2119, -0.0622]]), 'fc1.bias': tensor([-0.2500,  0.0068, -0.0640,  0.1147,  0.1399,  0.0770, -0.0628, -0.0693,\n",
      "        -0.1320,  0.1769, -0.0268,  0.1523, -0.0042,  0.2343,  0.1059,  0.0132,\n",
      "         0.0957,  0.1460,  0.0540,  0.0371,  0.0712,  0.1086,  0.0403, -0.1294,\n",
      "         0.0396,  0.0030, -0.0417, -0.0510,  0.1133, -0.0879,  0.2009, -0.0298,\n",
      "        -0.1837,  0.0659,  0.0065,  0.0314, -0.1626,  0.0663, -0.1887,  0.0989,\n",
      "        -0.0544, -0.0399,  0.1109,  0.1042, -0.1505,  0.0031,  0.0234,  0.1206,\n",
      "         0.1422, -0.0768,  0.0155,  0.1308,  0.0607,  0.0847, -0.1624,  0.1174,\n",
      "        -0.1715, -0.0254, -0.0085,  0.1126, -0.2491,  0.0387,  0.0411, -0.1094,\n",
      "         0.1908, -0.0276, -0.0507, -0.0845, -0.1583,  0.0177,  0.0551, -0.1731,\n",
      "         0.1698, -0.0135,  0.0798,  0.0314, -0.1476, -0.1531,  0.0889,  0.2079,\n",
      "        -0.0417,  0.0083, -0.1619,  0.0244, -0.0349, -0.1684,  0.0443,  0.1812,\n",
      "         0.0053,  0.0293, -0.1519,  0.0649,  0.0551, -0.0677, -0.1491, -0.2804,\n",
      "        -0.0309, -0.2079,  0.3016, -0.2315]), 'fc2.weight': tensor([[ 9.2183e-04,  3.1206e-02, -5.5734e-03,  ...,  6.2910e-03,\n",
      "         -2.7572e-02,  2.2306e-02],\n",
      "        [-1.0786e-02, -1.2398e-02, -1.1620e-02,  ...,  9.0372e-03,\n",
      "          5.4778e-03, -1.4910e-02],\n",
      "        [ 2.3595e-02,  1.4429e-02,  9.8547e-03,  ...,  7.4194e-03,\n",
      "          1.3725e-02,  1.8949e-02],\n",
      "        ...,\n",
      "        [-1.9413e-02,  2.2796e-02, -1.7981e-02,  ..., -9.4602e-03,\n",
      "         -4.6082e-02,  8.2877e-03],\n",
      "        [ 2.4592e-03,  2.1857e-02, -2.7644e-02,  ..., -5.5120e-03,\n",
      "         -2.8034e-03, -2.2653e-02],\n",
      "        [ 4.0112e-03,  2.1645e-02,  2.7216e-02,  ...,  1.9611e-02,\n",
      "         -2.7765e-06, -1.6634e-02]]), 'fc2.bias': tensor([ 0.0005, -0.0028, -0.0109, -0.0217,  0.0017,  0.0043,  0.0040, -0.0374,\n",
      "         0.0077,  0.0315,  0.0317,  0.0101, -0.0154,  0.0132,  0.0127, -0.0285,\n",
      "        -0.0190, -0.0038,  0.0249,  0.0023,  0.0167,  0.0329, -0.0037,  0.0240,\n",
      "        -0.0191,  0.0191, -0.0135,  0.0348, -0.0040,  0.0220,  0.0087, -0.0027,\n",
      "         0.0193, -0.0138,  0.0413, -0.0129, -0.0076, -0.0006,  0.0007, -0.0102,\n",
      "        -0.0104, -0.0099,  0.0091, -0.0485, -0.0176,  0.0004, -0.0146, -0.0110,\n",
      "         0.0052,  0.0220, -0.0305, -0.0281, -0.0156, -0.0302,  0.0195,  0.0314,\n",
      "        -0.0442, -0.0115, -0.0144,  0.0085, -0.0192, -0.0227, -0.0419,  0.0237,\n",
      "        -0.0063,  0.0202, -0.0179,  0.0260,  0.0041, -0.0104, -0.0229,  0.0036,\n",
      "        -0.0278,  0.0062,  0.0208, -0.0113,  0.0125, -0.0031, -0.0333, -0.0024,\n",
      "         0.0145, -0.0213,  0.0216, -0.0238, -0.0110, -0.0211,  0.0089,  0.0045,\n",
      "        -0.0069,  0.0145,  0.0160, -0.0049,  0.0058,  0.0185,  0.0122, -0.0204,\n",
      "        -0.0034,  0.0189,  0.0161, -0.0193]), 'fc3.weight': tensor([[-0.0015, -0.0020,  0.0003,  ..., -0.0247, -0.0482, -0.0042],\n",
      "        [ 0.0183, -0.0129,  0.0293,  ..., -0.0243, -0.0048, -0.0005],\n",
      "        [-0.0001,  0.0006, -0.0086,  ...,  0.0251,  0.0085, -0.0216],\n",
      "        ...,\n",
      "        [-0.0127, -0.0033,  0.0157,  ..., -0.0185,  0.0108, -0.0026],\n",
      "        [-0.0409,  0.0166,  0.0127,  ...,  0.0074, -0.0022, -0.0051],\n",
      "        [-0.0093, -0.0043,  0.0083,  ...,  0.0116,  0.0056, -0.0075]]), 'fc3.bias': tensor([ 1.2570e-04,  1.5307e-02,  3.0920e-02,  5.2040e-03,  5.9447e-02,\n",
      "        -2.4039e-03, -7.4201e-03, -2.9652e-02, -1.9450e-02,  3.0950e-02,\n",
      "        -2.0676e-03, -2.3236e-02, -7.8351e-03,  2.1595e-02,  9.4609e-03,\n",
      "        -1.5112e-02,  6.0985e-03,  8.3865e-03,  3.0821e-02,  5.4741e-03,\n",
      "        -3.0198e-02, -4.0095e-02,  1.5196e-02, -1.6814e-02, -1.5217e-02,\n",
      "         6.3541e-03,  6.2332e-03, -9.9825e-03,  2.7114e-03, -1.2424e-02,\n",
      "        -2.0038e-02, -6.4717e-03, -1.5030e-02,  2.8340e-03, -7.2240e-05,\n",
      "        -1.7472e-02, -4.1514e-03,  1.3867e-03, -2.0136e-02,  1.7298e-02,\n",
      "         5.7848e-02, -1.2768e-02,  1.8119e-02,  1.5074e-02,  5.4920e-03,\n",
      "         3.1518e-03,  5.3844e-03,  3.3847e-02,  8.8669e-03, -2.1975e-02,\n",
      "         1.8863e-02,  2.5482e-02,  2.7897e-03,  2.4482e-02, -1.3714e-02,\n",
      "        -1.1212e-03, -1.9422e-02,  5.0767e-03, -9.5430e-03, -6.7396e-03,\n",
      "         8.5386e-03,  1.6755e-02, -1.2825e-03,  1.3173e-02,  5.4084e-03,\n",
      "        -2.1169e-02,  3.1654e-02,  1.7495e-02, -2.0922e-02, -1.6809e-02,\n",
      "        -1.5984e-02,  1.8684e-02, -8.8623e-03, -4.5167e-02,  3.0987e-02,\n",
      "        -3.1577e-02, -2.8257e-04,  1.3466e-03,  8.2348e-03, -2.8993e-03,\n",
      "         1.2924e-03,  1.3454e-03, -7.1675e-03, -6.8551e-03,  1.9081e-03,\n",
      "        -2.6603e-02,  1.8825e-04, -1.7489e-02,  1.4110e-02,  8.4795e-04,\n",
      "        -2.0324e-02,  1.0490e-02, -1.8417e-02,  8.7942e-03, -3.2042e-02,\n",
      "         2.3473e-02,  2.7925e-02,  1.2636e-03, -7.3988e-03, -1.0554e-02]), 'fc4.weight': tensor([[-0.0112, -0.0219,  0.0172,  ...,  0.0370, -0.0016,  0.0156],\n",
      "        [ 0.0167,  0.0055,  0.0086,  ..., -0.0251,  0.0400, -0.0173],\n",
      "        [ 0.0158,  0.0098, -0.0268,  ...,  0.0048,  0.0351,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0186, -0.0206,  ...,  0.0177, -0.0180,  0.0199],\n",
      "        [ 0.0079, -0.0137,  0.0057,  ..., -0.0111,  0.0065,  0.0057],\n",
      "        [-0.0004, -0.0199,  0.0007,  ...,  0.0097, -0.0470, -0.0020]]), 'fc4.bias': tensor([-0.0013,  0.0145,  0.0212,  0.0164,  0.0012, -0.0102, -0.0021, -0.0122,\n",
      "         0.0035,  0.0229, -0.0126,  0.0204, -0.0060,  0.0130, -0.0066,  0.0291,\n",
      "        -0.0165, -0.0006,  0.0430,  0.0472,  0.0113,  0.0026, -0.0156, -0.0466,\n",
      "        -0.0069,  0.0104,  0.0293,  0.0078, -0.0239,  0.0333, -0.0090,  0.0279,\n",
      "        -0.0056,  0.0407, -0.0091,  0.0378, -0.0040,  0.0486,  0.0386, -0.0042,\n",
      "         0.0010, -0.0125,  0.0301,  0.0204,  0.0172,  0.0331,  0.0016, -0.0030,\n",
      "        -0.0320,  0.0274]), 'fc5.weight': tensor([[ 0.0147,  0.0297,  0.0205,  0.0049, -0.0149,  0.0085,  0.0145, -0.0039,\n",
      "         -0.0444,  0.0148, -0.0003,  0.0067, -0.0581,  0.0131,  0.0069,  0.0158,\n",
      "         -0.0040,  0.0285,  0.0239, -0.0008, -0.0299, -0.0260,  0.0059,  0.0217,\n",
      "          0.0030,  0.0223,  0.0441,  0.0041,  0.0098,  0.0117, -0.0318, -0.0285,\n",
      "          0.0410, -0.0322,  0.0358,  0.0076, -0.0032, -0.0302,  0.0150, -0.0677,\n",
      "          0.0137, -0.0031,  0.0003,  0.0079, -0.0328, -0.0292, -0.0008, -0.0480,\n",
      "          0.0095,  0.0123],\n",
      "        [-0.0155,  0.0179, -0.0257, -0.0185,  0.0299,  0.0030,  0.0243, -0.0125,\n",
      "         -0.0518, -0.0374, -0.0121,  0.0277, -0.0254, -0.0211, -0.0384,  0.0067,\n",
      "         -0.0219, -0.0041,  0.0414, -0.0833, -0.0544, -0.0378, -0.0173,  0.0572,\n",
      "         -0.0164, -0.0118, -0.0269,  0.0126, -0.0219, -0.0351, -0.0368,  0.0127,\n",
      "          0.0019,  0.0416, -0.0040, -0.0196,  0.0792, -0.0235, -0.0690, -0.0113,\n",
      "         -0.0394,  0.0221, -0.0014,  0.0621,  0.0274,  0.0336,  0.0291,  0.0289,\n",
      "          0.0416,  0.0476]]), 'fc5.bias': tensor([-0.0156, -0.0035])}\n"
     ]
    }
   ],
   "source": [
    "print(para['0'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
