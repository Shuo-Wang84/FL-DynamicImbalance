{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Incremental_sampling2(object):\n",
    "    def __init__(self,save_prototype_nums = 30):\n",
    "        self.save_prototype_nums = save_prototype_nums\n",
    "        self.incremental_prototypes = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.combined_map = {}\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"\n",
    "        上一轮获得的数据原型和这一轮的新数据进行合并\n",
    "        1.首先，按不同类别把数据和原型进行分开\n",
    "        2.判断是不是新出现的类别的数据\n",
    "            2.1、新类别数据,在原型map中直接扩展一个新类的map,{'新类':新类的数据}\n",
    "            2.2、原先类别的数据,在对应类的数据上进行扩展，{'已有类':已有数据+新数据}\n",
    "        \"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_prototypes:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_prototypes[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_prototypes[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_prototypes[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "        self.combined_map = self.incremental_prototypes\n",
    "    def cut_down_nearest_data_eu(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                # 计算成对距离\n",
    "                pairwise_distances = squareform(pdist(data, 'euclidean'))\n",
    "                np.fill_diagonal(pairwise_distances, np.inf)  # 将自身距离设置为无穷大，忽略自身距离\n",
    "\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 找到距离最小的一对\n",
    "                    min_dist_indices = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n",
    "                    # 保留一个样本，删除另一个\n",
    "                    data = np.delete(data, min_dist_indices[1], axis=0)\n",
    "                    # 从距离矩阵中删除对应的行和列\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=0)\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=1)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            # self.incremental_prototypes 在这之前已经和新数据合并了\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 建立 KD 树\n",
    "                    kdtree = KDTree(data)\n",
    "                    # 查询每个点的最近邻\n",
    "                    distances, indices = kdtree.query(data, k=2)  # k=2 因为第一个最近邻是点本身\n",
    "\n",
    "                    # 找到最近的两个点\n",
    "                    min_dist_idx = np.argmin(distances[:, 1])  # distances[:, 1] 是每个点的最近邻距离\n",
    "                    nearest_idx = indices[min_dist_idx, 1]\n",
    "\n",
    "                    # 删除其中一个点\n",
    "                    data = np.delete(data, nearest_idx, axis=0)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def compute_sampling_nums(self,combined_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combined_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def triplet_sampling(self, num_cluster, n_neighbors=5, randomOr=True, len_lim=True):\n",
    "        \"\"\"Triplets 数据采样\"\"\"\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        for label in self.incremental_prototypes:\n",
    "            data_min = self.incremental_prototypes[label]\n",
    "            if len(data_min) < num_cluster:\n",
    "                size = num_cluster - len(data_min)\n",
    "                weight = np.ones(len(data_min))\n",
    "                # 收集多数类样本\n",
    "                data_maj = np.vstack([self.incremental_prototypes[l] for l in self.incremental_prototypes if l != label])\n",
    "                gen_x_c, gen_y_c = self._sample_one(data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim)\n",
    "                gen_x += gen_x_c\n",
    "                gen_y += gen_y_c\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label in self.combined_map: # incremental_prototypes样本太少了，只是为了生成样本用的\n",
    "            data = self.incremental_prototypes[label]\n",
    "            resampling_data.append(data)\n",
    "            resampling_label.extend([label] * len(data))\n",
    "        resampling_data = np.vstack(resampling_data)\n",
    "        resampling_label = np.array(resampling_label)\n",
    "        if len(gen_x) > 0:\n",
    "            gen_x = np.vstack(gen_x)\n",
    "            gen_y = np.array(gen_y)\n",
    "            resampling_data = np.concatenate((resampling_data, gen_x), axis=0)\n",
    "            resampling_label = np.concatenate((resampling_label, gen_y), axis=0)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def _sample_one(self, data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        min_idxs = np.arange(len(data_min))\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(data_maj)\n",
    "        _, indices = nbrs.kneighbors(data_min)\n",
    "\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = data_min[min_idxs[j]]\n",
    "            tp2 = data_maj[indices[j][:5]].mean(axis=0)\n",
    "            tp3_ord = np.random.randint(n_neighbors)\n",
    "            tp3 = data_maj[indices[j][tp3_ord]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(label)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            offset_norm = norm(offset)\n",
    "            if offset_norm == 0:\n",
    "                continue\n",
    "\n",
    "            tp1_tp2_norm = norm(tp1 - tp2)\n",
    "            if tp1_tp2_norm == 0:\n",
    "                continue\n",
    "\n",
    "            if len_lim: offset = offset * min(1, tp1_tp2_norm / offset_norm)\n",
    "            coef = np.random.rand() if randomOr else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(label)\n",
    "\n",
    "        return gen_x, gen_y\n",
    "\n",
    "    def random_sampling(self,num_cluster,sampling_strategy):\n",
    "        \"\"\"\n",
    "        这里需要对数据采样\n",
    "        首先遍历self.incremental_prototypes ,每个类\n",
    "        以及每个类的数据的长度\n",
    "        然后比较每个类的数据的长度和num_cluster之间的差距\n",
    "        差额部分使用triplets的核心算法对齐进行生成样本\n",
    "        \"\"\"\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        if sampling_strategy == \"ros-p\":\n",
    "            # 随机上采样，差额数据从原型数据中随机复制\n",
    "            for label, data in self.combined_map.items():\n",
    "                if len(data) < num_cluster:  # 需要上采样的数据的条件\n",
    "                    prototype_data = self.incremental_prototypes[label]\n",
    "                    sampling_nums = num_cluster - len(data)\n",
    "\n",
    "                    if len(prototype_data) < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(len(prototype_data)), min(needed, len(prototype_data)))\n",
    "                            sampled_data.extend([prototype_data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(len(prototype_data)), sampling_nums)\n",
    "                        sampled_data = [prototype_data[i] for i in sampled_indices]\n",
    "\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        elif sampling_strategy == \"ros-h\" :\n",
    "            # 随机上采样，从混合数据中采样差额数据\n",
    "            for label, data in self.combined_map.items():\n",
    "                data_len = len(data)\n",
    "                if data_len < num_cluster:\n",
    "                    # 那就是下采样了,不放回采样\n",
    "                    sampling_nums = num_cluster-data_len\n",
    "                    if data_len < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(data_len), min(needed, data_len))\n",
    "                            sampled_data.extend([data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(data_len), sampling_nums)\n",
    "                        sampled_data = [data[i] for i in sampled_indices]\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    # 直接复制\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "        self.incremental_prototypes = last_round_prototype\n",
    "        self.data = new_data\n",
    "        self.label = new_data_label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_prototypes)\n",
    "        if sampling_strategy.lower() == \"tpl\":\n",
    "            resampling_data,resampling_label = self.triplet_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"ros-p\":\n",
    "            # rest data copied from prototype\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-p\")\n",
    "            print('use ros p')\n",
    "        elif sampling_strategy.lower() == \"ros-h\":\n",
    "            # rest data copied from hybrid data(combined data)\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-h\")\n",
    "            print('use ros h')\n",
    "        return resampling_data,resampling_label,self.incremental_prototypes\n",
    "\n",
    "def train_model_FedAvg_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "def train_model_FedProx_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    mu = 0.1\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "    # not use deepcopy ,because model as parameter transport in this ,update model also update model\n",
    "    # current_local_model = cmodel\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # FedProx\n",
    "        prox_term = 0.0\n",
    "        for p_i, param in enumerate(model.parameters()):\n",
    "                prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "        loss += prox_term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "    return losses\n",
    "\n",
    "def train_model_FedNova_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau +=len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    return losses, coeff, norm_grad,len(X_train_tensor)\n",
    "# 定义模型参数共享函数\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    # return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "\n",
    "# # 定义模型参数聚合函数\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def save_loss(loss_list,client_id,round_id,save_loss_path):\n",
    "    if not os.path.exists(save_loss_path):\n",
    "        os.makedirs(save_loss_path)\n",
    "    # 构建文件路径\n",
    "    file_path = os.path.join(save_loss_path, f\"client_{client_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 CSV 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 将损失值添加到 DataFrame 中\n",
    "    column_name = f'round_{round_id}'\n",
    "    df[column_name] = loss_list\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def save_model(global_model,round_id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'round_{round_id}_gm.pt')\n",
    "    torch.save(global_model,model_path)\n",
    "\n",
    "def save_metrics(title, rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    isap = Incremental_sampling2()\n",
    "    # resampling_data,resampling_label,prototype_map = isap.fit(last_round_prototype=prototype_map,data=raw_X,label=raw_y)\n",
    "    # def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "    def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "        self.incremental_prototypes = last_round_prototype\n",
    "        self.data = new_data\n",
    "        self.label = new_data_label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_prototypes)\n",
    "        if sampling_strategy.lower() == \"tpl\":\n",
    "            resampling_data,resampling_label = self.triplet_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"ros-p\":\n",
    "            # rest data copied from prototype\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-p\")\n",
    "        elif sampling_strategy.lower() == \"ros-h\":\n",
    "            # rest data copied from hybrid data(combined data)\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-h\")\n",
    "        return resampling_data,resampling_label,self.incremental_prototypes\n",
    "    resampling_data,resampling_label,prototype_map = isap.fit(new_data=raw_X,new_data_label=raw_y,last_round_prototype=prototype_map,sampling_strategy=cluster_strategy)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "def read_data_return_tensor(dataset_path, round_id, client_id, sampling_strategy='no',prototype_map = {},cluster_strategy='kmeans'):\n",
    "    folder_path = os.path.join(dataset_path, f'client_{client_id}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    open_file_path = os.path.join(folder_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :-1].values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    raw_y = raw_y.astype(float)\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    if sampling_strategy == 'IncrementalSampling2':\n",
    "        print(\"using IncrementalSampling\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy=cluster_strategy)\n",
    "    # else:\n",
    "    #     resampling_X,resampling_y = data_sampling(raw_X,raw_y,sampling_strategy)\n",
    "    #     resampling_y.astype(float)\n",
    "    resampling_X = np.array(resampling_X)  # 将列表转换为单个NumPy数组\n",
    "    resampling_y = np.array(resampling_y)  # 将列表转换为单个NumPy数组\n",
    "\n",
    "    X_train_tensor = torch.tensor(resampling_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(resampling_y, dtype=torch.long)   # 标签张量\n",
    "    return X_train_tensor,y_train_tensor,prototype_map\n",
    "def read_test_data(test_data_path):\n",
    "    data = pd.read_csv(test_data_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    X = data_shuffled.iloc[:, :-1].values.astype(float)  # 特征\n",
    "    # print(X)\n",
    "    y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    y = y.astype(float)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X, dtype=torch.float32)  # 特征张量\n",
    "    y_test_tensor = torch.tensor(y, dtype=torch.long)   # 标签张量\n",
    "    return X_test_tensor,y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/200], Loss: 0.0731\n",
      "Epoch [151/200], Loss: 0.0731\n",
      "Epoch [161/200], Loss: 0.0730\n",
      "Epoch [171/200], Loss: 0.0729\n",
      "Epoch [181/200], Loss: 0.0728\n",
      "Epoch [191/200], Loss: 0.0727\n",
      "Test Accuracy: 97.13%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0780\n",
      "Epoch [11/200], Loss: 0.0782\n",
      "Epoch [21/200], Loss: 0.0774\n",
      "Epoch [31/200], Loss: 0.0776\n",
      "Epoch [41/200], Loss: 0.0776\n",
      "Epoch [51/200], Loss: 0.0775\n",
      "Epoch [61/200], Loss: 0.0770\n",
      "Epoch [71/200], Loss: 0.0774\n",
      "Epoch [81/200], Loss: 0.0769\n",
      "Epoch [91/200], Loss: 0.0769\n",
      "Epoch [101/200], Loss: 0.0768\n",
      "Epoch [111/200], Loss: 0.0768\n",
      "Epoch [121/200], Loss: 0.0764\n",
      "Epoch [131/200], Loss: 0.0767\n",
      "Epoch [141/200], Loss: 0.0764\n",
      "Epoch [151/200], Loss: 0.0764\n",
      "Epoch [161/200], Loss: 0.0762\n",
      "Epoch [171/200], Loss: 0.0762\n",
      "Epoch [181/200], Loss: 0.0760\n",
      "Epoch [191/200], Loss: 0.0759\n",
      "Test Accuracy: 96.38%\n",
      "Test Accuracy: 96.58%\n",
      "gme acc: {'recall': array([0.93842803, 0.99218103, 1.        , 1.        , 0.9979571 ]), 'recall_micro': 0.9658291275483439, 'recall_macro': 0.9857132330959706, 'precision': array([0.99996153, 0.99916662, 0.99987323, 0.98158714, 0.15005376]), 'precision_micro': 0.9658291275483439, 'precision_macro': 0.8261284541105361, 'f1_score': array([0.9682181 , 0.99566157, 0.99993661, 0.99070802, 0.26088117]), 'g_mean': 0.9854185946527466, 'acc': 0.9658291275483439, 'auc': 0.9826732668607088, 'kappa': 0.941485071349982, 'confusion_matrix': array([[77989,    58,     0,     0,  5059],\n",
      "       [    1, 69538,     1,    71,   475],\n",
      "       [    0,     0,  7887,     0,     0],\n",
      "       [    0,     0,     0,  3785,     0],\n",
      "       [    2,     0,     0,     0,   977]], dtype=int64)}\n",
      "round193\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0756\n",
      "Epoch [11/200], Loss: 0.0752\n",
      "Epoch [21/200], Loss: 0.0748\n",
      "Epoch [31/200], Loss: 0.0746\n",
      "Epoch [41/200], Loss: 0.0743\n",
      "Epoch [51/200], Loss: 0.0742\n",
      "Epoch [61/200], Loss: 0.0741\n",
      "Epoch [71/200], Loss: 0.0739\n",
      "Epoch [81/200], Loss: 0.0738\n",
      "Epoch [91/200], Loss: 0.0737\n",
      "Epoch [101/200], Loss: 0.0736\n",
      "Epoch [111/200], Loss: 0.0735\n",
      "Epoch [121/200], Loss: 0.0735\n",
      "Epoch [131/200], Loss: 0.0734\n",
      "Epoch [141/200], Loss: 0.0733\n",
      "Epoch [151/200], Loss: 0.0732\n",
      "Epoch [161/200], Loss: 0.0732\n",
      "Epoch [171/200], Loss: 0.0731\n",
      "Epoch [181/200], Loss: 0.0730\n",
      "Epoch [191/200], Loss: 0.0730\n",
      "Test Accuracy: 96.64%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0776\n",
      "Epoch [11/200], Loss: 0.0781\n",
      "Epoch [21/200], Loss: 0.0776\n",
      "Epoch [31/200], Loss: 0.0776\n",
      "Epoch [41/200], Loss: 0.0771\n",
      "Epoch [51/200], Loss: 0.0771\n",
      "Epoch [61/200], Loss: 0.0766\n",
      "Epoch [71/200], Loss: 0.0768\n",
      "Epoch [81/200], Loss: 0.0764\n",
      "Epoch [91/200], Loss: 0.0765\n",
      "Epoch [101/200], Loss: 0.0761\n",
      "Epoch [111/200], Loss: 0.0762\n",
      "Epoch [121/200], Loss: 0.0759\n",
      "Epoch [131/200], Loss: 0.0760\n",
      "Epoch [141/200], Loss: 0.0757\n",
      "Epoch [151/200], Loss: 0.0758\n",
      "Epoch [161/200], Loss: 0.0755\n",
      "Epoch [171/200], Loss: 0.0756\n",
      "Epoch [181/200], Loss: 0.0754\n",
      "Epoch [191/200], Loss: 0.0754\n",
      "Test Accuracy: 96.69%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0716\n",
      "Epoch [11/200], Loss: 0.0718\n",
      "Epoch [21/200], Loss: 0.0715\n",
      "Epoch [31/200], Loss: 0.0716\n",
      "Epoch [41/200], Loss: 0.0713\n",
      "Epoch [51/200], Loss: 0.0713\n",
      "Epoch [61/200], Loss: 0.0713\n",
      "Epoch [71/200], Loss: 0.0713\n",
      "Epoch [81/200], Loss: 0.0711\n",
      "Epoch [91/200], Loss: 0.0711\n",
      "Epoch [101/200], Loss: 0.0710\n",
      "Epoch [111/200], Loss: 0.0709\n",
      "Epoch [121/200], Loss: 0.0708\n",
      "Epoch [131/200], Loss: 0.0707\n",
      "Epoch [141/200], Loss: 0.0706\n",
      "Epoch [151/200], Loss: 0.0705\n",
      "Epoch [161/200], Loss: 0.0705\n",
      "Epoch [171/200], Loss: 0.0704\n",
      "Epoch [181/200], Loss: 0.0703\n",
      "Epoch [191/200], Loss: 0.0703\n",
      "Test Accuracy: 96.26%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0725\n",
      "Epoch [11/200], Loss: 0.0723\n",
      "Epoch [21/200], Loss: 0.0721\n",
      "Epoch [31/200], Loss: 0.0719\n",
      "Epoch [41/200], Loss: 0.0718\n",
      "Epoch [51/200], Loss: 0.0717\n",
      "Epoch [61/200], Loss: 0.0716\n",
      "Epoch [71/200], Loss: 0.0715\n",
      "Epoch [81/200], Loss: 0.0714\n",
      "Epoch [91/200], Loss: 0.0713\n",
      "Epoch [101/200], Loss: 0.0713\n",
      "Epoch [111/200], Loss: 0.0713\n",
      "Epoch [121/200], Loss: 0.0712\n",
      "Epoch [131/200], Loss: 0.0711\n",
      "Epoch [141/200], Loss: 0.0710\n",
      "Epoch [151/200], Loss: 0.0709\n",
      "Epoch [161/200], Loss: 0.0709\n",
      "Epoch [171/200], Loss: 0.0708\n",
      "Epoch [181/200], Loss: 0.0707\n",
      "Epoch [191/200], Loss: 0.0707\n",
      "Test Accuracy: 96.09%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0663\n",
      "Epoch [11/200], Loss: 0.0659\n",
      "Epoch [21/200], Loss: 0.0656\n",
      "Epoch [31/200], Loss: 0.0655\n",
      "Epoch [41/200], Loss: 0.0653\n",
      "Epoch [51/200], Loss: 0.0652\n",
      "Epoch [61/200], Loss: 0.0650\n",
      "Epoch [71/200], Loss: 0.0650\n",
      "Epoch [81/200], Loss: 0.0648\n",
      "Epoch [91/200], Loss: 0.0648\n",
      "Epoch [101/200], Loss: 0.0646\n",
      "Epoch [111/200], Loss: 0.0646\n",
      "Epoch [121/200], Loss: 0.0645\n",
      "Epoch [131/200], Loss: 0.0645\n",
      "Epoch [141/200], Loss: 0.0643\n",
      "Epoch [151/200], Loss: 0.0643\n",
      "Epoch [161/200], Loss: 0.0642\n",
      "Epoch [171/200], Loss: 0.0641\n",
      "Epoch [181/200], Loss: 0.0640\n",
      "Epoch [191/200], Loss: 0.0640\n",
      "Test Accuracy: 95.87%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0810\n",
      "Epoch [11/200], Loss: 0.0810\n",
      "Epoch [21/200], Loss: 0.0803\n",
      "Epoch [31/200], Loss: 0.0801\n",
      "Epoch [41/200], Loss: 0.0799\n",
      "Epoch [51/200], Loss: 0.0797\n",
      "Epoch [61/200], Loss: 0.0796\n",
      "Epoch [71/200], Loss: 0.0794\n",
      "Epoch [81/200], Loss: 0.0792\n",
      "Epoch [91/200], Loss: 0.0791\n",
      "Epoch [101/200], Loss: 0.0789\n",
      "Epoch [111/200], Loss: 0.0787\n",
      "Epoch [121/200], Loss: 0.0786\n",
      "Epoch [131/200], Loss: 0.0785\n",
      "Epoch [141/200], Loss: 0.0784\n",
      "Epoch [151/200], Loss: 0.0782\n",
      "Epoch [161/200], Loss: 0.0781\n",
      "Epoch [171/200], Loss: 0.0780\n",
      "Epoch [181/200], Loss: 0.0779\n",
      "Epoch [191/200], Loss: 0.0778\n",
      "Test Accuracy: 96.53%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0807\n",
      "Epoch [11/200], Loss: 0.0802\n",
      "Epoch [21/200], Loss: 0.0800\n",
      "Epoch [31/200], Loss: 0.0799\n",
      "Epoch [41/200], Loss: 0.0797\n",
      "Epoch [51/200], Loss: 0.0796\n",
      "Epoch [61/200], Loss: 0.0795\n",
      "Epoch [71/200], Loss: 0.0794\n",
      "Epoch [81/200], Loss: 0.0792\n",
      "Epoch [91/200], Loss: 0.0791\n",
      "Epoch [101/200], Loss: 0.0790\n",
      "Epoch [111/200], Loss: 0.0789\n",
      "Epoch [121/200], Loss: 0.0788\n",
      "Epoch [131/200], Loss: 0.0787\n",
      "Epoch [141/200], Loss: 0.0786\n",
      "Epoch [151/200], Loss: 0.0785\n",
      "Epoch [161/200], Loss: 0.0784\n",
      "Epoch [171/200], Loss: 0.0784\n",
      "Epoch [181/200], Loss: 0.0783\n",
      "Epoch [191/200], Loss: 0.0782\n",
      "Test Accuracy: 96.88%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0726\n",
      "Epoch [11/200], Loss: 0.0729\n",
      "Epoch [21/200], Loss: 0.0727\n",
      "Epoch [31/200], Loss: 0.0725\n",
      "Epoch [41/200], Loss: 0.0724\n",
      "Epoch [51/200], Loss: 0.0722\n",
      "Epoch [61/200], Loss: 0.0721\n",
      "Epoch [71/200], Loss: 0.0719\n",
      "Epoch [81/200], Loss: 0.0718\n",
      "Epoch [91/200], Loss: 0.0717\n",
      "Epoch [101/200], Loss: 0.0716\n",
      "Epoch [111/200], Loss: 0.0715\n",
      "Epoch [121/200], Loss: 0.0714\n",
      "Epoch [131/200], Loss: 0.0713\n",
      "Epoch [141/200], Loss: 0.0712\n",
      "Epoch [151/200], Loss: 0.0711\n",
      "Epoch [161/200], Loss: 0.0710\n",
      "Epoch [171/200], Loss: 0.0710\n",
      "Epoch [181/200], Loss: 0.0709\n",
      "Epoch [191/200], Loss: 0.0708\n",
      "Test Accuracy: 96.63%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0742\n",
      "Epoch [11/200], Loss: 0.0745\n",
      "Epoch [21/200], Loss: 0.0737\n",
      "Epoch [31/200], Loss: 0.0737\n",
      "Epoch [41/200], Loss: 0.0739\n",
      "Epoch [51/200], Loss: 0.0738\n",
      "Epoch [61/200], Loss: 0.0732\n",
      "Epoch [71/200], Loss: 0.0733\n",
      "Epoch [81/200], Loss: 0.0731\n",
      "Epoch [91/200], Loss: 0.0730\n",
      "Epoch [101/200], Loss: 0.0728\n",
      "Epoch [111/200], Loss: 0.0729\n",
      "Epoch [121/200], Loss: 0.0726\n",
      "Epoch [131/200], Loss: 0.0727\n",
      "Epoch [141/200], Loss: 0.0726\n",
      "Epoch [151/200], Loss: 0.0726\n",
      "Epoch [161/200], Loss: 0.0723\n",
      "Epoch [171/200], Loss: 0.0724\n",
      "Epoch [181/200], Loss: 0.0722\n",
      "Epoch [191/200], Loss: 0.0723\n",
      "Test Accuracy: 96.37%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0773\n",
      "Epoch [11/200], Loss: 0.0781\n",
      "Epoch [21/200], Loss: 0.0774\n",
      "Epoch [31/200], Loss: 0.0778\n",
      "Epoch [41/200], Loss: 0.0774\n",
      "Epoch [51/200], Loss: 0.0774\n",
      "Epoch [61/200], Loss: 0.0771\n",
      "Epoch [71/200], Loss: 0.0773\n",
      "Epoch [81/200], Loss: 0.0767\n",
      "Epoch [91/200], Loss: 0.0772\n",
      "Epoch [101/200], Loss: 0.0767\n",
      "Epoch [111/200], Loss: 0.0767\n",
      "Epoch [121/200], Loss: 0.0766\n",
      "Epoch [131/200], Loss: 0.0767\n",
      "Epoch [141/200], Loss: 0.0764\n",
      "Epoch [151/200], Loss: 0.0764\n",
      "Epoch [161/200], Loss: 0.0763\n",
      "Epoch [171/200], Loss: 0.0762\n",
      "Epoch [181/200], Loss: 0.0760\n",
      "Epoch [191/200], Loss: 0.0757\n",
      "Test Accuracy: 96.34%\n",
      "Test Accuracy: 96.50%\n",
      "gme acc: {'recall': array([0.93677953, 0.99210969, 1.        , 1.        , 0.99897855]), 'recall_micro': 0.9649789258515584, 'recall_macro': 0.9855735555830819, 'precision': array([0.99997431, 0.99913784, 0.99987323, 0.98158714, 0.14702345]), 'precision_micro': 0.9649789258515584, 'precision_macro': 0.8255191937776267, 'f1_score': array([0.96734592, 0.99561137, 0.99993661, 0.99070802, 0.25632289]), 'g_mean': 0.9852595431029053, 'acc': 0.9649789258515584, 'auc': 0.9822442746141036, 'kappa': 0.9400722286881443, 'confusion_matrix': array([[77852,    60,     0,     0,  5194],\n",
      "       [    1, 69533,     1,    71,   480],\n",
      "       [    0,     0,  7887,     0,     0],\n",
      "       [    0,     0,     0,  3785,     0],\n",
      "       [    1,     0,     0,     0,   978]], dtype=int64)}\n",
      "round194\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0752\n",
      "Epoch [11/200], Loss: 0.0746\n",
      "Epoch [21/200], Loss: 0.0742\n",
      "Epoch [31/200], Loss: 0.0740\n",
      "Epoch [41/200], Loss: 0.0737\n",
      "Epoch [51/200], Loss: 0.0736\n",
      "Epoch [61/200], Loss: 0.0735\n",
      "Epoch [71/200], Loss: 0.0733\n",
      "Epoch [81/200], Loss: 0.0732\n",
      "Epoch [91/200], Loss: 0.0731\n",
      "Epoch [101/200], Loss: 0.0730\n",
      "Epoch [111/200], Loss: 0.0730\n",
      "Epoch [121/200], Loss: 0.0729\n",
      "Epoch [131/200], Loss: 0.0728\n",
      "Epoch [141/200], Loss: 0.0727\n",
      "Epoch [151/200], Loss: 0.0727\n",
      "Epoch [161/200], Loss: 0.0726\n",
      "Epoch [171/200], Loss: 0.0725\n",
      "Epoch [181/200], Loss: 0.0725\n",
      "Epoch [191/200], Loss: 0.0724\n",
      "Test Accuracy: 96.65%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0773\n",
      "Epoch [11/200], Loss: 0.0773\n",
      "Epoch [21/200], Loss: 0.0766\n",
      "Epoch [31/200], Loss: 0.0768\n",
      "Epoch [41/200], Loss: 0.0762\n",
      "Epoch [51/200], Loss: 0.0763\n",
      "Epoch [61/200], Loss: 0.0759\n",
      "Epoch [71/200], Loss: 0.0760\n",
      "Epoch [81/200], Loss: 0.0756\n",
      "Epoch [91/200], Loss: 0.0757\n",
      "Epoch [101/200], Loss: 0.0753\n",
      "Epoch [111/200], Loss: 0.0754\n",
      "Epoch [121/200], Loss: 0.0751\n",
      "Epoch [131/200], Loss: 0.0752\n",
      "Epoch [141/200], Loss: 0.0749\n",
      "Epoch [151/200], Loss: 0.0750\n",
      "Epoch [161/200], Loss: 0.0747\n",
      "Epoch [171/200], Loss: 0.0748\n",
      "Epoch [181/200], Loss: 0.0746\n",
      "Epoch [191/200], Loss: 0.0746\n",
      "Test Accuracy: 96.73%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0713\n",
      "Epoch [11/200], Loss: 0.0712\n",
      "Epoch [21/200], Loss: 0.0709\n",
      "Epoch [31/200], Loss: 0.0710\n",
      "Epoch [41/200], Loss: 0.0707\n",
      "Epoch [51/200], Loss: 0.0708\n",
      "Epoch [61/200], Loss: 0.0707\n",
      "Epoch [71/200], Loss: 0.0707\n",
      "Epoch [81/200], Loss: 0.0705\n",
      "Epoch [91/200], Loss: 0.0704\n",
      "Epoch [101/200], Loss: 0.0704\n",
      "Epoch [111/200], Loss: 0.0703\n",
      "Epoch [121/200], Loss: 0.0702\n",
      "Epoch [131/200], Loss: 0.0701\n",
      "Epoch [141/200], Loss: 0.0700\n",
      "Epoch [151/200], Loss: 0.0699\n",
      "Epoch [161/200], Loss: 0.0699\n",
      "Epoch [171/200], Loss: 0.0698\n",
      "Epoch [181/200], Loss: 0.0698\n",
      "Epoch [191/200], Loss: 0.0697\n",
      "Test Accuracy: 96.27%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0725\n",
      "Epoch [11/200], Loss: 0.0720\n",
      "Epoch [21/200], Loss: 0.0717\n",
      "Epoch [31/200], Loss: 0.0716\n",
      "Epoch [41/200], Loss: 0.0714\n",
      "Epoch [51/200], Loss: 0.0714\n",
      "Epoch [61/200], Loss: 0.0713\n",
      "Epoch [71/200], Loss: 0.0712\n",
      "Epoch [81/200], Loss: 0.0711\n",
      "Epoch [91/200], Loss: 0.0710\n",
      "Epoch [101/200], Loss: 0.0710\n",
      "Epoch [111/200], Loss: 0.0709\n",
      "Epoch [121/200], Loss: 0.0709\n",
      "Epoch [131/200], Loss: 0.0708\n",
      "Epoch [141/200], Loss: 0.0707\n",
      "Epoch [151/200], Loss: 0.0706\n",
      "Epoch [161/200], Loss: 0.0705\n",
      "Epoch [171/200], Loss: 0.0705\n",
      "Epoch [181/200], Loss: 0.0704\n",
      "Epoch [191/200], Loss: 0.0704\n",
      "Test Accuracy: 96.08%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0661\n",
      "Epoch [11/200], Loss: 0.0656\n",
      "Epoch [21/200], Loss: 0.0652\n",
      "Epoch [31/200], Loss: 0.0652\n",
      "Epoch [41/200], Loss: 0.0650\n",
      "Epoch [51/200], Loss: 0.0649\n",
      "Epoch [61/200], Loss: 0.0647\n",
      "Epoch [71/200], Loss: 0.0646\n",
      "Epoch [81/200], Loss: 0.0644\n",
      "Epoch [91/200], Loss: 0.0644\n",
      "Epoch [101/200], Loss: 0.0643\n",
      "Epoch [111/200], Loss: 0.0643\n",
      "Epoch [121/200], Loss: 0.0641\n",
      "Epoch [131/200], Loss: 0.0641\n",
      "Epoch [141/200], Loss: 0.0639\n",
      "Epoch [151/200], Loss: 0.0639\n",
      "Epoch [161/200], Loss: 0.0638\n",
      "Epoch [171/200], Loss: 0.0638\n",
      "Epoch [181/200], Loss: 0.0636\n",
      "Epoch [191/200], Loss: 0.0636\n",
      "Test Accuracy: 95.89%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0806\n",
      "Epoch [11/200], Loss: 0.0801\n",
      "Epoch [21/200], Loss: 0.0795\n",
      "Epoch [31/200], Loss: 0.0793\n",
      "Epoch [41/200], Loss: 0.0791\n",
      "Epoch [51/200], Loss: 0.0789\n",
      "Epoch [61/200], Loss: 0.0788\n",
      "Epoch [71/200], Loss: 0.0786\n",
      "Epoch [81/200], Loss: 0.0784\n",
      "Epoch [91/200], Loss: 0.0782\n",
      "Epoch [101/200], Loss: 0.0781\n",
      "Epoch [111/200], Loss: 0.0779\n",
      "Epoch [121/200], Loss: 0.0778\n",
      "Epoch [131/200], Loss: 0.0777\n",
      "Epoch [141/200], Loss: 0.0776\n",
      "Epoch [151/200], Loss: 0.0774\n",
      "Epoch [161/200], Loss: 0.0773\n",
      "Epoch [171/200], Loss: 0.0772\n",
      "Epoch [181/200], Loss: 0.0771\n",
      "Epoch [191/200], Loss: 0.0770\n",
      "Test Accuracy: 96.55%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0811\n",
      "Epoch [11/200], Loss: 0.0803\n",
      "Epoch [21/200], Loss: 0.0801\n",
      "Epoch [31/200], Loss: 0.0799\n",
      "Epoch [41/200], Loss: 0.0798\n",
      "Epoch [51/200], Loss: 0.0796\n",
      "Epoch [61/200], Loss: 0.0796\n",
      "Epoch [71/200], Loss: 0.0794\n",
      "Epoch [81/200], Loss: 0.0793\n",
      "Epoch [91/200], Loss: 0.0792\n",
      "Epoch [101/200], Loss: 0.0791\n",
      "Epoch [111/200], Loss: 0.0790\n",
      "Epoch [121/200], Loss: 0.0789\n",
      "Epoch [131/200], Loss: 0.0788\n",
      "Epoch [141/200], Loss: 0.0787\n",
      "Epoch [151/200], Loss: 0.0786\n",
      "Epoch [161/200], Loss: 0.0786\n",
      "Epoch [171/200], Loss: 0.0785\n",
      "Epoch [181/200], Loss: 0.0784\n",
      "Epoch [191/200], Loss: 0.0783\n",
      "Test Accuracy: 96.94%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0721\n",
      "Epoch [11/200], Loss: 0.0722\n",
      "Epoch [21/200], Loss: 0.0720\n",
      "Epoch [31/200], Loss: 0.0718\n",
      "Epoch [41/200], Loss: 0.0717\n",
      "Epoch [51/200], Loss: 0.0715\n",
      "Epoch [61/200], Loss: 0.0714\n",
      "Epoch [71/200], Loss: 0.0713\n",
      "Epoch [81/200], Loss: 0.0711\n",
      "Epoch [91/200], Loss: 0.0710\n",
      "Epoch [101/200], Loss: 0.0709\n",
      "Epoch [111/200], Loss: 0.0708\n",
      "Epoch [121/200], Loss: 0.0707\n",
      "Epoch [131/200], Loss: 0.0706\n",
      "Epoch [141/200], Loss: 0.0705\n",
      "Epoch [151/200], Loss: 0.0704\n",
      "Epoch [161/200], Loss: 0.0704\n",
      "Epoch [171/200], Loss: 0.0703\n",
      "Epoch [181/200], Loss: 0.0702\n",
      "Epoch [191/200], Loss: 0.0701\n",
      "Test Accuracy: 96.64%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0742\n",
      "Epoch [11/200], Loss: 0.0741\n",
      "Epoch [21/200], Loss: 0.0736\n",
      "Epoch [31/200], Loss: 0.0734\n",
      "Epoch [41/200], Loss: 0.0735\n",
      "Epoch [51/200], Loss: 0.0732\n",
      "Epoch [61/200], Loss: 0.0730\n",
      "Epoch [71/200], Loss: 0.0732\n",
      "Epoch [81/200], Loss: 0.0728\n",
      "Epoch [91/200], Loss: 0.0727\n",
      "Epoch [101/200], Loss: 0.0724\n",
      "Epoch [111/200], Loss: 0.0726\n",
      "Epoch [121/200], Loss: 0.0726\n",
      "Epoch [131/200], Loss: 0.0724\n",
      "Epoch [141/200], Loss: 0.0724\n",
      "Epoch [151/200], Loss: 0.0721\n",
      "Epoch [161/200], Loss: 0.0722\n",
      "Epoch [171/200], Loss: 0.0719\n",
      "Epoch [181/200], Loss: 0.0720\n",
      "Epoch [191/200], Loss: 0.0718\n",
      "Test Accuracy: 96.33%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "use ros h\n",
      "Epoch [1/200], Loss: 0.0769\n",
      "Epoch [11/200], Loss: 0.0774\n",
      "Epoch [21/200], Loss: 0.0767\n",
      "Epoch [31/200], Loss: 0.0770\n",
      "Epoch [41/200], Loss: 0.0764\n",
      "Epoch [51/200], Loss: 0.0768\n",
      "Epoch [61/200], Loss: 0.0762\n",
      "Epoch [71/200], Loss: 0.0763\n",
      "Epoch [81/200], Loss: 0.0762\n",
      "Epoch [91/200], Loss: 0.0762\n",
      "Epoch [101/200], Loss: 0.0758\n",
      "Epoch [111/200], Loss: 0.0761\n",
      "Epoch [121/200], Loss: 0.0758\n",
      "Epoch [131/200], Loss: 0.0758\n",
      "Epoch [141/200], Loss: 0.0756\n",
      "Epoch [151/200], Loss: 0.0756\n",
      "Epoch [161/200], Loss: 0.0754\n",
      "Epoch [171/200], Loss: 0.0754\n",
      "Epoch [181/200], Loss: 0.0752\n",
      "Epoch [191/200], Loss: 0.0751\n",
      "Test Accuracy: 96.37%\n",
      "Test Accuracy: 96.53%\n",
      "gme acc: {'recall': array([0.93722475, 0.99225238, 1.        , 1.        , 0.99897855]), 'recall_micro': 0.9652623264171536, 'recall_macro': 0.9856911348617409, 'precision': array([0.99997432, 0.99913797, 0.99987323, 0.98184176, 0.14804723]), 'precision_micro': 0.9652623264171536, 'precision_macro': 0.8257749020339427, 'f1_score': array([0.96758325, 0.99568327, 0.99993661, 0.9908377 , 0.25787739]), 'g_mean': 0.9853815167420668, 'acc': 0.9652623264171536, 'auc': 0.9823868688578091, 'kappa': 0.9405433947071487, 'confusion_matrix': array([[77889,    60,     0,     0,  5157],\n",
      "       [    1, 69543,     1,    70,   471],\n",
      "       [    0,     0,  7887,     0,     0],\n",
      "       [    0,     0,     0,  3785,     0],\n",
      "       [    1,     0,     0,     0,   978]], dtype=int64)}\n",
      "Total execution time: 47523.148163080215 seconds\n",
      "[47503.13124418259]\n"
     ]
    }
   ],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName,cluster_strategy):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}kdt_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}kdt_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}kdt_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i],cluster_strategy=cluster_strategy)\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list =['IncrementalSampling'] # ['no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client']#,'pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "cluster_strategy_list = ['ros-h'] # 'ros-p','ros-p']# ['gmm','kmeans++','OPTICS','meanshift'] # 'kmeans','kmeans++','OPTICS','meanshift','spectral','kmedoids',\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, cluster_strategy in enumerate(cluster_strategy_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName='IncrementalSampling2', settingName=settingname,cluster_strategy=cluster_strategy)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "# kdt ros-p(11hours)\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)\n",
    "# [34400.61084651947, 36640.83967113495]\n",
    "# Total execution time: 69354.06012558937 seconds\n",
    "# [33072.67344403267, 36241.37223672867]\n",
    "# import time\n",
    "#\n",
    "# sampling_strategy_name_list = ['IncrementalSampling2']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "# dataset_list = ['pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "#\n",
    "# total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "# ex_time_list = []\n",
    "# for j, settingname in enumerate(dataset_list):\n",
    "#     for i, saplingname in enumerate(sampling_strategy_name_list):\n",
    "#         start_time = time.time()  # 记录当前迭代的开始时间\n",
    "#         runFedNova(samplingName=saplingname, settingName=settingname,cluster_strategy='is2_triplets')\n",
    "#         end_time = time.time()  # 记录当前迭代的结束时间\n",
    "#         execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "#         ex_time_list.append(execution_time)\n",
    "#         time.sleep(20)\n",
    "#\n",
    "# total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "# total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "#\n",
    "# print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "# print(ex_time_list)\n",
    "# Total execution time: 69354.06012558937 seconds\n",
    "# [33072.67344403267, 36241.37223672867]\n",
    "# Total execution time: 36974.078832149506 seconds\n",
    "# [36954.06793355942]\n",
    "# 没有电风扇冷却\n",
    "# Total execution time: 47523.148163080215 seconds\n",
    "# [47503.13124418259]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v;\n"
     ]
    }
   ],
   "source": [
    "print('v;')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%p\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
