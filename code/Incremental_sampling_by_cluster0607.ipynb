{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, MeanShift ,SpectralClustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hdbscan\n",
    "# class IncrementalSampling(object):\n",
    "#     def __init__(self,prototype_nums = 10,cluster_strategy = 'kmeans'):\n",
    "#         self.incremental_data_map = {}\n",
    "#         self.data  = np.array([])\n",
    "#         self.label = np.array([])\n",
    "#         self.k_value = prototype_nums\n",
    "#         # 2024 06 07\n",
    "#         self.cluster_strategy = cluster_strategy\n",
    "#     def split_current_data_by_class(self):\n",
    "#         \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "#         # 获取唯一的类别标签\n",
    "#         current_unique_labels = np.unique(self.label)\n",
    "#         # 按类别分割当前轮获得的数据\n",
    "#         data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "#         for current_data_label in current_unique_labels:\n",
    "#             indices = np.where(self.label == current_data_label)[0]\n",
    "#             data_by_class[current_data_label] = self.data[indices]\n",
    "#         return data_by_class, current_unique_labels\n",
    "#     def data_combined(self):\n",
    "#         \"\"\"把当前轮的数据，和类map中的数据进行合并\"\"\"\n",
    "#         current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "#         # 考虑到有新类出现的情况\n",
    "#         # 如果current_unique_labels有新label,直接扩充incremental_data_map\n",
    "#         # 如果label 是incremental_data_map中已经有的，扩张incremental_data_map对应label中data的长度\n",
    "#         for new_data_label in current_unique_labels:\n",
    "#             if new_data_label in self.incremental_data_map:\n",
    "#                 # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "#                 self.incremental_data_map[new_data_label] = np.concatenate(\n",
    "#                     (self.incremental_data_map[new_data_label], current_data_map_by_class[new_data_label])\n",
    "#                 )\n",
    "#             else:\n",
    "#                 # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "#                 self.incremental_data_map[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "#     def cluster_data(self,data, num_clusters):\n",
    "#         # ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "#         # kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "#         cluster_centers = np.ndarray([])\n",
    "#         if self.cluster_strategy == 'kmeans':\n",
    "#             kmeans = KMeans(n_clusters=num_clusters)\n",
    "#             kmeans.fit(data)\n",
    "#             cluster_centers = kmeans.cluster_centers_\n",
    "#         elif self.cluster_strategy == 'spectral':\n",
    "#             spectral = SpectralClustering(n_clusters=num_clusters, random_state=0, affinity='nearest_neighbors').fit(data)\n",
    "#             labels_spectral = spectral.labels_\n",
    "#             cluster_centers = np.array([data[labels_spectral == i].mean(axis=0) for i in range(num_clusters)])\n",
    "#         elif self.cluster_strategy.lower() == 'hdbscan':\n",
    "#             min_cluster_size = 2\n",
    "#             hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(data)\n",
    "#             labels_hdbscan = hdb.labels_\n",
    "#             cluster_centers = np.array([data[labels_hdbscan == i].mean(axis=0) for i in range(len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0))])\n",
    "#         elif self.cluster_strategy.lower() == 'kmeans++' or self.cluster_strategy.lower() == 'kmeansplusplus' :\n",
    "#             kmeans_plus_plus = KMeans(n_clusters=num_clusters, init='k-means++', random_state=0).fit(data)\n",
    "#             cluster_centers = kmeans_plus_plus.cluster_centers_\n",
    "#         elif self.cluster_strategy.lower() =='kmedoids':\n",
    "#             kmedoids = KMedoids(n_clusters=num_clusters, random_state=0).fit(data)\n",
    "#             cluster_centers = kmedoids.cluster_centers_\n",
    "#         elif self.cluster_strategy.upper() =='OPTICS':\n",
    "#             min_samples = 2\n",
    "#             optics = OPTICS(min_samples=min_samples).fit(data)\n",
    "#             labels_optics = optics.labels_\n",
    "#             cluster_centers = np.array([data[labels_optics == i].mean(axis=0) for i in range(len(set(labels_optics)) - (1 if -1 in labels_optics else 0))])\n",
    "#         elif self.cluster_strategy.lower() =='meanshift':\n",
    "#             bandwidth = 0.1\n",
    "#             mean_shift = MeanShift(bandwidth=bandwidth).fit(data)\n",
    "#             cluster_centers = mean_shift.cluster_centers_\n",
    "#         elif self.cluster_strategy.lower() =='gmm':\n",
    "#             gmm = GaussianMixture(n_components=num_clusters, random_state=0).fit(data)\n",
    "#             cluster_centers = gmm.means_  # 高斯混合模型的质心是每个成分的均值\n",
    "#         return  cluster_centers\n",
    "#     def reCludter(self):\n",
    "#         # incremental_data_map的大小进行压缩self.reCluster_instance_nums\n",
    "#         new_cluster = {}\n",
    "#         for label, data in self.incremental_data_map.items():\n",
    "#             if len(data) <= self.k_value:\n",
    "#                 # 全部保存\n",
    "#                 new_cluster[label] = data\n",
    "#             else:\n",
    "#                 sampled_nums = self.k_value\n",
    "#                 clusters = self.cluster_data(data = data, num_clusters=sampled_nums)\n",
    "#                 new_cluster[label] = clusters\n",
    "#         self.incremental_data_map = new_cluster\n",
    "#     def compute_sampling_nums(self,combinde_map):\n",
    "#         # initialize\n",
    "#         min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "#         max_length = 0  # 初始化最大长度为0\n",
    "#         for label, data in combinde_map.items():\n",
    "#             data_length = len(data)\n",
    "#             if data_length < min_length:\n",
    "#                 min_length = data_length\n",
    "#             if data_length > max_length:\n",
    "#                 max_length = data_length\n",
    "#         return min_length, max_length\n",
    "#     def data_sampling(self,sampling_nums):\n",
    "#         resampling_data = []\n",
    "#         resampling_label = []\n",
    "#         for label, data in self.incremental_data_map.items():\n",
    "#             if len(data) > sampling_nums:\n",
    "#                 # 那就是下采样了,不放回采样\n",
    "#                 sampled_indices = random.sample(range(len(data)), sampling_nums)\n",
    "#                 sampled_data = [data[i] for i in sampled_indices]\n",
    "#                 resampling_data.extend(sampled_data)\n",
    "#                 resampling_label.extend([label] * sampling_nums)\n",
    "#             elif len(data) == sampling_nums:\n",
    "#                 # 直接复制\n",
    "#                 resampling_data.extend(data)\n",
    "#                 resampling_label.extend([label] * sampling_nums)\n",
    "#             else:\n",
    "#                 # 上采样,保存原样本\n",
    "#                 resampling_data.extend(data)\n",
    "#                 # 随机有放回的找差额部分\n",
    "#                 sampled_data = random.choices(data, k=(sampling_nums-len(data)))\n",
    "#                 resampling_data.extend(sampled_data)\n",
    "#                 resampling_label.extend([label] * sampling_nums)\n",
    "#         # 洗牌\n",
    "#         combined_data = list(zip(resampling_data, resampling_label))\n",
    "#         random.shuffle(combined_data)\n",
    "#         resampling_data, resampling_label = zip(*combined_data)\n",
    "#         return resampling_data,resampling_label\n",
    "#     def fit(self,incremental_prototype_map,data,label,sampling_strategy = \"OverSampling\"):\n",
    "#         self.incremental_data_map = incremental_prototype_map\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "#         self.data_combined()\n",
    "#         resampling_data = []\n",
    "#         resampling_label = []\n",
    "#         min_length, max_length =self.compute_sampling_nums(self.incremental_data_map)\n",
    "#         if sampling_strategy.lower() == \"oversampling\":\n",
    "#             resampling_data,resampling_label = self.data_sampling(max_length)\n",
    "#         elif sampling_strategy.lower() == \"downsampling\":\n",
    "#             resampling_data,resampling_label = self.data_sampling(min_length)\n",
    "#         else:\n",
    "#             print(\"No sampling measures have been taken\")\n",
    "#         self.reCludter()\n",
    "#         return resampling_data,resampling_label,self.incremental_data_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# 上诉代码没有共享模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.linalg import norm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, MeanShift ,SpectralClustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hdbscan\n",
    "import warnings\n",
    "# 定义MLP模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class IncrementalSampling(object):\n",
    "    def __init__(self,prototype_nums = 30,cluster_strategy = 'kmeans'):\n",
    "        self.incremental_data_map = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.k_value = prototype_nums\n",
    "        # 2024 06 07\n",
    "        self.cluster_strategy = cluster_strategy\n",
    "    # 把新获得的数据按不同类别划分\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    # 将划分好的数据按不同类别和不同类别的原型数据进行拼接\n",
    "    def data_combined(self):\n",
    "        \"\"\"把当前轮的数据，和类map中的数据进行合并\"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        # 考虑到有新类出现的情况\n",
    "        # 如果current_unique_labels有新label,直接扩充incremental_data_map\n",
    "        # 如果label 是incremental_data_map中已经有的，扩张incremental_data_map对应label中data的长度\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_data_map:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_data_map[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_data_map[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_data_map[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "    def cluster_data(self,data, num_clusters):\n",
    "        # ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "        # kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "        cluster_centers = np.ndarray([])\n",
    "        if self.cluster_strategy == 'kmeans':\n",
    "            with warnings.catch_warnings():\n",
    "            # KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "                kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "                kmeans.fit(data)\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "        elif self.cluster_strategy == 'spectral':\n",
    "            with warnings.catch_warnings():\n",
    "                # 报错，小类样本太少，无法聚类\n",
    "                print('cluster : spectral',{num_clusters})\n",
    "                spectral = SpectralClustering(n_clusters=num_clusters, random_state=0, affinity='nearest_neighbors').fit(data)\n",
    "                labels_spectral = spectral.labels_\n",
    "                cluster_centers = np.array([data[labels_spectral == i].mean(axis=0) for i in range(num_clusters)])\n",
    "                print(len(labels_spectral))\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.lower() == 'hdbscan':\n",
    "            with warnings.catch_warnings():\n",
    "                min_cluster_size = 2\n",
    "                hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(data)\n",
    "                labels_hdbscan = hdb.labels_\n",
    "                cluster_centers = np.array([data[labels_hdbscan == i].mean(axis=0) for i in range(len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0))])\n",
    "        elif self.cluster_strategy.lower() == 'kmeans++' or self.cluster_strategy.lower() == 'kmeansplusplus' :\n",
    "            with warnings.catch_warnings():\n",
    "                kmeans_plus_plus = KMeans(n_clusters=num_clusters, init='k-means++', random_state=0,n_init='auto').fit(data)\n",
    "                cluster_centers = kmeans_plus_plus.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='kmedoids':\n",
    "            with warnings.catch_warnings():\n",
    "                # 有几个空簇\n",
    "                print('cluster : spectral',{num_clusters})\n",
    "                kmedoids = KMedoids(n_clusters=num_clusters-10, random_state=0).fit(data)\n",
    "                labelxx = kmedoids.labels_\n",
    "                cluster_centers = kmedoids.cluster_centers_\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.upper() =='OPTICS':\n",
    "            with warnings.catch_warnings():\n",
    "                min_samples = 2\n",
    "                optics = OPTICS(min_samples=min_samples).fit(data)\n",
    "                labels_optics = optics.labels_\n",
    "                cluster_centers = np.array([data[labels_optics == i].mean(axis=0) for i in range(len(set(labels_optics)) - (1 if -1 in labels_optics else 0))])\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.lower() =='meanshift':\n",
    "            with warnings.catch_warnings():\n",
    "                bandwidth = 0.1\n",
    "                mean_shift = MeanShift(bandwidth=bandwidth).fit(data)\n",
    "                cluster_centers = mean_shift.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='gmm':\n",
    "            with warnings.catch_warnings():\n",
    "                gmm = GaussianMixture(n_components=num_clusters, random_state=0).fit(data)\n",
    "                cluster_centers = gmm.means_  # 高斯混合模型的质心是每个成分的均值\n",
    "        return  cluster_centers\n",
    "    def reCludter(self):\n",
    "        # incremental_data_map的大小进行压缩self.reCluster_instance_nums\n",
    "        new_cluster = {}\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) <= self.k_value:\n",
    "                # 全部保存\n",
    "                new_cluster[label] = data\n",
    "            else:\n",
    "                sampled_nums = self.k_value\n",
    "                clusters = self.cluster_data(data = data, num_clusters=sampled_nums)\n",
    "                new_cluster[label] = clusters\n",
    "        self.incremental_data_map = new_cluster\n",
    "    def compute_sampling_nums(self,combinde_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combinde_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def data_sampling(self,sampling_nums):\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) > sampling_nums:\n",
    "                # 那就是下采样了,不放回采样\n",
    "                sampled_indices = random.sample(range(len(data)), sampling_nums)\n",
    "                sampled_data = [data[i] for i in sampled_indices]\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            elif len(data) == sampling_nums:\n",
    "                # 直接复制\n",
    "                resampling_data.extend(data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            else:\n",
    "                # 上采样,保存原样本\n",
    "                resampling_data.extend(data)\n",
    "                # 随机有放回的找差额部分\n",
    "                sampled_data = random.choices(data, k=(sampling_nums-len(data)))\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,incremental_prototype_map,data,label,sampling_strategy = \"OverSampling\"):\n",
    "        self.incremental_data_map = incremental_prototype_map\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_data_map)\n",
    "        if sampling_strategy.lower() == \"oversampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"downsampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(min_length)\n",
    "        else:\n",
    "            print(\"No sampling measures have been taken\")\n",
    "        self.reCludter()\n",
    "        return resampling_data,resampling_label,self.incremental_data_map\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "\n",
    "class Triplets(object):\n",
    "    def __init__(self, n_neighbors=5, random=True, len_lim=True, **kwargs):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.random = random\n",
    "        self.len_lim = len_lim\n",
    "\n",
    "    def fit_resample(self, x, y):\n",
    "        strategy = self._sample_strategy(y)\n",
    "        self.n_neighbors = max(self.n_neighbors, self.counts.max() // self.counts.min())\n",
    "\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        # 这里的代码平衡状态会报错\n",
    "        for c, size in enumerate(strategy):\n",
    "            if size == 0: continue\n",
    "            weight = self._weights(x, y, c)\n",
    "            gen_x_c, gen_y_c = self._sample_one(x, y, c, size, weight)\n",
    "            gen_x += gen_x_c\n",
    "            gen_y += gen_y_c\n",
    "\n",
    "        # 为了这个方法在平衡状态下不报错，我们特地在这里加了这段代码\n",
    "        # To prevent errors in this method when in a balanced state, we intentionally added this code block\n",
    "        if len(gen_x)==0:\n",
    "            return x,y\n",
    "        gen_x = np.vstack(gen_x)\n",
    "        gen_y = np.array(gen_y)\n",
    "        return np.concatenate((x, gen_x), axis=0), np.concatenate((y, gen_y), axis=0)\n",
    "\n",
    "    def _sample_strategy(self, y):\n",
    "        _, self.counts = np.unique(y, return_counts=True)\n",
    "        return max(self.counts) - self.counts\n",
    "\n",
    "    def _weights(self, x, y, c):\n",
    "        return np.ones(self.counts[c])\n",
    "\n",
    "    def _sample_one(self, x, y, c, size, weight):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        # get the indices of minority and majority instances\n",
    "        min_idxs = np.where(y == c)[0]\n",
    "        maj_idxs = np.where(y != c)[0]\n",
    "\n",
    "        # find nearest majority neighbors for each minority instance\n",
    "        nbrs = NearestNeighbors(n_neighbors=self.n_neighbors).fit(x[maj_idxs])\n",
    "        _, indices = nbrs.kneighbors(x[min_idxs])\n",
    "\n",
    "        # generate synthetic data\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = x[min_idxs[j]]\n",
    "            tp2 = x[maj_idxs[indices[j][:5]]].mean(axis=0)\n",
    "            # tp3_ord = np.random.randint(1, self.n_neighbors)\n",
    "            tp3_ord = np.random.randint(self.n_neighbors)\n",
    "            tp3 = x[maj_idxs[indices[j][tp3_ord]]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(c)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            if self.len_lim: offset = offset * min(1, norm(tp1 - tp2) / norm(offset))\n",
    "            coef = np.random.rand() if self.random is True else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(c)\n",
    "        return gen_x, gen_y\n",
    "\n",
    "def train_model_FedAvg_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "def train_model_FedProx_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    mu = 0.1\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "    # not use deepcopy ,because model as parameter transport in this ,update model also update model\n",
    "    # current_local_model = cmodel\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # FedProx\n",
    "        prox_term = 0.0\n",
    "        for p_i, param in enumerate(model.parameters()):\n",
    "                prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "        loss += prox_term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #\n",
    "    #         # fedprox\n",
    "    #         prox_term = 0.0\n",
    "    #         for p_i, param in enumerate(model.parameters()):\n",
    "    #             prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "    #         loss += prox_term\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Total Loss: {total_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "def train_model_FedNova_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau +=len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         # model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         tau +=1\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    return losses, coeff, norm_grad,len(X_train_tensor)\n",
    "# 定义模型参数共享函数\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    # return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "\n",
    "# # 定义模型参数聚合函数\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def save_loss(loss_list,client_id,round_id,save_loss_path):\n",
    "    if not os.path.exists(save_loss_path):\n",
    "        os.makedirs(save_loss_path)\n",
    "    # 构建文件路径\n",
    "    file_path = os.path.join(save_loss_path, f\"client_{client_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 CSV 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 将损失值添加到 DataFrame 中\n",
    "    column_name = f'round_{round_id}'\n",
    "    df[column_name] = loss_list\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def save_model(global_model,round_id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'round_{round_id}_gm.pt')\n",
    "    torch.save(global_model,model_path)\n",
    "\n",
    "def save_metrics(title, rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE,KMeansSMOTE,SVMSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids,RandomUnderSampler,NearMiss,TomekLinks,EditedNearestNeighbours,RepeatedEditedNearestNeighbours,AllKNN,CondensedNearestNeighbour,OneSidedSelection,NeighbourhoodCleaningRule,InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "# SMOTE,ROS,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE1,BorderlineSMOTE2,KMeansSMOTE,SVMSMOTE\n",
    "# ClusterCentroids,RUS,NearMiss1,NearMiss2,NearMiss2,TomekLinks,ENN,RENN,AllKNN,CNN,OSS,NC,IHT\n",
    "# SMOTEENN,SMOTETomek\n",
    "def data_sampling(raw_X,raw_y,sampling_strategy):\n",
    "    if sampling_strategy.upper() == 'NO' :\n",
    "        return raw_X,raw_y\n",
    "    # overSampling\n",
    "    elif sampling_strategy.upper() == 'SMOTE': # overSampling\n",
    "        \"\"\"\n",
    "            1.对于样本x ,按照欧氏距离找到离其距离最近的K个近邻样本\n",
    "            2.确定采样比例，然后从K个近邻中选择x_n\n",
    "            3.公式 x_new = x + rand(0,1)*(x_n-x)\n",
    "        \"\"\"\n",
    "        smote = SMOTE( random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(raw_X,raw_y)\n",
    "        return X_resampled, y_resampled\n",
    "    elif sampling_strategy == \"RandomOverSampler\" or sampling_strategy.upper()=='ROS': # overSampling\n",
    "        ros = RandomOverSampler(random_state=1)\n",
    "        ros_data,ros_label = ros.fit_resample(raw_X,raw_y)\n",
    "        return ros_data,ros_label\n",
    "    elif sampling_strategy.upper() == 'SMOTENC':    # overSampling\n",
    "        smotenc = SMOTENC(random_state=1,categorical_features=[0])\n",
    "        smotenc_data,smotenc_label = smotenc.fit_resample(raw_X,raw_y)\n",
    "        return smotenc_data,smotenc_label\n",
    "    elif sampling_strategy.upper() == 'SMOTEN': # overSampling\n",
    "        smoten = SMOTEN(random_state=1)\n",
    "        smoten_data,smoten_label = smoten.fit_resample(raw_X,raw_y)\n",
    "        return smoten_data,smoten_label\n",
    "    elif sampling_strategy.upper() =='ADASYN':\n",
    "        adasyn = ADASYN(random_state=1)\n",
    "        adasyn_data,adasyn_label = adasyn.fit_resample(raw_X,raw_y)\n",
    "        return adasyn_data,adasyn_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE1' or sampling_strategy.upper()=='BSMOTE1':\n",
    "        bsmote1 = BorderlineSMOTE(kind='borderline-1',random_state=1)\n",
    "        bsmote1_data,bsmote1_label = bsmote1.fit_resample(raw_X,raw_y)\n",
    "        return bsmote1_data,bsmote1_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE2'or sampling_strategy.upper()=='BSMOTE2':\n",
    "        bsmote2 = BorderlineSMOTE(kind='borderline-2',random_state=1)\n",
    "        bsmote2_data,bsmote2_label = bsmote2.fit_resample(raw_X,raw_y)\n",
    "        return bsmote2_data,bsmote2_label\n",
    "    elif sampling_strategy == 'KMeansSMOTE' or sampling_strategy.upper() == 'KSMOTE':\n",
    "        kmeanssmote = KMeansSMOTE(random_state=1)\n",
    "        kmeanssmote_data,kmeanssmote_label = kmeanssmote.fit_resample(raw_X,raw_y)\n",
    "        return kmeanssmote_data,kmeanssmote_label\n",
    "    elif sampling_strategy == 'SVMSMOTE':\n",
    "        svmsmote = SVMSMOTE(random_state=1)\n",
    "        svmsmote_data,svmsmote_label = svmsmote.fit_resample(raw_X,raw_y)\n",
    "        return svmsmote_data,svmsmote_label\n",
    "    # downSampling\n",
    "    elif sampling_strategy == 'ClusterCentroids': # down-sampling,generate\n",
    "        clustercentroids = ClusterCentroids(random_state=1)\n",
    "        clustercentroids_data,clustercentroids_label = clustercentroids.fit_resample(raw_X,raw_y)\n",
    "        return clustercentroids_data,clustercentroids_label\n",
    "    elif sampling_strategy=='RandomUnderSampler' or sampling_strategy.upper()=='RUS':\n",
    "        rus = RandomUnderSampler(random_state=1)\n",
    "        rus_data,rus_label = rus.fit_resample(raw_X,raw_y)\n",
    "        return rus_data,rus_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS1':\n",
    "        # 在k个少数类别样本中，选择出与他们-平均距离最近的多数类样本-进行保存\n",
    "        nearmiss1 = NearMiss(version=1)\n",
    "        nearmiss1_data,nearmiss1_label = nearmiss1.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss1_data,nearmiss1_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS2':\n",
    "        # 选择K个距离最远的少数类别样本，然后根据这些样本选出的\"平均距离最近\"的样本进行保存\n",
    "        nearmiss2 = NearMiss(version=2)\n",
    "        nearmiss2_data,nearmiss2_label = nearmiss2.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss2_data,nearmiss2_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS3':\n",
    "        # 1、对于每一个少数类别样本，保留其K个最近邻多数类样本；2、把到K个少数样本平均距离最大的多数类样本保存下来。\n",
    "        nearmiss3 = NearMiss(version=3)\n",
    "        nearmiss3_data,nearmiss3_label = nearmiss3.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss3_data,nearmiss3_label\n",
    "    elif sampling_strategy == 'TomekLinks' or sampling_strategy.upper()=='TOMEK':\n",
    "        # 它需要计算每个样本之间的距离，然后把互为最近邻且类别不同的一对样本拿出来，根据需求的选择将这一对数据进行剔除 or 把多数类样本剔除\n",
    "        tomelink = TomekLinks(sampling_strategy='all')#sampling_strategy='all'表示全部删除，'auto'表示只删除多数类\n",
    "        tomelink_data,tomelink_label = tomelink.fit_resample(raw_X,raw_y)\n",
    "        return tomelink_data,tomelink_label\n",
    "    elif sampling_strategy == 'EditedNearestNeighbours' or sampling_strategy.upper() == 'ENN':\n",
    "        ENN = EditedNearestNeighbours()\n",
    "        ENN_data,ENN_label = ENN.fit_resample(raw_X,raw_y)\n",
    "        return ENN_data,ENN_label\n",
    "    elif sampling_strategy == 'RepeatedEditedNearestNeighbours' or sampling_strategy.upper() == 'RENN':\n",
    "        RENN = RepeatedEditedNearestNeighbours()\n",
    "        RENN_data,RENN_label = RENN.fit_resample(raw_X,raw_y)\n",
    "        return RENN_data,RENN_label\n",
    "    elif sampling_strategy =='AllKNN':\n",
    "        ## ENN的改进版本，和RepeatedEditedNearestNeighbours一样，会多次迭代ENN 算法，不同之处在于，他会每次增加KNN的K值\n",
    "        allknn = AllKNN()\n",
    "        allknn_data,allknn_label = allknn.fit_resample(raw_X,raw_y)\n",
    "        return allknn_data,allknn_label\n",
    "    elif sampling_strategy == 'CondensedNearestNeighbour'or sampling_strategy.upper() == 'CNN':\n",
    "        ## 如果有样本无法和其他多数类样本聚类到一起，那么说明它极有可能是边界的样本，所以将这些样本加入到集合中\n",
    "        CNN = CondensedNearestNeighbour(random_state=1)\n",
    "        CNN_data,CNN_label = CNN.fit_resample(raw_X,raw_y)\n",
    "        return CNN_data,CNN_label\n",
    "    elif sampling_strategy == 'OneSidedSelection' or sampling_strategy.upper() == 'OSS':\n",
    "        # OneSidedSelection = tomekLinks + CondensedNearestNeighbour,先使用自杀式的方式把大类数据中的其他值剔除，然后再使用CondensedNearestNeighbour的下采样\n",
    "        OSS = OneSidedSelection(random_state=1)\n",
    "        OSS_data,OSS_label = OSS.fit_resample(raw_X,raw_y)\n",
    "        return OSS_data,OSS_label\n",
    "    elif sampling_strategy == 'NeighbourhoodCleaningRule'or sampling_strategy.upper() == 'NC':\n",
    "        # 若在大类的K-近邻中，少数类占多数，那就剔除这个多数类别的样本\n",
    "        NC = NeighbourhoodCleaningRule()\n",
    "        NC_data,NC_label = NC.fit_resample(raw_X,raw_y)\n",
    "        return NC_data,NC_label\n",
    "    elif sampling_strategy == 'InstanceHardnessThreshold' or sampling_strategy.upper() == 'IHT':\n",
    "        # 默认算法是随机森林，通过分类算法给出样本阈值来剔除部分样本，（阈值较低的可以剔除）,慢\n",
    "        IHT = InstanceHardnessThreshold(random_state=1)\n",
    "        IHT_data,IHT_label = IHT.fit_resample(raw_X,raw_y)\n",
    "        return IHT_data,IHT_label\n",
    "    # hibird\n",
    "    elif sampling_strategy.upper() =='SMOTEENN':\n",
    "        se = SMOTEENN(random_state=1)\n",
    "        se_data,se_label = se.fit_resample(raw_X,raw_y)\n",
    "        return se_data,se_label\n",
    "    elif sampling_strategy.upper() =='SMOTETOMEK':\n",
    "        st = SMOTETomek(random_state=1)\n",
    "        st_data,st_label = st.fit_resample(raw_X,raw_y)\n",
    "        return st_data,st_label\n",
    "    elif sampling_strategy == 'Triplets':\n",
    "        print(\" Triplets sampling\")\n",
    "        tpl = Triplets()\n",
    "        tpl_data,tpl_label = tpl.fit_resample(raw_X,raw_y)\n",
    "        return tpl_data,tpl_label\n",
    "    else :\n",
    "        print(\"skipped all the sampling strategy,but return the raw data and label\")\n",
    "        return raw_X,raw_y\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "def inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    isap = IncrementalSampling(cluster_strategy=cluster_strategy)\n",
    "    resampling_data,resampling_label,prototype_map = isap.fit(incremental_prototype_map=prototype_map,data=raw_X,label=raw_y)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "def read_data_return_tensor(dataset_path, round_id, client_id, sampling_strategy='no',prototype_map = {},cluster_strategy='kmeans'):\n",
    "    folder_path = os.path.join(dataset_path, f'client_{client_id}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    open_file_path = os.path.join(folder_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :-1].values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    raw_y = raw_y.astype(float)\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    if sampling_strategy == 'IncrementalSampling':\n",
    "        print(\"using IncrementalSampling\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy=cluster_strategy)\n",
    "    else:\n",
    "        resampling_X,resampling_y = data_sampling(raw_X,raw_y,sampling_strategy)\n",
    "        resampling_y.astype(float)\n",
    "    resampling_X = np.array(resampling_X)  # 将列表转换为单个NumPy数组\n",
    "    resampling_y = np.array(resampling_y)  # 将列表转换为单个NumPy数组\n",
    "\n",
    "    X_train_tensor = torch.tensor(resampling_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(resampling_y, dtype=torch.long)   # 标签张量\n",
    "    return X_train_tensor,y_train_tensor,prototype_map\n",
    "def read_test_data(test_data_path):\n",
    "    data = pd.read_csv(test_data_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    X = data_shuffled.iloc[:, :-1].values.astype(float)  # 特征\n",
    "    # print(X)\n",
    "    y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    y = y.astype(float)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X, dtype=torch.float32)  # 特征张量\n",
    "    y_test_tensor = torch.tensor(y, dtype=torch.long)   # 标签张量\n",
    "    return X_test_tensor,y_test_tensor\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T08:18:27.456713100Z",
     "start_time": "2024-06-08T08:18:27.346413200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round0\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.9482\n",
      "Epoch [11/200], Loss: 1.6062\n",
      "Epoch [21/200], Loss: 1.5344\n",
      "Epoch [31/200], Loss: 1.4803\n",
      "Epoch [41/200], Loss: 1.4370\n",
      "Epoch [51/200], Loss: 1.4015\n",
      "Epoch [61/200], Loss: 1.3718\n",
      "Epoch [71/200], Loss: 1.3463\n",
      "Epoch [81/200], Loss: 1.3236\n",
      "Epoch [91/200], Loss: 1.3027\n",
      "Epoch [101/200], Loss: 1.2839\n",
      "Epoch [111/200], Loss: 1.2666\n",
      "Epoch [121/200], Loss: 1.2505\n",
      "Epoch [131/200], Loss: 1.2352\n",
      "Epoch [141/200], Loss: 1.2207\n",
      "Epoch [151/200], Loss: 1.2067\n",
      "Epoch [161/200], Loss: 1.1934\n",
      "Epoch [171/200], Loss: 1.1817\n",
      "Epoch [181/200], Loss: 1.1706\n",
      "Epoch [191/200], Loss: 1.1600\n",
      "Test Accuracy: 27.35%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.0749\n",
      "Epoch [11/200], Loss: 1.5651\n",
      "Epoch [21/200], Loss: 1.4897\n",
      "Epoch [31/200], Loss: 1.4314\n",
      "Epoch [41/200], Loss: 1.3837\n",
      "Epoch [51/200], Loss: 1.3434\n",
      "Epoch [61/200], Loss: 1.3079\n",
      "Epoch [71/200], Loss: 1.2762\n",
      "Epoch [81/200], Loss: 1.2478\n",
      "Epoch [91/200], Loss: 1.2220\n",
      "Epoch [101/200], Loss: 1.1985\n",
      "Epoch [111/200], Loss: 1.1763\n",
      "Epoch [121/200], Loss: 1.1553\n",
      "Epoch [131/200], Loss: 1.1358\n",
      "Epoch [141/200], Loss: 1.1173\n",
      "Epoch [151/200], Loss: 1.0996\n",
      "Epoch [161/200], Loss: 1.0835\n",
      "Epoch [171/200], Loss: 1.0684\n",
      "Epoch [181/200], Loss: 1.0537\n",
      "Epoch [191/200], Loss: 1.0398\n",
      "Test Accuracy: 16.85%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.3087\n",
      "Epoch [11/200], Loss: 1.5559\n",
      "Epoch [21/200], Loss: 1.4645\n",
      "Epoch [31/200], Loss: 1.3938\n",
      "Epoch [41/200], Loss: 1.3377\n",
      "Epoch [51/200], Loss: 1.2897\n",
      "Epoch [61/200], Loss: 1.2468\n",
      "Epoch [71/200], Loss: 1.2103\n",
      "Epoch [81/200], Loss: 1.1782\n",
      "Epoch [91/200], Loss: 1.1494\n",
      "Epoch [101/200], Loss: 1.1232\n",
      "Epoch [111/200], Loss: 1.0995\n",
      "Epoch [121/200], Loss: 1.0780\n",
      "Epoch [131/200], Loss: 1.0582\n",
      "Epoch [141/200], Loss: 1.0398\n",
      "Epoch [151/200], Loss: 1.0230\n",
      "Epoch [161/200], Loss: 1.0073\n",
      "Epoch [171/200], Loss: 0.9923\n",
      "Epoch [181/200], Loss: 0.9781\n",
      "Epoch [191/200], Loss: 0.9645\n",
      "Test Accuracy: 13.38%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.3720\n",
      "Epoch [11/200], Loss: 1.5258\n",
      "Epoch [21/200], Loss: 1.4774\n",
      "Epoch [31/200], Loss: 1.4349\n",
      "Epoch [41/200], Loss: 1.3975\n",
      "Epoch [51/200], Loss: 1.3654\n",
      "Epoch [61/200], Loss: 1.3365\n",
      "Epoch [71/200], Loss: 1.3096\n",
      "Epoch [81/200], Loss: 1.2860\n",
      "Epoch [91/200], Loss: 1.2642\n",
      "Epoch [101/200], Loss: 1.2450\n",
      "Epoch [111/200], Loss: 1.2273\n",
      "Epoch [121/200], Loss: 1.2110\n",
      "Epoch [131/200], Loss: 1.1955\n",
      "Epoch [141/200], Loss: 1.1809\n",
      "Epoch [151/200], Loss: 1.1674\n",
      "Epoch [161/200], Loss: 1.1547\n",
      "Epoch [171/200], Loss: 1.1427\n",
      "Epoch [181/200], Loss: 1.1322\n",
      "Epoch [191/200], Loss: 1.1224\n",
      "Test Accuracy: 30.97%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.1404\n",
      "Epoch [11/200], Loss: 1.6319\n",
      "Epoch [21/200], Loss: 1.5595\n",
      "Epoch [31/200], Loss: 1.5079\n",
      "Epoch [41/200], Loss: 1.4680\n",
      "Epoch [51/200], Loss: 1.4353\n",
      "Epoch [61/200], Loss: 1.4071\n",
      "Epoch [71/200], Loss: 1.3822\n",
      "Epoch [81/200], Loss: 1.3595\n",
      "Epoch [91/200], Loss: 1.3386\n",
      "Epoch [101/200], Loss: 1.3192\n",
      "Epoch [111/200], Loss: 1.3008\n",
      "Epoch [121/200], Loss: 1.2836\n",
      "Epoch [131/200], Loss: 1.2673\n",
      "Epoch [141/200], Loss: 1.2517\n",
      "Epoch [151/200], Loss: 1.2368\n",
      "Epoch [161/200], Loss: 1.2227\n",
      "Epoch [171/200], Loss: 1.2094\n",
      "Epoch [181/200], Loss: 1.1964\n",
      "Epoch [191/200], Loss: 1.1844\n",
      "Test Accuracy: 37.39%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.3039\n",
      "Epoch [11/200], Loss: 1.5703\n",
      "Epoch [21/200], Loss: 1.4725\n",
      "Epoch [31/200], Loss: 1.3994\n",
      "Epoch [41/200], Loss: 1.3408\n",
      "Epoch [51/200], Loss: 1.2916\n",
      "Epoch [61/200], Loss: 1.2491\n",
      "Epoch [71/200], Loss: 1.2121\n",
      "Epoch [81/200], Loss: 1.1798\n",
      "Epoch [91/200], Loss: 1.1505\n",
      "Epoch [101/200], Loss: 1.1240\n",
      "Epoch [111/200], Loss: 1.1001\n",
      "Epoch [121/200], Loss: 1.0783\n",
      "Epoch [131/200], Loss: 1.0583\n",
      "Epoch [141/200], Loss: 1.0398\n",
      "Epoch [151/200], Loss: 1.0227\n",
      "Epoch [161/200], Loss: 1.0069\n",
      "Epoch [171/200], Loss: 0.9921\n",
      "Epoch [181/200], Loss: 0.9786\n",
      "Epoch [191/200], Loss: 0.9660\n",
      "Test Accuracy: 23.30%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.5746\n",
      "Epoch [11/200], Loss: 1.4951\n",
      "Epoch [21/200], Loss: 1.4514\n",
      "Epoch [31/200], Loss: 1.4150\n",
      "Epoch [41/200], Loss: 1.3835\n",
      "Epoch [51/200], Loss: 1.3566\n",
      "Epoch [61/200], Loss: 1.3318\n",
      "Epoch [71/200], Loss: 1.3097\n",
      "Epoch [81/200], Loss: 1.2896\n",
      "Epoch [91/200], Loss: 1.2706\n",
      "Epoch [101/200], Loss: 1.2528\n",
      "Epoch [111/200], Loss: 1.2359\n",
      "Epoch [121/200], Loss: 1.2198\n",
      "Epoch [131/200], Loss: 1.2042\n",
      "Epoch [141/200], Loss: 1.1899\n",
      "Epoch [151/200], Loss: 1.1768\n",
      "Epoch [161/200], Loss: 1.1641\n",
      "Epoch [171/200], Loss: 1.1518\n",
      "Epoch [181/200], Loss: 1.1402\n",
      "Epoch [191/200], Loss: 1.1281\n",
      "Test Accuracy: 18.14%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.4346\n",
      "Epoch [11/200], Loss: 1.5722\n",
      "Epoch [21/200], Loss: 1.4557\n",
      "Epoch [31/200], Loss: 1.3685\n",
      "Epoch [41/200], Loss: 1.3000\n",
      "Epoch [51/200], Loss: 1.2442\n",
      "Epoch [61/200], Loss: 1.1973\n",
      "Epoch [71/200], Loss: 1.1570\n",
      "Epoch [81/200], Loss: 1.1221\n",
      "Epoch [91/200], Loss: 1.0915\n",
      "Epoch [101/200], Loss: 1.0644\n",
      "Epoch [111/200], Loss: 1.0401\n",
      "Epoch [121/200], Loss: 1.0183\n",
      "Epoch [131/200], Loss: 0.9986\n",
      "Epoch [141/200], Loss: 0.9806\n",
      "Epoch [151/200], Loss: 0.9641\n",
      "Epoch [161/200], Loss: 0.9492\n",
      "Epoch [171/200], Loss: 0.9355\n",
      "Epoch [181/200], Loss: 0.9227\n",
      "Epoch [191/200], Loss: 0.9108\n",
      "Test Accuracy: 24.41%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.7186\n",
      "Epoch [11/200], Loss: 1.5874\n",
      "Epoch [21/200], Loss: 1.5025\n",
      "Epoch [31/200], Loss: 1.4362\n",
      "Epoch [41/200], Loss: 1.3853\n",
      "Epoch [51/200], Loss: 1.3446\n",
      "Epoch [61/200], Loss: 1.3098\n",
      "Epoch [71/200], Loss: 1.2792\n",
      "Epoch [81/200], Loss: 1.2515\n",
      "Epoch [91/200], Loss: 1.2263\n",
      "Epoch [101/200], Loss: 1.2032\n",
      "Epoch [111/200], Loss: 1.1822\n",
      "Epoch [121/200], Loss: 1.1631\n",
      "Epoch [131/200], Loss: 1.1451\n",
      "Epoch [141/200], Loss: 1.1285\n",
      "Epoch [151/200], Loss: 1.1130\n",
      "Epoch [161/200], Loss: 1.0983\n",
      "Epoch [171/200], Loss: 1.0846\n",
      "Epoch [181/200], Loss: 1.0716\n",
      "Epoch [191/200], Loss: 1.0593\n",
      "Test Accuracy: 23.72%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.2417\n",
      "Epoch [11/200], Loss: 1.4990\n",
      "Epoch [21/200], Loss: 1.4303\n",
      "Epoch [31/200], Loss: 1.3771\n",
      "Epoch [41/200], Loss: 1.3335\n",
      "Epoch [51/200], Loss: 1.2966\n",
      "Epoch [61/200], Loss: 1.2634\n",
      "Epoch [71/200], Loss: 1.2342\n",
      "Epoch [81/200], Loss: 1.2078\n",
      "Epoch [91/200], Loss: 1.1841\n",
      "Epoch [101/200], Loss: 1.1624\n",
      "Epoch [111/200], Loss: 1.1421\n",
      "Epoch [121/200], Loss: 1.1220\n",
      "Epoch [131/200], Loss: 1.1051\n",
      "Epoch [141/200], Loss: 1.0900\n",
      "Epoch [151/200], Loss: 1.0760\n",
      "Epoch [161/200], Loss: 1.0629\n",
      "Epoch [171/200], Loss: 1.0505\n",
      "Epoch [181/200], Loss: 1.0385\n",
      "Epoch [191/200], Loss: 1.0272\n",
      "Test Accuracy: 17.07%\n",
      "Test Accuracy: 42.26%\n",
      "gme acc: {'recall': array([0.00000000e+00, 9.99914391e-01, 2.53581844e-04, 0.00000000e+00,\n",
      "       0.00000000e+00]), 'recall_micro': 0.4225803922987404, 'recall_macro': 0.20003359454694616, 'precision': array([0.       , 0.4226091, 0.125    , 0.       , 0.       ]), 'precision_micro': 0.4225803922987404, 'precision_macro': 0.10952182093386482, 'f1_score': array([0.00000000e+00, 5.94117323e-01, 5.06136910e-04, 0.00000000e+00,\n",
      "       0.00000000e+00]), 'g_mean': 0.0, 'acc': 0.4225803922987404, 'auc': 0.500007899365156, 'kappa': 2.0892826444685042e-05, 'confusion_matrix': array([[    0, 83106,     0,     0,     0],\n",
      "       [    0, 70080,     6,     0,     0],\n",
      "       [    0,  7885,     2,     0,     0],\n",
      "       [    0,  3777,     8,     0,     0],\n",
      "       [    0,   979,     0,     0,     0]], dtype=int64)}\n",
      "round1\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.4802\n",
      "Epoch [11/200], Loss: 1.5495\n",
      "Epoch [21/200], Loss: 1.5128\n",
      "Epoch [31/200], Loss: 1.4846\n",
      "Epoch [41/200], Loss: 1.4609\n",
      "Epoch [51/200], Loss: 1.4416\n",
      "Epoch [61/200], Loss: 1.4247\n",
      "Epoch [71/200], Loss: 1.4096\n",
      "Epoch [81/200], Loss: 1.3959\n",
      "Epoch [91/200], Loss: 1.3835\n",
      "Epoch [101/200], Loss: 1.3718\n",
      "Epoch [111/200], Loss: 1.3607\n",
      "Epoch [121/200], Loss: 1.3504\n",
      "Epoch [131/200], Loss: 1.3408\n",
      "Epoch [141/200], Loss: 1.3316\n",
      "Epoch [151/200], Loss: 1.3229\n",
      "Epoch [161/200], Loss: 1.3144\n",
      "Epoch [171/200], Loss: 1.3065\n",
      "Epoch [181/200], Loss: 1.2990\n",
      "Epoch [191/200], Loss: 1.2913\n",
      "Test Accuracy: 26.84%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.3970\n",
      "Epoch [11/200], Loss: 1.4518\n",
      "Epoch [21/200], Loss: 1.3983\n",
      "Epoch [31/200], Loss: 1.3561\n",
      "Epoch [41/200], Loss: 1.3218\n",
      "Epoch [51/200], Loss: 1.2917\n",
      "Epoch [61/200], Loss: 1.2658\n",
      "Epoch [71/200], Loss: 1.2423\n",
      "Epoch [81/200], Loss: 1.2207\n",
      "Epoch [91/200], Loss: 1.2007\n",
      "Epoch [101/200], Loss: 1.1821\n",
      "Epoch [111/200], Loss: 1.1650\n",
      "Epoch [121/200], Loss: 1.1487\n",
      "Epoch [131/200], Loss: 1.1338\n",
      "Epoch [141/200], Loss: 1.1200\n",
      "Epoch [151/200], Loss: 1.1073\n",
      "Epoch [161/200], Loss: 1.0955\n",
      "Epoch [171/200], Loss: 1.0843\n",
      "Epoch [181/200], Loss: 1.0738\n",
      "Epoch [191/200], Loss: 1.0638\n",
      "Test Accuracy: 18.46%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.6667\n",
      "Epoch [11/200], Loss: 1.5370\n",
      "Epoch [21/200], Loss: 1.4754\n",
      "Epoch [31/200], Loss: 1.4295\n",
      "Epoch [41/200], Loss: 1.3923\n",
      "Epoch [51/200], Loss: 1.3610\n",
      "Epoch [61/200], Loss: 1.3343\n",
      "Epoch [71/200], Loss: 1.3107\n",
      "Epoch [81/200], Loss: 1.2893\n",
      "Epoch [91/200], Loss: 1.2698\n",
      "Epoch [101/200], Loss: 1.2519\n",
      "Epoch [111/200], Loss: 1.2351\n",
      "Epoch [121/200], Loss: 1.2196\n",
      "Epoch [131/200], Loss: 1.2055\n",
      "Epoch [141/200], Loss: 1.1924\n",
      "Epoch [151/200], Loss: 1.1801\n",
      "Epoch [161/200], Loss: 1.1687\n",
      "Epoch [171/200], Loss: 1.1578\n",
      "Epoch [181/200], Loss: 1.1475\n",
      "Epoch [191/200], Loss: 1.1378\n",
      "Test Accuracy: 30.55%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.4442\n",
      "Epoch [11/200], Loss: 1.5267\n",
      "Epoch [21/200], Loss: 1.4932\n",
      "Epoch [31/200], Loss: 1.4689\n",
      "Epoch [41/200], Loss: 1.4497\n",
      "Epoch [51/200], Loss: 1.4337\n",
      "Epoch [61/200], Loss: 1.4198\n",
      "Epoch [71/200], Loss: 1.4074\n",
      "Epoch [81/200], Loss: 1.3960\n",
      "Epoch [91/200], Loss: 1.3852\n",
      "Epoch [101/200], Loss: 1.3750\n",
      "Epoch [111/200], Loss: 1.3656\n",
      "Epoch [121/200], Loss: 1.3571\n",
      "Epoch [131/200], Loss: 1.3489\n",
      "Epoch [141/200], Loss: 1.3410\n",
      "Epoch [151/200], Loss: 1.3335\n",
      "Epoch [161/200], Loss: 1.3264\n",
      "Epoch [171/200], Loss: 1.3197\n",
      "Epoch [181/200], Loss: 1.3135\n",
      "Epoch [191/200], Loss: 1.3076\n",
      "Test Accuracy: 30.72%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.5517\n",
      "Epoch [11/200], Loss: 1.5648\n",
      "Epoch [21/200], Loss: 1.5336\n",
      "Epoch [31/200], Loss: 1.5090\n",
      "Epoch [41/200], Loss: 1.4884\n",
      "Epoch [51/200], Loss: 1.4697\n",
      "Epoch [61/200], Loss: 1.4522\n",
      "Epoch [71/200], Loss: 1.4357\n",
      "Epoch [81/200], Loss: 1.4205\n",
      "Epoch [91/200], Loss: 1.4063\n",
      "Epoch [101/200], Loss: 1.3938\n",
      "Epoch [111/200], Loss: 1.3822\n",
      "Epoch [121/200], Loss: 1.3707\n",
      "Epoch [131/200], Loss: 1.3602\n",
      "Epoch [141/200], Loss: 1.3509\n",
      "Epoch [151/200], Loss: 1.3420\n",
      "Epoch [161/200], Loss: 1.3336\n",
      "Epoch [171/200], Loss: 1.3255\n",
      "Epoch [181/200], Loss: 1.3181\n",
      "Epoch [191/200], Loss: 1.3110\n",
      "Test Accuracy: 34.10%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.4750\n",
      "Epoch [11/200], Loss: 1.4450\n",
      "Epoch [21/200], Loss: 1.3741\n",
      "Epoch [31/200], Loss: 1.3178\n",
      "Epoch [41/200], Loss: 1.2715\n",
      "Epoch [51/200], Loss: 1.2319\n",
      "Epoch [61/200], Loss: 1.1982\n",
      "Epoch [71/200], Loss: 1.1688\n",
      "Epoch [81/200], Loss: 1.1429\n",
      "Epoch [91/200], Loss: 1.1197\n",
      "Epoch [101/200], Loss: 1.0992\n",
      "Epoch [111/200], Loss: 1.0807\n",
      "Epoch [121/200], Loss: 1.0640\n",
      "Epoch [131/200], Loss: 1.0490\n",
      "Epoch [141/200], Loss: 1.0354\n",
      "Epoch [151/200], Loss: 1.0230\n",
      "Epoch [161/200], Loss: 1.0117\n",
      "Epoch [171/200], Loss: 1.0012\n",
      "Epoch [181/200], Loss: 0.9915\n",
      "Epoch [191/200], Loss: 0.9824\n",
      "Test Accuracy: 26.93%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.6140\n",
      "Epoch [11/200], Loss: 1.5692\n",
      "Epoch [21/200], Loss: 1.5299\n",
      "Epoch [31/200], Loss: 1.5006\n",
      "Epoch [41/200], Loss: 1.4767\n",
      "Epoch [51/200], Loss: 1.4562\n",
      "Epoch [61/200], Loss: 1.4381\n",
      "Epoch [71/200], Loss: 1.4215\n",
      "Epoch [81/200], Loss: 1.4069\n",
      "Epoch [91/200], Loss: 1.3934\n",
      "Epoch [101/200], Loss: 1.3809\n",
      "Epoch [111/200], Loss: 1.3692\n",
      "Epoch [121/200], Loss: 1.3577\n",
      "Epoch [131/200], Loss: 1.3467\n",
      "Epoch [141/200], Loss: 1.3364\n",
      "Epoch [151/200], Loss: 1.3271\n",
      "Epoch [161/200], Loss: 1.3183\n",
      "Epoch [171/200], Loss: 1.3097\n",
      "Epoch [181/200], Loss: 1.3015\n",
      "Epoch [191/200], Loss: 1.2936\n",
      "Test Accuracy: 37.57%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.3984\n",
      "Epoch [11/200], Loss: 1.4312\n",
      "Epoch [21/200], Loss: 1.3845\n",
      "Epoch [31/200], Loss: 1.3456\n",
      "Epoch [41/200], Loss: 1.3125\n",
      "Epoch [51/200], Loss: 1.2833\n",
      "Epoch [61/200], Loss: 1.2565\n",
      "Epoch [71/200], Loss: 1.2320\n",
      "Epoch [81/200], Loss: 1.2103\n",
      "Epoch [91/200], Loss: 1.1908\n",
      "Epoch [101/200], Loss: 1.1735\n",
      "Epoch [111/200], Loss: 1.1579\n",
      "Epoch [121/200], Loss: 1.1438\n",
      "Epoch [131/200], Loss: 1.1309\n",
      "Epoch [141/200], Loss: 1.1188\n",
      "Epoch [151/200], Loss: 1.1077\n",
      "Epoch [161/200], Loss: 1.0974\n",
      "Epoch [171/200], Loss: 1.0878\n",
      "Epoch [181/200], Loss: 1.0788\n",
      "Epoch [191/200], Loss: 1.0703\n",
      "Test Accuracy: 28.05%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.4925\n",
      "Epoch [11/200], Loss: 1.5431\n",
      "Epoch [21/200], Loss: 1.5172\n",
      "Epoch [31/200], Loss: 1.4963\n",
      "Epoch [41/200], Loss: 1.4791\n",
      "Epoch [51/200], Loss: 1.4640\n",
      "Epoch [61/200], Loss: 1.4507\n",
      "Epoch [71/200], Loss: 1.4388\n",
      "Epoch [81/200], Loss: 1.4276\n",
      "Epoch [91/200], Loss: 1.4174\n",
      "Epoch [101/200], Loss: 1.4077\n",
      "Epoch [111/200], Loss: 1.3984\n",
      "Epoch [121/200], Loss: 1.3899\n",
      "Epoch [131/200], Loss: 1.3818\n",
      "Epoch [141/200], Loss: 1.3740\n",
      "Epoch [151/200], Loss: 1.3665\n",
      "Epoch [161/200], Loss: 1.3592\n",
      "Epoch [171/200], Loss: 1.3521\n",
      "Epoch [181/200], Loss: 1.3450\n",
      "Epoch [191/200], Loss: 1.3383\n",
      "Test Accuracy: 22.67%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 2.5641\n",
      "Epoch [11/200], Loss: 1.5108\n",
      "Epoch [21/200], Loss: 1.4739\n",
      "Epoch [31/200], Loss: 1.4465\n",
      "Epoch [41/200], Loss: 1.4241\n",
      "Epoch [51/200], Loss: 1.4048\n",
      "Epoch [61/200], Loss: 1.3879\n",
      "Epoch [71/200], Loss: 1.3723\n",
      "Epoch [81/200], Loss: 1.3575\n",
      "Epoch [91/200], Loss: 1.3428\n",
      "Epoch [101/200], Loss: 1.3291\n",
      "Epoch [111/200], Loss: 1.3162\n",
      "Epoch [121/200], Loss: 1.3037\n",
      "Epoch [131/200], Loss: 1.2912\n",
      "Epoch [141/200], Loss: 1.2787\n",
      "Epoch [151/200], Loss: 1.2663\n",
      "Epoch [161/200], Loss: 1.2545\n",
      "Epoch [171/200], Loss: 1.2437\n",
      "Epoch [181/200], Loss: 1.2329\n",
      "Epoch [191/200], Loss: 1.2224\n",
      "Test Accuracy: 19.54%\n",
      "Test Accuracy: 20.34%\n",
      "gme acc: {'recall': array([0.03810796, 0.3697172 , 0.25675162, 0.49775429, 0.7671093 ]), 'recall_micro': 0.20343939750245715, 'recall_macro': 0.3858880736268732, 'precision': array([0.44845653, 0.32604374, 0.23634454, 0.14591078, 0.01298701]), 'precision_micro': 0.20343939750245715, 'precision_macro': 0.23394851938832334, 'f1_score': array([0.07024665, 0.34650976, 0.2461258 , 0.22566928, 0.02554161]), 'g_mean': 0.26795024482336455, 'acc': 0.20343939750245715, 'auc': 0.4689483057353999, 'kappa': -0.034693261725601277, 'confusion_matrix': array([[ 3167, 52166,  2716,  2285, 22772],\n",
      "       [ 3594, 25912,  3631,  7944, 29005],\n",
      "       [  298,  1018,  2025,   763,  3783],\n",
      "       [    2,   287,    96,  1884,  1516],\n",
      "       [    1,    91,   100,    36,   751]], dtype=int64)}\n",
      "round2\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4164\n",
      "Epoch [11/200], Loss: 1.3818\n",
      "Epoch [21/200], Loss: 1.3616\n",
      "Epoch [31/200], Loss: 1.3448\n",
      "Epoch [41/200], Loss: 1.3320\n",
      "Epoch [51/200], Loss: 1.3204\n",
      "Epoch [61/200], Loss: 1.3102\n",
      "Epoch [71/200], Loss: 1.3009\n",
      "Epoch [81/200], Loss: 1.2923\n",
      "Epoch [91/200], Loss: 1.2842\n",
      "Epoch [101/200], Loss: 1.2769\n",
      "Epoch [111/200], Loss: 1.2703\n",
      "Epoch [121/200], Loss: 1.2639\n",
      "Epoch [131/200], Loss: 1.2576\n",
      "Epoch [141/200], Loss: 1.2513\n",
      "Epoch [151/200], Loss: 1.2452\n",
      "Epoch [161/200], Loss: 1.2392\n",
      "Epoch [171/200], Loss: 1.2330\n",
      "Epoch [181/200], Loss: 1.2278\n",
      "Epoch [191/200], Loss: 1.2229\n",
      "Test Accuracy: 27.77%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3638\n",
      "Epoch [11/200], Loss: 1.3110\n",
      "Epoch [21/200], Loss: 1.2837\n",
      "Epoch [31/200], Loss: 1.2594\n",
      "Epoch [41/200], Loss: 1.2374\n",
      "Epoch [51/200], Loss: 1.2182\n",
      "Epoch [61/200], Loss: 1.2006\n",
      "Epoch [71/200], Loss: 1.1845\n",
      "Epoch [81/200], Loss: 1.1696\n",
      "Epoch [91/200], Loss: 1.1558\n",
      "Epoch [101/200], Loss: 1.1435\n",
      "Epoch [111/200], Loss: 1.1320\n",
      "Epoch [121/200], Loss: 1.1212\n",
      "Epoch [131/200], Loss: 1.1110\n",
      "Epoch [141/200], Loss: 1.1013\n",
      "Epoch [151/200], Loss: 1.0920\n",
      "Epoch [161/200], Loss: 1.0832\n",
      "Epoch [171/200], Loss: 1.0750\n",
      "Epoch [181/200], Loss: 1.0676\n",
      "Epoch [191/200], Loss: 1.0606\n",
      "Test Accuracy: 22.12%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4575\n",
      "Epoch [11/200], Loss: 1.3932\n",
      "Epoch [21/200], Loss: 1.3633\n",
      "Epoch [31/200], Loss: 1.3384\n",
      "Epoch [41/200], Loss: 1.3173\n",
      "Epoch [51/200], Loss: 1.2985\n",
      "Epoch [61/200], Loss: 1.2814\n",
      "Epoch [71/200], Loss: 1.2656\n",
      "Epoch [81/200], Loss: 1.2511\n",
      "Epoch [91/200], Loss: 1.2374\n",
      "Epoch [101/200], Loss: 1.2250\n",
      "Epoch [111/200], Loss: 1.2132\n",
      "Epoch [121/200], Loss: 1.2020\n",
      "Epoch [131/200], Loss: 1.1916\n",
      "Epoch [141/200], Loss: 1.1822\n",
      "Epoch [151/200], Loss: 1.1732\n",
      "Epoch [161/200], Loss: 1.1648\n",
      "Epoch [171/200], Loss: 1.1568\n",
      "Epoch [181/200], Loss: 1.1492\n",
      "Epoch [191/200], Loss: 1.1419\n",
      "Test Accuracy: 28.62%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4365\n",
      "Epoch [11/200], Loss: 1.3970\n",
      "Epoch [21/200], Loss: 1.3751\n",
      "Epoch [31/200], Loss: 1.3580\n",
      "Epoch [41/200], Loss: 1.3441\n",
      "Epoch [51/200], Loss: 1.3321\n",
      "Epoch [61/200], Loss: 1.3220\n",
      "Epoch [71/200], Loss: 1.3129\n",
      "Epoch [81/200], Loss: 1.3047\n",
      "Epoch [91/200], Loss: 1.2970\n",
      "Epoch [101/200], Loss: 1.2898\n",
      "Epoch [111/200], Loss: 1.2830\n",
      "Epoch [121/200], Loss: 1.2763\n",
      "Epoch [131/200], Loss: 1.2697\n",
      "Epoch [141/200], Loss: 1.2633\n",
      "Epoch [151/200], Loss: 1.2569\n",
      "Epoch [161/200], Loss: 1.2511\n",
      "Epoch [171/200], Loss: 1.2453\n",
      "Epoch [181/200], Loss: 1.2397\n",
      "Epoch [191/200], Loss: 1.2342\n",
      "Test Accuracy: 38.22%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4819\n",
      "Epoch [11/200], Loss: 1.4339\n",
      "Epoch [21/200], Loss: 1.4166\n",
      "Epoch [31/200], Loss: 1.4030\n",
      "Epoch [41/200], Loss: 1.3914\n",
      "Epoch [51/200], Loss: 1.3816\n",
      "Epoch [61/200], Loss: 1.3727\n",
      "Epoch [71/200], Loss: 1.3645\n",
      "Epoch [81/200], Loss: 1.3570\n",
      "Epoch [91/200], Loss: 1.3499\n",
      "Epoch [101/200], Loss: 1.3432\n",
      "Epoch [111/200], Loss: 1.3369\n",
      "Epoch [121/200], Loss: 1.3309\n",
      "Epoch [131/200], Loss: 1.3252\n",
      "Epoch [141/200], Loss: 1.3198\n",
      "Epoch [151/200], Loss: 1.3145\n",
      "Epoch [161/200], Loss: 1.3093\n",
      "Epoch [171/200], Loss: 1.3041\n",
      "Epoch [181/200], Loss: 1.2992\n",
      "Epoch [191/200], Loss: 1.2945\n",
      "Test Accuracy: 41.40%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4140\n",
      "Epoch [11/200], Loss: 1.3537\n",
      "Epoch [21/200], Loss: 1.3246\n",
      "Epoch [31/200], Loss: 1.2999\n",
      "Epoch [41/200], Loss: 1.2786\n",
      "Epoch [51/200], Loss: 1.2599\n",
      "Epoch [61/200], Loss: 1.2435\n",
      "Epoch [71/200], Loss: 1.2286\n",
      "Epoch [81/200], Loss: 1.2151\n",
      "Epoch [91/200], Loss: 1.2025\n",
      "Epoch [101/200], Loss: 1.1909\n",
      "Epoch [111/200], Loss: 1.1802\n",
      "Epoch [121/200], Loss: 1.1701\n",
      "Epoch [131/200], Loss: 1.1606\n",
      "Epoch [141/200], Loss: 1.1517\n",
      "Epoch [151/200], Loss: 1.1435\n",
      "Epoch [161/200], Loss: 1.1361\n",
      "Epoch [171/200], Loss: 1.1290\n",
      "Epoch [181/200], Loss: 1.1222\n",
      "Epoch [191/200], Loss: 1.1158\n",
      "Test Accuracy: 27.17%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4444\n",
      "Epoch [11/200], Loss: 1.4002\n",
      "Epoch [21/200], Loss: 1.3787\n",
      "Epoch [31/200], Loss: 1.3603\n",
      "Epoch [41/200], Loss: 1.3441\n",
      "Epoch [51/200], Loss: 1.3300\n",
      "Epoch [61/200], Loss: 1.3178\n",
      "Epoch [71/200], Loss: 1.3067\n",
      "Epoch [81/200], Loss: 1.2967\n",
      "Epoch [91/200], Loss: 1.2874\n",
      "Epoch [101/200], Loss: 1.2787\n",
      "Epoch [111/200], Loss: 1.2705\n",
      "Epoch [121/200], Loss: 1.2627\n",
      "Epoch [131/200], Loss: 1.2553\n",
      "Epoch [141/200], Loss: 1.2483\n",
      "Epoch [151/200], Loss: 1.2418\n",
      "Epoch [161/200], Loss: 1.2358\n",
      "Epoch [171/200], Loss: 1.2302\n",
      "Epoch [181/200], Loss: 1.2248\n",
      "Epoch [191/200], Loss: 1.2196\n",
      "Test Accuracy: 31.10%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3728\n",
      "Epoch [11/200], Loss: 1.3206\n",
      "Epoch [21/200], Loss: 1.2940\n",
      "Epoch [31/200], Loss: 1.2715\n",
      "Epoch [41/200], Loss: 1.2529\n",
      "Epoch [51/200], Loss: 1.2367\n",
      "Epoch [61/200], Loss: 1.2220\n",
      "Epoch [71/200], Loss: 1.2085\n",
      "Epoch [81/200], Loss: 1.1961\n",
      "Epoch [91/200], Loss: 1.1844\n",
      "Epoch [101/200], Loss: 1.1733\n",
      "Epoch [111/200], Loss: 1.1630\n",
      "Epoch [121/200], Loss: 1.1536\n",
      "Epoch [131/200], Loss: 1.1446\n",
      "Epoch [141/200], Loss: 1.1361\n",
      "Epoch [151/200], Loss: 1.1281\n",
      "Epoch [161/200], Loss: 1.1207\n",
      "Epoch [171/200], Loss: 1.1136\n",
      "Epoch [181/200], Loss: 1.1067\n",
      "Epoch [191/200], Loss: 1.1001\n",
      "Test Accuracy: 35.30%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4355\n",
      "Epoch [11/200], Loss: 1.4004\n",
      "Epoch [21/200], Loss: 1.3852\n",
      "Epoch [31/200], Loss: 1.3724\n",
      "Epoch [41/200], Loss: 1.3603\n",
      "Epoch [51/200], Loss: 1.3484\n",
      "Epoch [61/200], Loss: 1.3389\n",
      "Epoch [71/200], Loss: 1.3301\n",
      "Epoch [81/200], Loss: 1.3219\n",
      "Epoch [91/200], Loss: 1.3140\n",
      "Epoch [101/200], Loss: 1.3064\n",
      "Epoch [111/200], Loss: 1.2991\n",
      "Epoch [121/200], Loss: 1.2920\n",
      "Epoch [131/200], Loss: 1.2853\n",
      "Epoch [141/200], Loss: 1.2789\n",
      "Epoch [151/200], Loss: 1.2727\n",
      "Epoch [161/200], Loss: 1.2668\n",
      "Epoch [171/200], Loss: 1.2610\n",
      "Epoch [181/200], Loss: 1.2553\n",
      "Epoch [191/200], Loss: 1.2498\n",
      "Test Accuracy: 28.63%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4223\n",
      "Epoch [11/200], Loss: 1.3804\n",
      "Epoch [21/200], Loss: 1.3594\n",
      "Epoch [31/200], Loss: 1.3405\n",
      "Epoch [41/200], Loss: 1.3231\n",
      "Epoch [51/200], Loss: 1.3074\n",
      "Epoch [61/200], Loss: 1.2934\n",
      "Epoch [71/200], Loss: 1.2812\n",
      "Epoch [81/200], Loss: 1.2701\n",
      "Epoch [91/200], Loss: 1.2601\n",
      "Epoch [101/200], Loss: 1.2511\n",
      "Epoch [111/200], Loss: 1.2425\n",
      "Epoch [121/200], Loss: 1.2343\n",
      "Epoch [131/200], Loss: 1.2263\n",
      "Epoch [141/200], Loss: 1.2186\n",
      "Epoch [151/200], Loss: 1.2113\n",
      "Epoch [161/200], Loss: 1.2041\n",
      "Epoch [171/200], Loss: 1.1972\n",
      "Epoch [181/200], Loss: 1.1905\n",
      "Epoch [191/200], Loss: 1.1840\n",
      "Test Accuracy: 21.55%\n",
      "Test Accuracy: 29.74%\n",
      "gme acc: {'recall': array([0.37884148, 0.18082071, 0.29935337, 0.534214  , 0.80286006]), 'recall_micro': 0.2974258786924983, 'recall_macro': 0.43921792315480435, 'precision': array([0.66637035, 0.39701137, 0.26902917, 0.18526663, 0.01173397]), 'precision_micro': 0.2974258786924983, 'precision_macro': 0.3058822988308314, 'f1_score': array([0.48305754, 0.24847314, 0.28338234, 0.27512076, 0.02312989]), 'g_mean': 0.3880154248311965, 'acc': 0.2974258786924983, 'auc': 0.5557264977478565, 'kappa': 0.08696546205649713, 'confusion_matrix': array([[31484, 18227,  2118,  1378, 29899],\n",
      "       [15265, 12673,  4110,  6661, 31377],\n",
      "       [  407,   661,  2361,   819,  3639],\n",
      "       [   89,   265,   125,  2022,  1284],\n",
      "       [    2,    95,    62,    34,   786]], dtype=int64)}\n",
      "round3\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3576\n",
      "Epoch [11/200], Loss: 1.3290\n",
      "Epoch [21/200], Loss: 1.3142\n",
      "Epoch [31/200], Loss: 1.3027\n",
      "Epoch [41/200], Loss: 1.2926\n",
      "Epoch [51/200], Loss: 1.2833\n",
      "Epoch [61/200], Loss: 1.2753\n",
      "Epoch [71/200], Loss: 1.2675\n",
      "Epoch [81/200], Loss: 1.2602\n",
      "Epoch [91/200], Loss: 1.2535\n",
      "Epoch [101/200], Loss: 1.2469\n",
      "Epoch [111/200], Loss: 1.2406\n",
      "Epoch [121/200], Loss: 1.2338\n",
      "Epoch [131/200], Loss: 1.2281\n",
      "Epoch [141/200], Loss: 1.2225\n",
      "Epoch [151/200], Loss: 1.2174\n",
      "Epoch [161/200], Loss: 1.2124\n",
      "Epoch [171/200], Loss: 1.2075\n",
      "Epoch [181/200], Loss: 1.2028\n",
      "Epoch [191/200], Loss: 1.1981\n",
      "Test Accuracy: 30.76%\n",
      "client1\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.2958\n",
      "Epoch [11/200], Loss: 1.2619\n",
      "Epoch [21/200], Loss: 1.2410\n",
      "Epoch [31/200], Loss: 1.2235\n",
      "Epoch [41/200], Loss: 1.2090\n",
      "Epoch [51/200], Loss: 1.1964\n",
      "Epoch [61/200], Loss: 1.1846\n",
      "Epoch [71/200], Loss: 1.1736\n",
      "Epoch [81/200], Loss: 1.1634\n",
      "Epoch [91/200], Loss: 1.1538\n",
      "Epoch [101/200], Loss: 1.1450\n",
      "Epoch [111/200], Loss: 1.1369\n",
      "Epoch [121/200], Loss: 1.1296\n",
      "Epoch [131/200], Loss: 1.1227\n",
      "Epoch [141/200], Loss: 1.1161\n",
      "Epoch [151/200], Loss: 1.1098\n",
      "Epoch [161/200], Loss: 1.1039\n",
      "Epoch [171/200], Loss: 1.0982\n",
      "Epoch [181/200], Loss: 1.0928\n",
      "Epoch [191/200], Loss: 1.0875\n",
      "Test Accuracy: 21.99%\n",
      "client2\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4067\n",
      "Epoch [11/200], Loss: 1.3618\n",
      "Epoch [21/200], Loss: 1.3396\n",
      "Epoch [31/200], Loss: 1.3217\n",
      "Epoch [41/200], Loss: 1.3064\n",
      "Epoch [51/200], Loss: 1.2929\n",
      "Epoch [61/200], Loss: 1.2811\n",
      "Epoch [71/200], Loss: 1.2703\n",
      "Epoch [81/200], Loss: 1.2602\n",
      "Epoch [91/200], Loss: 1.2509\n",
      "Epoch [101/200], Loss: 1.2420\n",
      "Epoch [111/200], Loss: 1.2339\n",
      "Epoch [121/200], Loss: 1.2263\n",
      "Epoch [131/200], Loss: 1.2191\n",
      "Epoch [141/200], Loss: 1.2123\n",
      "Epoch [151/200], Loss: 1.2057\n",
      "Epoch [161/200], Loss: 1.1993\n",
      "Epoch [171/200], Loss: 1.1929\n",
      "Epoch [181/200], Loss: 1.1867\n",
      "Epoch [191/200], Loss: 1.1809\n",
      "Test Accuracy: 26.47%\n",
      "client3\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3659\n",
      "Epoch [11/200], Loss: 1.3356\n",
      "Epoch [21/200], Loss: 1.3181\n",
      "Epoch [31/200], Loss: 1.3052\n",
      "Epoch [41/200], Loss: 1.2947\n",
      "Epoch [51/200], Loss: 1.2850\n",
      "Epoch [61/200], Loss: 1.2760\n",
      "Epoch [71/200], Loss: 1.2669\n",
      "Epoch [81/200], Loss: 1.2590\n",
      "Epoch [91/200], Loss: 1.2521\n",
      "Epoch [101/200], Loss: 1.2454\n",
      "Epoch [111/200], Loss: 1.2391\n",
      "Epoch [121/200], Loss: 1.2326\n",
      "Epoch [131/200], Loss: 1.2260\n",
      "Epoch [141/200], Loss: 1.2202\n",
      "Epoch [151/200], Loss: 1.2149\n",
      "Epoch [161/200], Loss: 1.2098\n",
      "Epoch [171/200], Loss: 1.2049\n",
      "Epoch [181/200], Loss: 1.2002\n",
      "Epoch [191/200], Loss: 1.1955\n",
      "Test Accuracy: 40.44%\n",
      "client4\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.4056\n",
      "Epoch [11/200], Loss: 1.3763\n",
      "Epoch [21/200], Loss: 1.3604\n",
      "Epoch [31/200], Loss: 1.3463\n",
      "Epoch [41/200], Loss: 1.3345\n",
      "Epoch [51/200], Loss: 1.3237\n",
      "Epoch [61/200], Loss: 1.3141\n",
      "Epoch [71/200], Loss: 1.3052\n",
      "Epoch [81/200], Loss: 1.2969\n",
      "Epoch [91/200], Loss: 1.2893\n",
      "Epoch [101/200], Loss: 1.2821\n",
      "Epoch [111/200], Loss: 1.2752\n",
      "Epoch [121/200], Loss: 1.2687\n",
      "Epoch [131/200], Loss: 1.2624\n",
      "Epoch [141/200], Loss: 1.2564\n",
      "Epoch [151/200], Loss: 1.2505\n",
      "Epoch [161/200], Loss: 1.2448\n",
      "Epoch [171/200], Loss: 1.2393\n",
      "Epoch [181/200], Loss: 1.2340\n",
      "Epoch [191/200], Loss: 1.2287\n",
      "Test Accuracy: 45.33%\n",
      "client5\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3354\n",
      "Epoch [11/200], Loss: 1.2927\n",
      "Epoch [21/200], Loss: 1.2737\n",
      "Epoch [31/200], Loss: 1.2587\n",
      "Epoch [41/200], Loss: 1.2464\n",
      "Epoch [51/200], Loss: 1.2356\n",
      "Epoch [61/200], Loss: 1.2256\n",
      "Epoch [71/200], Loss: 1.2164\n",
      "Epoch [81/200], Loss: 1.2079\n",
      "Epoch [91/200], Loss: 1.2002\n",
      "Epoch [101/200], Loss: 1.1929\n",
      "Epoch [111/200], Loss: 1.1859\n",
      "Epoch [121/200], Loss: 1.1793\n",
      "Epoch [131/200], Loss: 1.1732\n",
      "Epoch [141/200], Loss: 1.1673\n",
      "Epoch [151/200], Loss: 1.1616\n",
      "Epoch [161/200], Loss: 1.1560\n",
      "Epoch [171/200], Loss: 1.1504\n",
      "Epoch [181/200], Loss: 1.1450\n",
      "Epoch [191/200], Loss: 1.1399\n",
      "Test Accuracy: 35.90%\n",
      "client6\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3104\n",
      "Epoch [11/200], Loss: 1.2785\n",
      "Epoch [21/200], Loss: 1.2614\n",
      "Epoch [31/200], Loss: 1.2470\n",
      "Epoch [41/200], Loss: 1.2360\n",
      "Epoch [51/200], Loss: 1.2264\n",
      "Epoch [61/200], Loss: 1.2177\n",
      "Epoch [71/200], Loss: 1.2098\n",
      "Epoch [81/200], Loss: 1.2024\n",
      "Epoch [91/200], Loss: 1.1955\n",
      "Epoch [101/200], Loss: 1.1889\n",
      "Epoch [111/200], Loss: 1.1827\n",
      "Epoch [121/200], Loss: 1.1769\n",
      "Epoch [131/200], Loss: 1.1714\n",
      "Epoch [141/200], Loss: 1.1661\n",
      "Epoch [151/200], Loss: 1.1610\n",
      "Epoch [161/200], Loss: 1.1562\n",
      "Epoch [171/200], Loss: 1.1515\n",
      "Epoch [181/200], Loss: 1.1470\n",
      "Epoch [191/200], Loss: 1.1427\n",
      "Test Accuracy: 33.35%\n",
      "client7\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3233\n",
      "Epoch [11/200], Loss: 1.2886\n",
      "Epoch [21/200], Loss: 1.2734\n",
      "Epoch [31/200], Loss: 1.2607\n",
      "Epoch [41/200], Loss: 1.2495\n",
      "Epoch [51/200], Loss: 1.2396\n",
      "Epoch [61/200], Loss: 1.2306\n",
      "Epoch [71/200], Loss: 1.2227\n",
      "Epoch [81/200], Loss: 1.2155\n",
      "Epoch [91/200], Loss: 1.2086\n",
      "Epoch [101/200], Loss: 1.2021\n",
      "Epoch [111/200], Loss: 1.1960\n",
      "Epoch [121/200], Loss: 1.1901\n",
      "Epoch [131/200], Loss: 1.1845\n",
      "Epoch [141/200], Loss: 1.1791\n",
      "Epoch [151/200], Loss: 1.1740\n",
      "Epoch [161/200], Loss: 1.1692\n",
      "Epoch [171/200], Loss: 1.1645\n",
      "Epoch [181/200], Loss: 1.1598\n",
      "Epoch [191/200], Loss: 1.1551\n",
      "Test Accuracy: 34.70%\n",
      "client8\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3961\n",
      "Epoch [11/200], Loss: 1.3689\n",
      "Epoch [21/200], Loss: 1.3581\n",
      "Epoch [31/200], Loss: 1.3491\n",
      "Epoch [41/200], Loss: 1.3396\n",
      "Epoch [51/200], Loss: 1.3318\n",
      "Epoch [61/200], Loss: 1.3258\n",
      "Epoch [71/200], Loss: 1.3201\n",
      "Epoch [81/200], Loss: 1.3147\n",
      "Epoch [91/200], Loss: 1.3094\n",
      "Epoch [101/200], Loss: 1.3045\n",
      "Epoch [111/200], Loss: 1.2996\n",
      "Epoch [121/200], Loss: 1.2948\n",
      "Epoch [131/200], Loss: 1.2899\n",
      "Epoch [141/200], Loss: 1.2853\n",
      "Epoch [151/200], Loss: 1.2808\n",
      "Epoch [161/200], Loss: 1.2764\n",
      "Epoch [171/200], Loss: 1.2718\n",
      "Epoch [181/200], Loss: 1.2673\n",
      "Epoch [191/200], Loss: 1.2632\n",
      "Test Accuracy: 33.07%\n",
      "client9\n",
      "using IncrementalSampling\n",
      "class nums 5\n",
      "Epoch [1/200], Loss: 1.3791\n",
      "Epoch [11/200], Loss: 1.3482\n",
      "Epoch [21/200], Loss: 1.3340\n",
      "Epoch [31/200], Loss: 1.3211\n",
      "Epoch [41/200], Loss: 1.3088\n",
      "Epoch [51/200], Loss: 1.2989\n",
      "Epoch [61/200], Loss: 1.2897\n",
      "Epoch [71/200], Loss: 1.2808\n",
      "Epoch [81/200], Loss: 1.2727\n",
      "Epoch [91/200], Loss: 1.2648\n",
      "Epoch [101/200], Loss: 1.2573\n",
      "Epoch [111/200], Loss: 1.2499\n",
      "Epoch [121/200], Loss: 1.2427\n",
      "Epoch [131/200], Loss: 1.2359\n",
      "Epoch [141/200], Loss: 1.2292\n",
      "Epoch [151/200], Loss: 1.2224\n",
      "Epoch [161/200], Loss: 1.2159\n",
      "Epoch [171/200], Loss: 1.2094\n",
      "Epoch [181/200], Loss: 1.2030\n",
      "Epoch [191/200], Loss: 1.1969\n",
      "Test Accuracy: 29.14%\n"
     ]
    }
   ],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName,cluster_strategy):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{cluster_strategy}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i],cluster_strategy=cluster_strategy)\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list =['IncrementalSampling'] # ['no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "cluster_strategy_list = ['gmm','kmeans++','OPTICS','meanshift'] # 'kmeans','kmeans++','OPTICS','meanshift','spectral','kmedoids',\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, cluster_strategy in enumerate(cluster_strategy_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName='IncrementalSampling', settingName=settingname,cluster_strategy=cluster_strategy)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-08T08:21:27.129152400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
