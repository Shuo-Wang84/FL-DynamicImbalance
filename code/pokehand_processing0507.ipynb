{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "source_path = 'E:/FedStream/real_data_set/poker_lsn_arff/poker-lsn.arff'\n",
    "# 读取 arff 文件\n",
    "data, meta = arff.loadarff(source_path)\n",
    "\n",
    "# 将数据转换为 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 输出前6行数据\n",
    "print(df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 查看day列有几种不同的值\n",
    "unique_days = df['class'].unique()\n",
    "\n",
    "# 输出不同的值\n",
    "print(unique_days)\n",
    "\n",
    "# 输出不同值的数量\n",
    "print(len(unique_days))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_counts = df['class'].value_counts()\n",
    "print(class_counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # print(df.info())\n",
    "# print(829201 - 415526)\n",
    "# print(829201 - 415526 - 350426)\n",
    "# # 415526,350426,63249(39432,23817)\n",
    "# #  0.5011,0.4226,0.076(0.623,0.376)\n",
    "# # 6.5:5.56:1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将字节字符串转换为整数\n",
    "for i in range(5):\n",
    "    df[f's{i+1}']= df[f's{i+1}'].apply(lambda x: int(x.decode('utf-8')))\n",
    "df['class']= df['class'].apply(lambda x: int(x.decode('utf-8')))\n",
    "print(df.head(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_counts = df['class'].value_counts()\n",
    "print(class_counts)\n",
    "\"\"\"\n",
    "0    415526 没有任何特殊牌型\n",
    "1    350426 一对（One Pair）\t即有两张相同数字的牌\n",
    "2     39432 两对（Two Pairs）\t即有两对相同数字的牌\n",
    "3     17541 三条（Three of a Kind）\t即有三张相同数字的牌\n",
    "4      3225 顺子（Straight）\t即五张连续数字的牌\n",
    "5      1657 同花（Flush）\t即五张同一花色的牌\n",
    "6      1186 葫芦（Full House）\t即三张相同数字和一对相同数字的牌(应该不能保留)\n",
    "7       195 四条（Four of a kind）\t即有四张相同数字的牌\n",
    "8        11 同花顺（Straight Flush）\t即既是顺子又是同花的牌\n",
    "9         2 皇家同花顺（Royal Flush）\t即从10到A的同花顺\n",
    "0,     1(对子),     2(两对),    (3+6+7 有3张数字相同的牌),(4+5+8+9同花和顺子)\n",
    "415526 350426 39432 17541+1186+195(18922)  3225+1657+11+2(4895)\n",
    "84.88  71.588 8.05  3.8655                  1\n",
    "\"\"\"\n",
    "# 829201\n",
    "# 85+ 71.5+8+4+1 = 170\n",
    "# [170,143,16,7,2]\n",
    "#round client   sample_nums\n",
    "# 829  10       100\n",
    "# 415  10       200\n",
    "# 243  10       340\n",
    "# 415526(0无),350426(1对)，39432(两对2)，17541+1186+195(3张相同数字3，6，7)，3225+1657+11+2（同花和顺子4，5，8，9）\n",
    "# 415526(0无),350426(1对)，39432(两对2)，17541+1186+195(3张相同数字3，6，7)，3225+1657+11+2（同花和顺子4，5，8，9）\n",
    "'d'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0,     1(对子),     2(两对),    (3+6+7 有3张数字相同的牌),(4+5+8+9同花和顺子)\n",
    "# 415526 350426       39432     17541+1186+195(18922)  3225+1657+11+2(4895)\n",
    "# 84.88  71.588       8.05      3.8655                  1\n",
    "# print(17541+195)\n",
    "# print(sum([415526 ,350426,39432,17736,4895]))\n",
    "# print(17736/4895*2)\n",
    "# print(39432/4895*2)\n",
    "# print(350426/4895*2)\n",
    "# print(415526/4895*2)\n",
    "# [169,143,16,7,2]\n",
    "'d'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 将 0 类的数据归为 0，其他类归为 1\n",
    "# import copy\n",
    "# dfc = copy.deepcopy(df)\n",
    "# dfc['class'] = dfc['class'].apply(lambda x: 0 if x == 0 else 1)\n",
    "# print(dfc.head(10))\n",
    "# dfc.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_binary/pokerhand_binary.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 0,     1(对子),     2(两对),    (3+6+7 有3张数字相同的牌),(4+5+8+9同花和顺子)\n",
    "# 415526 350426 39432 17541+1186+195(18922)  3225+1657+11+2(4895)\n",
    "# 84.88  71.588 8.05  3.8655                  1\n",
    "\"\"\"\n",
    "# 829201\n",
    "# 85+ 71.5+8+4+1 = 170\n",
    "# [170,143,16,7,2]\n",
    "#round client   sample_nums\n",
    "# 829  10       100\n",
    "# 415  10       200\n",
    "# 243  10       340\n",
    "# 创建一个映射字典进行类别替换\n",
    "import copy\n",
    "dffive = copy.deepcopy(df)\n",
    "class_mapping = {3: 3, 6: 3, 7: 3, 4: 4, 5: 4, 8: 4, 9: 4}\n",
    "\n",
    "# 使用replace()函数替换类别\n",
    "dffive['class'] = dffive['class'].replace(class_mapping)\n",
    "class_countsf = dffive['class'].value_counts()\n",
    "print(class_countsf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dffive.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveclass.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from scipy.io import arff\n",
    "# source_path = 'E:/FedStream/real_data_set/poker_lsn_arff/poker-lsn.arff'\n",
    "# # 读取 arff 文件\n",
    "# data, meta = arff.loadarff(source_path)\n",
    "#\n",
    "# # 将数据转换为 DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# dffivedropC6 = copy.deepcopy(df)\n",
    "# dffivedropC6 = dffivedropC6[dffivedropC6['class'] != 6]\n",
    "# class_mapping = {3: 3, 7: 3, 4: 4, 5: 4, 8: 4, 9: 4}\n",
    "#\n",
    "# # 使用replace()函数替换类别\n",
    "# dffivedropC6['class'] = dffivedropC6['class'].replace(class_mapping)\n",
    "# dffivedropC6.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 分测试集和训练集\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# X = dfc.iloc[:, :-1]  # 特征\n",
    "# y = dfc.iloc[:, -1]   # 目标\n",
    "#\n",
    "# # 使用 train_test_split 分割数据\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # 如果你想要保持 DataFrame 的格式，你可以将 numpy 数组转换回 DataFrame\n",
    "# train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "# train_df['class'] = y_train\n",
    "#\n",
    "# test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "# test_df['class'] = y_test\n",
    "#\n",
    "# # 现在 train_df 包含训练集的特征和目标，test_df 包含测试集的特征和目标\n",
    "# # 将训练集保存为 CSV 文件\n",
    "# train_df.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_binary/train.csv', index=False)\n",
    "#\n",
    "# # 将测试集保存为 CSV 文件\n",
    "# test_df.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_binary/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设df是包含新的数据集的DataFrame，df['class']是类别列\n",
    "# 按类别进行数据集拆分\n",
    "train_data = pd.DataFrame()  # 存储训练集数据\n",
    "test_data = pd.DataFrame()   # 存储测试集数据\n",
    "\n",
    "# 按类别拆分数据集\n",
    "for class_label in dffive['class'].unique():\n",
    "    class_data = dffive[dffive['class'] == class_label]  # 获取特定类别的数据\n",
    "\n",
    "    # 将特定类别的数据集拆分为训练集和测试集\n",
    "    X_train, X_test = train_test_split(class_data, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # 将拆分后的数据添加到相应的数据集中\n",
    "    train_data = train_data.append(X_train)\n",
    "    test_data = test_data.append(X_test)\n",
    "\n",
    "class_countsft = train_data['class'].value_counts()\n",
    "print(class_countsft)\n",
    "class_countsftt = test_data['class'].value_counts()\n",
    "print(class_countsftt)\n",
    "# # 将训练集保存为 CSV 文件\n",
    "# train_data.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_five/train.csv', index=False)\n",
    "#\n",
    "# # 将测试集保存为 CSV 文件\n",
    "# test_data.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将训练集保存为 CSV 文件\n",
    "train_data.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_five/train.csv', index=False)\n",
    "\n",
    "# 将测试集保存为 CSV 文件\n",
    "test_data.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # 假设df是包含新的数据集的DataFrame，df['class']是类别列\n",
    "# # 按类别进行数据集拆分\n",
    "# train_datad = pd.DataFrame()  # 存储训练集数据\n",
    "# test_datad = pd.DataFrame()   # 存储测试集数据\n",
    "#\n",
    "# # 按类别拆分数据集\n",
    "# for class_label in dffivedropC6['class'].unique():\n",
    "#     class_data = dffivedropC6[dffivedropC6['class'] == class_label]  # 获取特定类别的数据\n",
    "#\n",
    "#     # 将特定类别的数据集拆分为训练集和测试集\n",
    "#     X_train, X_test = train_test_split(class_data, test_size=0.2, shuffle=False)\n",
    "#\n",
    "#     # 将拆分后的数据添加到相应的数据集中\n",
    "#     train_datad = train_datad.append(X_train)\n",
    "#     test_datad = test_datad.append(X_test)\n",
    "# # 将训练集保存为 CSV 文件\n",
    "# train_datad.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6/train.csv', index=False)\n",
    "#\n",
    "# # 将测试集保存为 CSV 文件\n",
    "# test_datad.to_csv('E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6/test.csv', index\n",
    "print(test_data.head())\n",
    "print(test_data.info())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "\"\"\"\n",
    "# 0,     1(对子),     2(两对),    (3+6+7 有3张数字相同的牌),(4+5+8+9同花和顺子)\n",
    "# 415526 350426 39432 17541+1186+195(18922)  3225+1657+11+2(4895)\n",
    "# 84.88  71.588 8.05  3.8655                  1\n",
    "\"\"\"\n",
    "# 829201\n",
    "# 85+ 71.5+8+4+1 = 170\n",
    "# [170,143,16,7,2]\n",
    "#round client   sample_nums\n",
    "# 829  10       100\n",
    "# 415  10       200\n",
    "# 195  10       340 # 663358\n",
    "# 创建一个映射字典进行类别替换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 4.联邦学习数据集\n",
    "# import os\n",
    "# client_nums = 10\n",
    "# per_round_data_nums = 100\n",
    "# base_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_binary/pokerhand_client/'\n",
    "# # 829201/1000 =\n",
    "# # 多余轮数是空数据\n",
    "# for r in range(830):    # 0,1,2...36\n",
    "#     for c in range(client_nums):     # 0,1,2...9\n",
    "#         start_index = (r*10 + c)*per_round_data_nums\n",
    "#         # end_index = start_index+per_round_data_nums\n",
    "#         path = os.path.join(base_path,f'client_{c}')\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         selected_data = train_df.iloc[start_index:start_index + per_round_data_nums]\n",
    "#         file_path = os.path.join(path,f'round_{r}.csv')\n",
    "#         selected_data.to_csv(file_path, index=False)\n",
    "#         print(f\"client: {c} round:{r}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(829201/1000)\n",
    "print(663358/sum([170, 143, 16, 7, 2]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 类别比例和类别标签\n",
    "class_ratios = [170, 143, 16, 7, 2]\n",
    "class_labels = train_data['class'].unique()  # 确保dffive里有class_label这一列\n",
    "total_samples_per_file = sum(class_ratios)  # 每个文件的总样本数\n",
    "\"\"\"\n",
    "0    332420\n",
    "1    280340\n",
    "2     31545\n",
    "3     15137\n",
    "4      3916\n",
    "\"\"\"\n",
    "# 基本目录\n",
    "base_dir = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/pokerhand_client/'\n",
    "\n",
    "# 创建文件夹（如果尚不存在）\n",
    "for j in range(10):\n",
    "    os.makedirs(os.path.join(base_dir, f\"client_{j}\"), exist_ok=True)\n",
    "\n",
    "# 计算可以生成的总文件数\n",
    "# 3916/2 = 1958\n",
    "# 分割数据并保存为文件\n",
    "for i in range(1958):\n",
    "    samples = []\n",
    "    for label, ratio in zip(class_labels, class_ratios):\n",
    "        class_subset = train_data[train_data['class'] == label]\n",
    "        if len(class_subset) < ratio:\n",
    "            continue  # 如果剩余样本不足，则跳过此类别\n",
    "        sample = class_subset.sample(n=ratio, replace=False)\n",
    "        train_data = train_data.drop(sample.index)  # 实现不放回抽样\n",
    "        samples.append(sample)\n",
    "        print(f'class: {label}, number: {ratio}')\n",
    "\n",
    "    # 合并样本并保存到文件\n",
    "    result_df = pd.concat(samples, ignore_index=True)\n",
    "    client_id = i % 10\n",
    "    round_number = i // 10\n",
    "    file_path = os.path.join(base_dir, f\"client_{client_id}\", f\"round_{round_number}.csv\")\n",
    "    result_df.to_csv(file_path, index=False)\n",
    "    print(f'round : {round_number},clent:{client_id}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 假设以下变量已定义\n",
    "class_ratios = [170, 143, 16, 7, 2]\n",
    "class_labels = train_data['class'].unique()  # 确保dffive里有class_label这一列\n",
    "total_samples_per_file = sum(class_ratios)  # 每个文件的总样本数\n",
    "\n",
    "# 基本目录\n",
    "base_dir = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/pokerhand_client/'\n",
    "\n",
    "# 创建文件夹（如果尚不存在）\n",
    "for j in range(10):\n",
    "    os.makedirs(os.path.join(base_dir, f\"client_{j}\"), exist_ok=True)\n",
    "\n",
    "# 计算可以生成的总文件数\n",
    "# 3916/2 = 1958\n",
    "# 分割数据并保存为文件\n",
    "for i in range(1958,1959):\n",
    "    samples = []\n",
    "    sample_counts = {label: 0 for label in class_labels}  # 初始化字典记录每个类别的样本数量\n",
    "\n",
    "    train_data = dffive.copy()  # 每次循环重新复制数据\n",
    "\n",
    "    for label, ratio in zip(class_labels, class_ratios):\n",
    "        class_subset = train_data[train_data['class'] == label]\n",
    "        if len(class_subset) < ratio:\n",
    "            print(f\"Not enough samples for class {label} in round {i}\")\n",
    "            continue  # 如果剩余样本不足，则跳过此类别\n",
    "        sample = class_subset.sample(n=ratio, replace=False)\n",
    "        train_data = train_data.drop(sample.index)  # 实现不放回抽样\n",
    "        samples.append(sample)\n",
    "        sample_counts[label] += len(sample)\n",
    "        print(f'class: {label}, number: {len(sample)}')\n",
    "\n",
    "    # 合并样本并保存到文件\n",
    "    result_df = pd.concat(samples, ignore_index=True)\n",
    "    client_id = i % 10\n",
    "    round_number = i // 10\n",
    "    file_path = os.path.join(base_dir, f\"client_{client_id}\", f\"round_{round_number}.csv\")\n",
    "    result_df.to_csv(file_path, index=False)\n",
    "    print(f'round: {round_number}, client: {client_id}')\n",
    "    print(f\"Round {i}: {sample_counts}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 类别比例和类别标签\n",
    "# class_ratios = [169,143,16,7,2]\n",
    "# class_labels = dffivedropC6['class'].unique()  # 确保dffive里有class_label这一列\n",
    "# total_samples_per_file = sum(class_ratios)  # 每个文件的总样本数\n",
    "#\n",
    "# # 基本目录\n",
    "# base_dir = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6/pokerhand_client/'\n",
    "#\n",
    "# # 创建文件夹（如果尚不存在）\n",
    "# for j in range(10):\n",
    "#     os.makedirs(os.path.join(base_dir, f\"client_{j}\"), exist_ok=True)\n",
    "#\n",
    "# # 计算可以生成的总文件数\n",
    "#\n",
    "# # 分割数据并保存为文件\n",
    "# for i in range(1963):\n",
    "#     samples = []\n",
    "#     for label, ratio in zip(class_labels, class_ratios):\n",
    "#         class_subset = dffivedropC6[dffivedropC6['class'] == label]\n",
    "#         if len(class_subset) < ratio:\n",
    "#             continue  # 如果剩余样本不足，则跳过此类别\n",
    "#         sample = class_subset.sample(n=ratio, replace=False)\n",
    "#         dffivedropC6 = dffivedropC6.drop(sample.index)  # 实现不放回抽样\n",
    "#         samples.append(sample)\n",
    "#\n",
    "#     # 合并样本并保存到文件\n",
    "#     result_df = pd.concat(samples, ignore_index=True)\n",
    "#     client_id = i % 10\n",
    "#     round_number = i // 10\n",
    "#     file_path = os.path.join(base_dir, f\"client_{client_id}\", f\"round_{round_number}.csv\")\n",
    "#     result_df.to_csv(file_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 上诉代码没有共享模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.linalg import norm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 定义MLP模型\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class IncrementalSampling(object):\n",
    "    def __init__(self,prototype_nums = 30):\n",
    "        self.incremental_data_map = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.k_value = prototype_nums\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"把当前轮的数据，和类map中的数据进行合并\"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        # 考虑到有新类出现的情况\n",
    "        # 如果current_unique_labels有新label,直接扩充incremental_data_map\n",
    "        # 如果label 是incremental_data_map中已经有的，扩张incremental_data_map对应label中data的长度\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_data_map:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_data_map[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_data_map[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_data_map[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "    def cluster_data(self,data, num_clusters):\n",
    "        kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "        # kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(data)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        return  cluster_centers\n",
    "    def reCludter(self):\n",
    "        # incremental_data_map的大小进行压缩self.reCluster_instance_nums\n",
    "        new_cluster = {}\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) <= self.k_value:\n",
    "                # 全部保存\n",
    "                new_cluster[label] = data\n",
    "            else:\n",
    "                sampled_nums = self.k_value\n",
    "                clusters = self.cluster_data(data = data, num_clusters=sampled_nums)\n",
    "                new_cluster[label] = clusters\n",
    "        self.incremental_data_map = new_cluster\n",
    "    def compute_sampling_nums(self,combinde_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combinde_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def data_sampling(self,sampling_nums):\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) > sampling_nums:\n",
    "                # 那就是下采样了,不放回采样\n",
    "                sampled_indices = random.sample(range(len(data)), sampling_nums)\n",
    "                sampled_data = [data[i] for i in sampled_indices]\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            elif len(data) == sampling_nums:\n",
    "                # 直接复制\n",
    "                resampling_data.extend(data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            else:\n",
    "                # 上采样,保存原样本\n",
    "                resampling_data.extend(data)\n",
    "                # 随机有放回的找差额部分\n",
    "                sampled_data = random.choices(data, k=(sampling_nums-len(data)))\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,incremental_prototype_map,data,label,sampling_strategy = \"OverSampling\"):\n",
    "        self.incremental_data_map = incremental_prototype_map\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_data_map)\n",
    "        if sampling_strategy.lower() == \"oversampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"downsampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(min_length)\n",
    "        else:\n",
    "            print(\"No sampling measures have been taken\")\n",
    "        self.reCludter()\n",
    "        return resampling_data,resampling_label,self.incremental_data_map\n",
    "\n",
    "class Triplets(object):\n",
    "    def __init__(self, n_neighbors=5, random=True, len_lim=True, **kwargs):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.random = random\n",
    "        self.len_lim = len_lim\n",
    "\n",
    "    def fit_resample(self, x, y):\n",
    "        strategy = self._sample_strategy(y)\n",
    "        self.n_neighbors = max(self.n_neighbors, self.counts.max() // self.counts.min())\n",
    "\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        # 这里的代码平衡状态会报错\n",
    "        for c, size in enumerate(strategy):\n",
    "            if size == 0: continue\n",
    "            weight = self._weights(x, y, c)\n",
    "            gen_x_c, gen_y_c = self._sample_one(x, y, c, size, weight)\n",
    "            gen_x += gen_x_c\n",
    "            gen_y += gen_y_c\n",
    "\n",
    "        # 为了这个方法在平衡状态下不报错，我们特地在这里加了这段代码\n",
    "        # To prevent errors in this method when in a balanced state, we intentionally added this code block\n",
    "        if len(gen_x)==0:\n",
    "            return x,y\n",
    "        gen_x = np.vstack(gen_x)\n",
    "        gen_y = np.array(gen_y)\n",
    "        return np.concatenate((x, gen_x), axis=0), np.concatenate((y, gen_y), axis=0)\n",
    "\n",
    "    def _sample_strategy(self, y):\n",
    "        _, self.counts = np.unique(y, return_counts=True)\n",
    "        return max(self.counts) - self.counts\n",
    "\n",
    "    def _weights(self, x, y, c):\n",
    "        return np.ones(self.counts[c])\n",
    "\n",
    "    def _sample_one(self, x, y, c, size, weight):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        # get the indices of minority and majority instances\n",
    "        min_idxs = np.where(y == c)[0]\n",
    "        maj_idxs = np.where(y != c)[0]\n",
    "\n",
    "        # find nearest majority neighbors for each minority instance\n",
    "        nbrs = NearestNeighbors(n_neighbors=self.n_neighbors).fit(x[maj_idxs])\n",
    "        _, indices = nbrs.kneighbors(x[min_idxs])\n",
    "\n",
    "        # generate synthetic data\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = x[min_idxs[j]]\n",
    "            tp2 = x[maj_idxs[indices[j][:5]]].mean(axis=0)\n",
    "            # tp3_ord = np.random.randint(1, self.n_neighbors)\n",
    "            tp3_ord = np.random.randint(self.n_neighbors)\n",
    "            tp3 = x[maj_idxs[indices[j][tp3_ord]]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(c)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            if self.len_lim: offset = offset * min(1, norm(tp1 - tp2) / norm(offset))\n",
    "            coef = np.random.rand() if self.random is True else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(c)\n",
    "        return gen_x, gen_y\n",
    "\n",
    "def train_model_FedAvg_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "def train_model_FedProx_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    mu = 0.1\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "    # not use deepcopy ,because model as parameter transport in this ,update model also update model\n",
    "    # current_local_model = cmodel\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # FedProx\n",
    "        prox_term = 0.0\n",
    "        for p_i, param in enumerate(model.parameters()):\n",
    "                prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "        loss += prox_term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #\n",
    "    #         # fedprox\n",
    "    #         prox_term = 0.0\n",
    "    #         for p_i, param in enumerate(model.parameters()):\n",
    "    #             prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "    #         loss += prox_term\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Total Loss: {total_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "def train_model_FedNova_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau +=len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         # model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         tau +=1\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    return losses, coeff, norm_grad,len(X_train_tensor)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "def train_model_FedNova_local_gpu(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # 初始化损失列表和模型\n",
    "    losses = []\n",
    "    model = input_model.to(device)  # 将模型迁移到GPU\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 保留初始模型权重的深拷贝并迁移到设备\n",
    "    global_weights = {k: v.clone().to(device) for k, v in input_model.state_dict().items()}\n",
    "\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "\n",
    "    # 将数据迁移到GPU\n",
    "    X_train_tensor = X_train_tensor.to(device)\n",
    "    y_train_tensor = y_train_tensor.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau += len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 将每个epoch的损失保存到CPU内存中\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # 计算修正因子\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "\n",
    "    # 获取当前模型状态并计算归一化梯度\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    # 将归一化梯度迁移到CPU\n",
    "    norm_grad = {k: v.cpu() for k, v in norm_grad.items()}\n",
    "\n",
    "    # 将模型权重转回CPU\n",
    "    model.to('cpu')\n",
    "\n",
    "    return losses, coeff, norm_grad, len(X_train_tensor)\n",
    "\n",
    "# 定义模型参数共享函数\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "\n",
    "# # 定义模型参数聚合函数\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def save_loss(loss_list,client_id,round_id,save_loss_path):\n",
    "    if not os.path.exists(save_loss_path):\n",
    "        os.makedirs(save_loss_path)\n",
    "    # 构建文件路径\n",
    "    file_path = os.path.join(save_loss_path, f\"client_{client_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 CSV 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 将损失值添加到 DataFrame 中\n",
    "    column_name = f'round_{round_id}'\n",
    "    df[column_name] = loss_list\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def save_model(global_model,round_id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'round_{round_id}_gm.pt')\n",
    "    torch.save(global_model,model_path)\n",
    "\n",
    "def save_metrics(title, rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE,KMeansSMOTE,SVMSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids,RandomUnderSampler,NearMiss,TomekLinks,EditedNearestNeighbours,RepeatedEditedNearestNeighbours,AllKNN,CondensedNearestNeighbour,OneSidedSelection,NeighbourhoodCleaningRule,InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "# SMOTE,ROS,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE1,BorderlineSMOTE2,KMeansSMOTE,SVMSMOTE\n",
    "# ClusterCentroids,RUS,NearMiss1,NearMiss2,NearMiss2,TomekLinks,ENN,RENN,AllKNN,CNN,OSS,NC,IHT\n",
    "# SMOTEENN,SMOTETomek\n",
    "def data_sampling(raw_X,raw_y,sampling_strategy):\n",
    "    if sampling_strategy.upper() == 'NO' :\n",
    "        return raw_X,raw_y\n",
    "    # overSampling\n",
    "    elif sampling_strategy.upper() == 'SMOTE': # overSampling\n",
    "        \"\"\"\n",
    "            1.对于样本x ,按照欧氏距离找到离其距离最近的K个近邻样本\n",
    "            2.确定采样比例，然后从K个近邻中选择x_n\n",
    "            3.公式 x_new = x + rand(0,1)*(x_n-x)\n",
    "        \"\"\"\n",
    "        smote = SMOTE( random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(raw_X,raw_y)\n",
    "        return X_resampled, y_resampled\n",
    "    elif sampling_strategy == \"RandomOverSampler\" or sampling_strategy.upper()=='ROS': # overSampling\n",
    "        ros = RandomOverSampler(random_state=1)\n",
    "        ros_data,ros_label = ros.fit_resample(raw_X,raw_y)\n",
    "        return ros_data,ros_label\n",
    "    elif sampling_strategy.upper() == 'SMOTENC':    # overSampling\n",
    "        smotenc = SMOTENC(random_state=1,categorical_features=[0])\n",
    "        smotenc_data,smotenc_label = smotenc.fit_resample(raw_X,raw_y)\n",
    "        return smotenc_data,smotenc_label\n",
    "    elif sampling_strategy.upper() == 'SMOTEN': # overSampling\n",
    "        smoten = SMOTEN(random_state=1)\n",
    "        smoten_data,smoten_label = smoten.fit_resample(raw_X,raw_y)\n",
    "        return smoten_data,smoten_label\n",
    "    elif sampling_strategy.upper() =='ADASYN':\n",
    "        adasyn = ADASYN(random_state=1)\n",
    "        adasyn_data,adasyn_label = adasyn.fit_resample(raw_X,raw_y)\n",
    "        return adasyn_data,adasyn_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE1' or sampling_strategy.upper()=='BSMOTE1':\n",
    "        bsmote1 = BorderlineSMOTE(kind='borderline-1',random_state=1)\n",
    "        bsmote1_data,bsmote1_label = bsmote1.fit_resample(raw_X,raw_y)\n",
    "        return bsmote1_data,bsmote1_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE2'or sampling_strategy.upper()=='BSMOTE2':\n",
    "        bsmote2 = BorderlineSMOTE(kind='borderline-2',random_state=1)\n",
    "        bsmote2_data,bsmote2_label = bsmote2.fit_resample(raw_X,raw_y)\n",
    "        return bsmote2_data,bsmote2_label\n",
    "    elif sampling_strategy == 'KMeansSMOTE' or sampling_strategy.upper() == 'KSMOTE':\n",
    "        kmeanssmote = KMeansSMOTE(random_state=1)\n",
    "        kmeanssmote_data,kmeanssmote_label = kmeanssmote.fit_resample(raw_X,raw_y)\n",
    "        return kmeanssmote_data,kmeanssmote_label\n",
    "    elif sampling_strategy == 'SVMSMOTE':\n",
    "        svmsmote = SVMSMOTE(random_state=1)\n",
    "        svmsmote_data,svmsmote_label = svmsmote.fit_resample(raw_X,raw_y)\n",
    "        return svmsmote_data,svmsmote_label\n",
    "    # downSampling\n",
    "    elif sampling_strategy == 'ClusterCentroids': # down-sampling,generate\n",
    "        clustercentroids = ClusterCentroids(random_state=1)\n",
    "        clustercentroids_data,clustercentroids_label = clustercentroids.fit_resample(raw_X,raw_y)\n",
    "        return clustercentroids_data,clustercentroids_label\n",
    "    elif sampling_strategy=='RandomUnderSampler' or sampling_strategy.upper()=='RUS':\n",
    "        rus = RandomUnderSampler(random_state=1)\n",
    "        rus_data,rus_label = rus.fit_resample(raw_X,raw_y)\n",
    "        return rus_data,rus_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS1':\n",
    "        # 在k个少数类别样本中，选择出与他们-平均距离最近的多数类样本-进行保存\n",
    "        nearmiss1 = NearMiss(version=1)\n",
    "        nearmiss1_data,nearmiss1_label = nearmiss1.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss1_data,nearmiss1_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS2':\n",
    "        # 选择K个距离最远的少数类别样本，然后根据这些样本选出的\"平均距离最近\"的样本进行保存\n",
    "        nearmiss2 = NearMiss(version=2)\n",
    "        nearmiss2_data,nearmiss2_label = nearmiss2.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss2_data,nearmiss2_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS3':\n",
    "        # 1、对于每一个少数类别样本，保留其K个最近邻多数类样本；2、把到K个少数样本平均距离最大的多数类样本保存下来。\n",
    "        nearmiss3 = NearMiss(version=3)\n",
    "        nearmiss3_data,nearmiss3_label = nearmiss3.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss3_data,nearmiss3_label\n",
    "    elif sampling_strategy == 'TomekLinks' or sampling_strategy.upper()=='TOMEK':\n",
    "        # 它需要计算每个样本之间的距离，然后把互为最近邻且类别不同的一对样本拿出来，根据需求的选择将这一对数据进行剔除 or 把多数类样本剔除\n",
    "        tomelink = TomekLinks(sampling_strategy='all')#sampling_strategy='all'表示全部删除，'auto'表示只删除多数类\n",
    "        tomelink_data,tomelink_label = tomelink.fit_resample(raw_X,raw_y)\n",
    "        return tomelink_data,tomelink_label\n",
    "    elif sampling_strategy == 'EditedNearestNeighbours' or sampling_strategy.upper() == 'ENN':\n",
    "        ENN = EditedNearestNeighbours()\n",
    "        ENN_data,ENN_label = ENN.fit_resample(raw_X,raw_y)\n",
    "        return ENN_data,ENN_label\n",
    "    elif sampling_strategy == 'RepeatedEditedNearestNeighbours' or sampling_strategy.upper() == 'RENN':\n",
    "        RENN = RepeatedEditedNearestNeighbours()\n",
    "        RENN_data,RENN_label = RENN.fit_resample(raw_X,raw_y)\n",
    "        return RENN_data,RENN_label\n",
    "    elif sampling_strategy =='AllKNN':\n",
    "        ## ENN的改进版本，和RepeatedEditedNearestNeighbours一样，会多次迭代ENN 算法，不同之处在于，他会每次增加KNN的K值\n",
    "        allknn = AllKNN()\n",
    "        allknn_data,allknn_label = allknn.fit_resample(raw_X,raw_y)\n",
    "        return allknn_data,allknn_label\n",
    "    elif sampling_strategy == 'CondensedNearestNeighbour'or sampling_strategy.upper() == 'CNN':\n",
    "        ## 如果有样本无法和其他多数类样本聚类到一起，那么说明它极有可能是边界的样本，所以将这些样本加入到集合中\n",
    "        CNN = CondensedNearestNeighbour(random_state=1)\n",
    "        CNN_data,CNN_label = CNN.fit_resample(raw_X,raw_y)\n",
    "        return CNN_data,CNN_label\n",
    "    elif sampling_strategy == 'OneSidedSelection' or sampling_strategy.upper() == 'OSS':\n",
    "        # OneSidedSelection = tomekLinks + CondensedNearestNeighbour,先使用自杀式的方式把大类数据中的其他值剔除，然后再使用CondensedNearestNeighbour的下采样\n",
    "        OSS = OneSidedSelection(random_state=1)\n",
    "        OSS_data,OSS_label = OSS.fit_resample(raw_X,raw_y)\n",
    "        return OSS_data,OSS_label\n",
    "    elif sampling_strategy == 'NeighbourhoodCleaningRule'or sampling_strategy.upper() == 'NC':\n",
    "        # 若在大类的K-近邻中，少数类占多数，那就剔除这个多数类别的样本\n",
    "        NC = NeighbourhoodCleaningRule()\n",
    "        NC_data,NC_label = NC.fit_resample(raw_X,raw_y)\n",
    "        return NC_data,NC_label\n",
    "    elif sampling_strategy == 'InstanceHardnessThreshold' or sampling_strategy.upper() == 'IHT':\n",
    "        # 默认算法是随机森林，通过分类算法给出样本阈值来剔除部分样本，（阈值较低的可以剔除）,慢\n",
    "        IHT = InstanceHardnessThreshold(random_state=1)\n",
    "        IHT_data,IHT_label = IHT.fit_resample(raw_X,raw_y)\n",
    "        return IHT_data,IHT_label\n",
    "    # hibird\n",
    "    elif sampling_strategy.upper() =='SMOTEENN':\n",
    "        se = SMOTEENN(random_state=1)\n",
    "        se_data,se_label = se.fit_resample(raw_X,raw_y)\n",
    "        return se_data,se_label\n",
    "    elif sampling_strategy.upper() =='SMOTETOMEK':\n",
    "        st = SMOTETomek(random_state=1)\n",
    "        st_data,st_label = st.fit_resample(raw_X,raw_y)\n",
    "        return st_data,st_label\n",
    "    elif sampling_strategy == 'Triplets':\n",
    "        print(\" Triplets sampling\")\n",
    "        tpl = Triplets()\n",
    "        tpl_data,tpl_label = tpl.fit_resample(raw_X,raw_y)\n",
    "        return tpl_data,tpl_label\n",
    "    else :\n",
    "        print(\"skipped all the sampling strategy,but return the raw data and label\")\n",
    "        return raw_X,raw_y\n",
    "def inremental_sampling(prototype_map,raw_X,raw_y):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    isap = IncrementalSampling()\n",
    "    resampling_data,resampling_label,prototype_map = isap.fit(incremental_prototype_map=prototype_map,\n",
    "                                                              data=raw_X,\n",
    "                                                              label=raw_y)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "\n",
    "def read_data_return_tensor(dataset_path, round_id, client_id, sampling_strategy='no',prototype_map = {}):\n",
    "    folder_path = os.path.join(dataset_path, f'client_{client_id}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    open_file_path = os.path.join(folder_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :-1].values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    raw_y = raw_y.astype(float)\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    if sampling_strategy == 'IncrementalSampling':\n",
    "        print(\"using IncrementalSampling\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling(prototype_map,raw_X,raw_y)\n",
    "    else:\n",
    "        resampling_X,resampling_y = data_sampling(raw_X,raw_y,sampling_strategy)\n",
    "        resampling_y.astype(float)\n",
    "    resampling_X = np.array(resampling_X)  # 将列表转换为单个NumPy数组\n",
    "    resampling_y = np.array(resampling_y)  # 将列表转换为单个NumPy数组\n",
    "\n",
    "    X_train_tensor = torch.tensor(resampling_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(resampling_y, dtype=torch.long)   # 标签张量\n",
    "    return X_train_tensor,y_train_tensor,prototype_map\n",
    "\n",
    "def read_test_data(test_data_path):\n",
    "    data = pd.read_csv(test_data_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    X = data_shuffled.iloc[:, :-1].values.astype(float)  # 特征\n",
    "    # print(X)\n",
    "    y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    y = y.astype(float)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X, dtype=torch.float32)  # 特征张量\n",
    "    y_test_tensor = torch.tensor(y, dtype=torch.long)   # 标签张量\n",
    "    return X_test_tensor,y_test_tensor\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 2\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 200\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_binary'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_binary/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local_gpu(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list = ['no','IncrementalSampling']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, saplingname in enumerate(sampling_strategy_name_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName=saplingname, settingName=settingname)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local_gpu(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list = ['no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets']\n",
    "dataset_list = ['pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, saplingname in enumerate(sampling_strategy_name_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName=saplingname, settingName=settingname)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list = ['no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, saplingname in enumerate(sampling_strategy_name_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName=saplingname, settingName=settingname)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "# total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "# total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "#\n",
    "# print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "# print(ex_time_list)\n",
    "\"\"\"\n",
    "Total execution time: 135627.5007185936 seconds\n",
    "[2434.614844560623, 4426.562899589539, 3613.1970567703247, 1943.9509642124176, 2404.102521419525, 5765.652966499329, 2518.134982585907, 7660.604575634003, 6349.954786539078, 2033.7760112285614, 2528.6347618103027, 7154.482527017593, 2691.5173461437225, 5801.096624612808, 3338.4122800827026, 1937.0850281715393, 2505.6549842357635, 5244.7070598602295, 2638.279860496521, 8688.388283491135, 6830.487842321396, 2135.0945937633514, 2608.741988658905, 7460.140278339386, 2768.8091423511505, 9888.478866577148, 7655.043273925781, 2270.2126574516296, 2683.927391767502, 9047.510742425919]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6/pokerhand_client/\n",
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_fiveDropC6'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_fiveDropC6/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local_gpu(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        # save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "\n",
    "import time\n",
    "\n",
    "sampling_strategy_name_list = ['IncrementalSampling']#, 'RandomOverSampler', 'RandomUnderSampler', 'CondensedNearestNeighbour', 'Triplets', 'IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client']#,'CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class','CoverType_client_Full_class']\n",
    "\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, saplingname in enumerate(sampling_strategy_name_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedNova(samplingName=saplingname, settingName=settingname)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        time.sleep(20)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp = []\n",
    "def runFedAvg(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedAvg_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,\n",
    "                                                                            sampling_strategy=sampling_strategy_name,\n",
    "                                                                            prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses = train_model_FedAvg_local(clients_models[i],\n",
    "                             X_train_local,\n",
    "                             y_train_local,\n",
    "                             num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "            # 共享本地模型参数\n",
    "            local_params_list.append(share_params(clients_models[i]))\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        # 聚合本地模型参数到全局模型\n",
    "        aggregated_params = aggregate_params(local_params_list)\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        # save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor)\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(me)\n",
    "def runFedProx(samplingName,settingName):\n",
    "    num_clients = 10\n",
    "    # 初始化全局模型和客户端模型\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    # 'E:/FedStream/real_data_set/realdataset0427/elecNorm/Electricity_client_random/'\n",
    "    base_path ='E:/FedStream/' # 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName # 'CovA_Abrupt_BtoVH'\n",
    "    # SMOTE cant use in VH,H\n",
    "    # sampling_strategy_name_list = ['no','RandomOverSampler','RandomUnderSampler','CondensedNearestNeighbour','Triplets','IncrementalSampling']\n",
    "    # sampling_strategy_name = sampling_strategy_name_list[sampling_id]\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedProx_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\\CoverType_client_Full_class\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    # E:\\FedStream\\real_data_set\\realdataset0427\\covertypeNorm\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,\n",
    "                                                                            sampling_strategy=sampling_strategy_name,\n",
    "                                                                            prototype_map=client_prototype_map[i])\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses = train_model_FedProx_local(clients_models[i],\n",
    "                             X_train_local,\n",
    "                             y_train_local,\n",
    "                             num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "            # 共享本地模型参数\n",
    "            local_params_list.append(share_params(clients_models[i]))\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        # 聚合本地模型参数到全局模型\n",
    "        aggregated_params = aggregate_params(local_params_list)\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        # save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor)\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(me)\n",
    "# # 8*6*5*2=480/60=8\n",
    "# import time\n",
    "# # sampling_strategy_name_list = ['Triplets','no']# 'IncrementalSampling']# 'CondensedNearestNeighbour']# 'RandomOverSampler', 'RandomUnderSampler'] # , 'CondensedNearestNeighbour', 'Triplets','no','IncrementalSampling']\n",
    "# # dataset_list = ['pokerhand_client' ,'pokerhand_client' ,'pokerhand_client','pokerhand_client','pokerhand_client']\n",
    "# # for j ,settingname in enumerate(dataset_list):\n",
    "# #     for i,saplingname in enumerate(sampling_strategy_name_list):\n",
    "# #         runFedAvg(samplingName=saplingname,settingName=settingname)\n",
    "# #         time.sleep(10)\n",
    "#\n",
    "# sampling_strategy_name_list = ['Triplets','no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler' , 'CondensedNearestNeighbour']#, 'Triplets','no','IncrementalSampling']\n",
    "# dataset_list = ['pokerhand_client' ,'pokerhand_client' ,'pokerhand_client','pokerhand_client','pokerhand_client']\n",
    "# for j ,settingname in enumerate(dataset_list):\n",
    "#     for i,saplingname in enumerate(sampling_strategy_name_list):\n",
    "#         runFedAvg(samplingName=saplingname,settingName=settingname)\n",
    "#         time.sleep(10)\n",
    "#         # runFedProx(samplingName=saplingname,settingName=settingname)\n",
    "#         # time.sleep(10)\n",
    "# # porx ros 4,rus 3,avg ros 0 rus 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "sampling_strategy_name_list = ['Triplets','no','IncrementalSampling','RandomOverSampler', 'RandomUnderSampler' , 'CondensedNearestNeighbour']#, 'Triplets','no','IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client' ,'pokerhand_client']# ,'pokerhand_client','pokerhand_client','pokerhand_client']\n",
    "for j ,settingname in enumerate(dataset_list):\n",
    "    for i,saplingname in enumerate(sampling_strategy_name_list):\n",
    "        # runFedAvg(samplingName=saplingname,settingName=settingname)\n",
    "        # time.sleep(10)\n",
    "        runFedProx(samplingName=saplingname,settingName=settingname)\n",
    "        gc.collect()\n",
    "        time.sleep(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 定义客户端训练函数\n",
    "def train_model_FedScaFFold_local(input_model, X_train_tensor, y_train_tensor, num_epochs, c_global, c_local, lr=0.01):\n",
    "    model = input_model\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # 初始化控制变量差异\n",
    "    if not c_local:\n",
    "        c_local = [torch.zeros_like(param) for param in model.parameters()]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "\n",
    "        # 计算并加上控制变量差异\n",
    "        c_diff = [c_g - c_l for c_g, c_l in zip(c_global, c_local)]\n",
    "        for param, c_d in zip(model.parameters(), c_diff):\n",
    "            param.grad += c_d.data\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # 计算 y_delta（模型参数的变化量）\n",
    "    y_delta = [param.data - global_weights[name].data for name, param in model.named_parameters()]\n",
    "\n",
    "    # 更新本地控制变量\n",
    "    coef = 1 / (num_epochs * lr)\n",
    "    c_local = [c_l - c_g - coef * delta for c_l, c_g, delta in zip(c_local, c_global, y_delta)]\n",
    "\n",
    "    return model.state_dict(), y_delta, c_local\n",
    "# 定义服务器端聚合函数\n",
    "def scaffold_aggregator(local_params):\n",
    "    global_params = local_params[0][0]\n",
    "    global_c = local_params[0][2]\n",
    "    num_clients = len(local_params)\n",
    "\n",
    "    # 初始化全局y_delta和c_delta\n",
    "    avg_y_delta = [torch.zeros_like(param) for param in global_params.values()]\n",
    "    avg_c_delta = [torch.zeros_like(c) for c in global_c]\n",
    "\n",
    "    for params, y_delta, c_local in local_params:\n",
    "        for i, delta in enumerate(y_delta):\n",
    "            avg_y_delta[i] += delta / num_clients\n",
    "        for i, c_delta in enumerate(c_local):\n",
    "            avg_c_delta[i] += c_delta / num_clients\n",
    "\n",
    "    # 更新全局模型参数\n",
    "    for (name, param), delta in zip(global_params.items(), avg_y_delta):\n",
    "        param.data += delta\n",
    "\n",
    "    # 更新全局控制变量\n",
    "    for i, delta in enumerate(avg_c_delta):\n",
    "        global_c[i] += delta\n",
    "\n",
    "    return global_params, global_c\n",
    "\n",
    "# 运行SCAFFOLD算法\n",
    "def runFedScaFFold(samplingName, settingName):\n",
    "    num_clients = 10\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "    c_global = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "    c_locals = [[] for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    base_path = 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName\n",
    "    sampling_strategy_name = samplingName\n",
    "\n",
    "    algorithm = 'FedScaFFold'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    save_metrics_path = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}_{experiment_times}'\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor, y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round {update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"client {i}\")\n",
    "            X_train_local, y_train_local, prototype_map_r = read_data_return_tensor(\n",
    "                read_data_path, round_id=update, client_id=i,\n",
    "                sampling_strategy=sampling_strategy_name,\n",
    "                prototype_map=client_prototype_map[i]\n",
    "            )\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "\n",
    "            model_weights, y_delta, updated_c_local = train_model_FedScaFFold_local(\n",
    "                clients_models[i], X_train_local, y_train_local, num_epochs=num_epochs,\n",
    "                c_global=c_global, c_local=c_locals[i], lr=0.01\n",
    "            )\n",
    "            local_params_list.append((model_weights, y_delta, updated_c_local))\n",
    "            c_locals[i] = updated_c_local\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]), X_test_tensor, y_test_tensor)\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics, save_folder=save_metrics_path)\n",
    "\n",
    "        # 聚合本地模型参数到全局模型\n",
    "        aggregated_params, updated_c_global = scaffold_aggregator(local_params_list)\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "        c_global = updated_c_global\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm), update, save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm, X_test_tensor, y_test_tensor)\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me, save_folder=save_metrics_path)\n",
    "        print(me)\n",
    "\n",
    "import time\n",
    "sampling_strategy_name_list = ['RandomOverSampler', 'RandomUnderSampler','CondensedNearestNeighbour', 'Triplets','no']# 'IncrementalSampling']# 'CondensedNearestNeighbour']# 'RandomOverSampler', 'RandomUnderSampler'] # , 'CondensedNearestNeighbour', 'Triplets','no','IncrementalSampling']\n",
    "dataset_list = ['pokerhand_client' ,'pokerhand_client' ,'pokerhand_client','pokerhand_client','pokerhand_client']\n",
    "for j ,settingname in enumerate(dataset_list):\n",
    "    for i,saplingname in enumerate(sampling_strategy_name_list):\n",
    "        runFedScaFFold(samplingName=saplingname,settingName=settingname)\n",
    "        time.sleep(10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, MeanShift ,SpectralClustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import hdbscan\n",
    "class IncrementalSampling(object):\n",
    "    def __init__(self,prototype_nums = 30,cluster_strategy = 'kmeans'):\n",
    "        self.incremental_data_map = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.k_value = prototype_nums\n",
    "        # 2024 06 07\n",
    "        self.cluster_strategy = cluster_strategy\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"把当前轮的数据，和类map中的数据进行合并\"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        # 考虑到有新类出现的情况\n",
    "        # 如果current_unique_labels有新label,直接扩充incremental_data_map\n",
    "        # 如果label 是incremental_data_map中已经有的，扩张incremental_data_map对应label中data的长度\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_data_map:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_data_map[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_data_map[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_data_map[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "    def cluster_data(self,data, num_clusters):\n",
    "        # ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "        # kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "        cluster_centers = np.ndarray([])\n",
    "        if self.cluster_strategy == 'kmeans':#  'gmm','meanshift','OPTICS','kmeans++','kmeans'\n",
    "            # KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "            print(\"cluster useing k-means\")\n",
    "            kmeans = KMeans(n_clusters=num_clusters)# ,n_init='auto')\n",
    "            kmeans.fit(data)\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() == 'hdbscan':\n",
    "            print(\"cluster using hdbscan\")\n",
    "            min_cluster_size = 2\n",
    "            hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(data)\n",
    "            labels_hdbscan = hdb.labels_\n",
    "            cluster_centers = np.array([data[labels_hdbscan == i].mean(axis=0) for i in range(len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0))])\n",
    "        elif self.cluster_strategy.lower() == 'kmeans++' or self.cluster_strategy.lower() == 'kmeansplusplus' :\n",
    "            print(\"cluster using kmeans++\")\n",
    "            # kmeans_plus_plus = KMeans(n_clusters=num_clusters, init='k-means++', random_state=0,n_init='auto').fit(data)\n",
    "            kmeans_plus_plus = KMeans(n_clusters=num_clusters, init='k-means++', random_state=0).fit(data)\n",
    "            cluster_centers = kmeans_plus_plus.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='kmedoids':\n",
    "            # 有几个空簇\n",
    "            print(\" cluster kmedoids\")\n",
    "            print('cluster : kmedoids',{num_clusters})\n",
    "            kmedoids = KMedoids(n_clusters=num_clusters-10, random_state=0).fit(data)\n",
    "            labelxx = kmedoids.labels_\n",
    "            cluster_centers = kmedoids.cluster_centers_\n",
    "            print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.upper() =='OPTICS':\n",
    "            print(\"cluster optics\")\n",
    "            min_samples = 2\n",
    "            optics = OPTICS(min_samples=min_samples).fit(data)\n",
    "            labels_optics = optics.labels_\n",
    "            cluster_centers = np.array([data[labels_optics == i].mean(axis=0) for i in range(len(set(labels_optics)) - (1 if -1 in labels_optics else 0))])\n",
    "            print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.lower() =='meanshift':\n",
    "            print(\"cluster mean shift\")\n",
    "            bandwidth = 0.1\n",
    "            mean_shift = MeanShift(bandwidth=bandwidth).fit(data)\n",
    "            cluster_centers = mean_shift.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='gmm': #  'gmm',\n",
    "            print(\"cluster gmm\")\n",
    "            gmm = GaussianMixture(n_components=num_clusters, random_state=0).fit(data)\n",
    "            cluster_centers = gmm.means_  # 高斯混合模型的质心是每个成分的均值\n",
    "        return  cluster_centers\n",
    "    def reCludter(self):\n",
    "        # incremental_data_map的大小进行压缩self.reCluster_instance_nums\n",
    "        new_cluster = {}\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) <= self.k_value:\n",
    "                # 全部保存\n",
    "                new_cluster[label] = data\n",
    "            else:\n",
    "                sampled_nums = self.k_value\n",
    "                clusters = self.cluster_data(data = data, num_clusters=sampled_nums)\n",
    "                new_cluster[label] = clusters\n",
    "        self.incremental_data_map = new_cluster\n",
    "    def compute_sampling_nums(self,combinde_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combinde_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def data_sampling(self,sampling_nums):\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) > sampling_nums:\n",
    "                # 那就是下采样了,不放回采样\n",
    "                sampled_indices = random.sample(range(len(data)), sampling_nums)\n",
    "                sampled_data = [data[i] for i in sampled_indices]\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            elif len(data) == sampling_nums:\n",
    "                # 直接复制\n",
    "                resampling_data.extend(data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            else:\n",
    "                # 上采样,保存原样本\n",
    "                resampling_data.extend(data)\n",
    "                # 随机有放回的找差额部分\n",
    "                sampled_data = random.choices(data, k=(sampling_nums-len(data)))\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,incremental_prototype_map,data,label,sampling_strategy = \"OverSampling\"):\n",
    "        self.incremental_data_map = incremental_prototype_map\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_data_map)\n",
    "        if sampling_strategy.lower() == \"oversampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"downsampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(min_length)\n",
    "        else:\n",
    "            print(\"No sampling measures have been taken\")\n",
    "        self.reCludter()\n",
    "        return resampling_data,resampling_label,self.incremental_data_map\n",
    "\n",
    "class Incremental_sampling2(object):\n",
    "    def __init__(self,save_prototype_nums = 30):\n",
    "        self.save_prototype_nums = save_prototype_nums\n",
    "        self.incremental_prototypes = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.combined_map = {}\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"\n",
    "        上一轮获得的数据原型和这一轮的新数据进行合并\n",
    "        1.首先，按不同类别把数据和原型进行分开\n",
    "        2.判断是不是新出现的类别的数据\n",
    "            2.1、新类别数据,在原型map中直接扩展一个新类的map,{'新类':新类的数据}\n",
    "            2.2、原先类别的数据,在对应类的数据上进行扩展，{'已有类':已有数据+新数据}\n",
    "        \"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_prototypes:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_prototypes[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_prototypes[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_prototypes[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "        self.combined_map = self.incremental_prototypes\n",
    "    def cut_down_nearest_data_eu(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                # 计算成对距离\n",
    "                pairwise_distances = squareform(pdist(data, 'euclidean'))\n",
    "                np.fill_diagonal(pairwise_distances, np.inf)  # 将自身距离设置为无穷大，忽略自身距离\n",
    "\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 找到距离最小的一对\n",
    "                    min_dist_indices = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n",
    "                    # 保留一个样本，删除另一个\n",
    "                    data = np.delete(data, min_dist_indices[1], axis=0)\n",
    "                    # 从距离矩阵中删除对应的行和列\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=0)\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=1)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data_kdtree(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            # self.incremental_prototypes 在这之前已经和新数据合并了\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 建立 KD 树\n",
    "                    kdtree = KDTree(data)\n",
    "                    # 查询每个点的最近邻\n",
    "                    distances, indices = kdtree.query(data, k=2)  # k=2 因为第一个最近邻是点本身\n",
    "\n",
    "                    # 找到最近的两个点\n",
    "                    min_dist_idx = np.argmin(distances[:, 1])  # distances[:, 1] 是每个点的最近邻距离\n",
    "                    nearest_idx = indices[min_dist_idx, 1]\n",
    "\n",
    "                    # 删除其中一个点\n",
    "                    data = np.delete(data, nearest_idx, axis=0)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data_nn(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            使用批量删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 建立 NearestNeighbors 模型\n",
    "                    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(data)\n",
    "                    distances, indices = nbrs.kneighbors(data)\n",
    "\n",
    "                    # 找到距离最小的一对\n",
    "                    min_dist_idx = np.argmin(distances[:, 1])\n",
    "                    nearest_idx = indices[min_dist_idx, 1]\n",
    "\n",
    "                    # 批量删除，尽量减少删除操作次数\n",
    "                    delete_indices = [min_dist_idx, nearest_idx]\n",
    "                    data = np.delete(data, delete_indices, axis=0)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            保留距离最远的样本\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                # 计算成对距离矩阵\n",
    "                pairwise_distances = squareform(pdist(data, 'euclidean'))\n",
    "\n",
    "                # 对距离矩阵进行排序，获取距离最远的样本索引\n",
    "                farthest_indices = np.argsort(-pairwise_distances.sum(axis=1))\n",
    "\n",
    "                # 保留距离最远的前save_prototype_nums个样本\n",
    "                keep_indices = farthest_indices[:self.save_prototype_nums]\n",
    "                data = data[keep_indices]\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def compute_sampling_nums(self,combined_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combined_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def triplet_sampling(self, num_cluster, n_neighbors=5, randomOr=True, len_lim=True):\n",
    "        \"\"\"Triplets 数据采样\"\"\"\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        for label in self.incremental_prototypes:\n",
    "            data_min = self.incremental_prototypes[label]\n",
    "            if len(data_min) < num_cluster:\n",
    "                size = num_cluster - len(data_min)\n",
    "                weight = np.ones(len(data_min))\n",
    "                # 收集多数类样本\n",
    "                data_maj = np.vstack([self.incremental_prototypes[l] for l in self.incremental_prototypes if l != label])\n",
    "                gen_x_c, gen_y_c = self._sample_one(data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim)\n",
    "                gen_x += gen_x_c\n",
    "                gen_y += gen_y_c\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label in self.combined_map: # incremental_prototypes样本太少了，只是为了生成样本用的\n",
    "            data = self.incremental_prototypes[label]\n",
    "            resampling_data.append(data)\n",
    "            resampling_label.extend([label] * len(data))\n",
    "        resampling_data = np.vstack(resampling_data)\n",
    "        resampling_label = np.array(resampling_label)\n",
    "        if len(gen_x) > 0:\n",
    "            gen_x = np.vstack(gen_x)\n",
    "            gen_y = np.array(gen_y)\n",
    "            resampling_data = np.concatenate((resampling_data, gen_x), axis=0)\n",
    "            resampling_label = np.concatenate((resampling_label, gen_y), axis=0)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def _sample_one(self, data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        min_idxs = np.arange(len(data_min))\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(data_maj)\n",
    "        _, indices = nbrs.kneighbors(data_min)\n",
    "\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = data_min[min_idxs[j]]\n",
    "            tp2 = data_maj[indices[j][:5]].mean(axis=0)\n",
    "            tp3_ord = np.random.randint(n_neighbors)\n",
    "            tp3 = data_maj[indices[j][tp3_ord]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(label)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            offset_norm = norm(offset)\n",
    "            if offset_norm == 0:\n",
    "                continue\n",
    "\n",
    "            tp1_tp2_norm = norm(tp1 - tp2)\n",
    "            if tp1_tp2_norm == 0:\n",
    "                continue\n",
    "\n",
    "            if len_lim: offset = offset * min(1, tp1_tp2_norm / offset_norm)\n",
    "            coef = np.random.rand() if randomOr else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(label)\n",
    "\n",
    "        return gen_x, gen_y\n",
    "\n",
    "    def random_sampling(self,num_cluster,sampling_strategy):\n",
    "        \"\"\"\n",
    "        这里需要对数据采样\n",
    "        首先遍历self.incremental_prototypes ,每个类\n",
    "        以及每个类的数据的长度\n",
    "        然后比较每个类的数据的长度和num_cluster之间的差距\n",
    "        差额部分使用triplets的核心算法对齐进行生成样本\n",
    "        \"\"\"\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        if sampling_strategy == \"ros-p\":\n",
    "            # 随机上采样，差额数据从原型数据中随机复制\n",
    "            for label, data in self.combined_map.items():\n",
    "                if len(data) < num_cluster:  # 需要上采样的数据的条件\n",
    "                    prototype_data = self.incremental_prototypes[label]\n",
    "                    sampling_nums = num_cluster - len(data)\n",
    "\n",
    "                    if len(prototype_data) < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(len(prototype_data)), min(needed, len(prototype_data)))\n",
    "                            sampled_data.extend([prototype_data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(len(prototype_data)), sampling_nums)\n",
    "                        sampled_data = [prototype_data[i] for i in sampled_indices]\n",
    "\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        elif sampling_strategy == \"ros-h\" :\n",
    "            # 随机上采样，从混合数据中采样差额数据\n",
    "            for label, data in self.combined_map.items():\n",
    "                data_len = len(data)\n",
    "                if data_len < num_cluster:\n",
    "                    # 那就是下采样了,不放回采样\n",
    "                    sampling_nums = num_cluster-data_len\n",
    "                    if data_len < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(data_len), min(needed, data_len))\n",
    "                            sampled_data.extend([data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(data_len), sampling_nums)\n",
    "                        sampled_data = [data[i] for i in sampled_indices]\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    # 直接复制\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "        self.incremental_prototypes = last_round_prototype\n",
    "        self.data = new_data\n",
    "        self.label = new_data_label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_prototypes)\n",
    "        if sampling_strategy.lower() == \"tpl\":\n",
    "            resampling_data,resampling_label = self.triplet_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"ros-p\":\n",
    "            # rest data copied from prototype\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-p\")\n",
    "            print('use ros p')\n",
    "        elif sampling_strategy.lower() == \"ros-h\":\n",
    "            # rest data copied from hybrid data(combined data)\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-h\")\n",
    "            print('use ros h')\n",
    "        return resampling_data,resampling_label,self.incremental_prototypes\n",
    "\n",
    "def train_model_FedReweight_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses = []\n",
    "    model = copy.deepcopy(input_model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    prox_term = 0.0\n",
    "    for p_i, param in enumerate(model.parameters()):\n",
    "        prox_term += torch.norm(param - global_weights[p_i]) ** 2\n",
    "    print(\"prox_term : \",prox_term.item())\n",
    "    return copy.deepcopy(model.state_dict()), prox_term.item(), losses\n",
    "\n",
    "def train_model_FedAvg_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train_model_FedProx_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    mu = 0.1\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "    # not use deepcopy ,because model as parameter transport in this ,update model also update model\n",
    "    # current_local_model = cmodel\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # FedProx\n",
    "        prox_term = 0.0\n",
    "        for p_i, param in enumerate(model.parameters()):\n",
    "                prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "        loss += prox_term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "    return losses\n",
    "\n",
    "def train_model_FedNova_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau +=len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    return losses, coeff, norm_grad,len(X_train_tensor)\n",
    "# 定义模型参数共享函数\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    # return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "\n",
    "# # 定义模型参数聚合函数\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def save_loss(loss_list,client_id,round_id,save_loss_path):\n",
    "    if not os.path.exists(save_loss_path):\n",
    "        os.makedirs(save_loss_path)\n",
    "    # 构建文件路径\n",
    "    file_path = os.path.join(save_loss_path, f\"client_{client_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 CSV 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 将损失值添加到 DataFrame 中\n",
    "    column_name = f'round_{round_id}'\n",
    "    df[column_name] = loss_list\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def save_model(global_model,round_id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'round_{round_id}_gm.pt')\n",
    "    torch.save(global_model,model_path)\n",
    "\n",
    "def save_metrics(title, rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def inremental_sampling2(prototype_map,raw_X,raw_y,cluster_strategy):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    isap = Incremental_sampling2()\n",
    "    # resampling_data,resampling_label,prototype_map = isap.fit(last_round_prototype=prototype_map,data=raw_X,label=raw_y)\n",
    "    # def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "    def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "        self.incremental_prototypes = last_round_prototype\n",
    "        self.data = new_data\n",
    "        self.label = new_data_label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_prototypes)\n",
    "        if sampling_strategy.lower() == \"tpl\":\n",
    "            resampling_data,resampling_label = self.triplet_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"ros-p\":\n",
    "            # rest data copied from prototype\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-p\")\n",
    "        elif sampling_strategy.lower() == \"ros-h\":\n",
    "            # rest data copied from hybrid data(combined data)\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-h\")\n",
    "        return resampling_data,resampling_label,self.incremental_prototypes\n",
    "    resampling_data,resampling_label,prototype_map = isap.fit(new_data=raw_X,new_data_label=raw_y,last_round_prototype=prototype_map,sampling_strategy=cluster_strategy)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "def inremental_sampling1(prototype_map,raw_X,raw_y,cluster_strategy):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    # 'gmm','meanshift','OPTICS','kmeans++','kmeans'\n",
    "    isap = IncrementalSampling(cluster_strategy =cluster_strategy)\n",
    "    resampling_data,resampling_label,prototype_map = isap.fit(incremental_prototype_map=prototype_map,data=raw_X,label=raw_y)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "def read_data_return_tensor(dataset_path, round_id, client_id, sampling_strategy='no',prototype_map = {},cluster_strategy='kmeans'):\n",
    "    folder_path = os.path.join(dataset_path, f'client_{client_id}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    open_file_path = os.path.join(folder_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :-1].values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    raw_y = raw_y.astype(float)\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    if sampling_strategy == 'IncrementalSampling2':\n",
    "        print(\"using IncrementalSampling 2\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling2(prototype_map,raw_X,raw_y,cluster_strategy=cluster_strategy)\n",
    "    elif sampling_strategy == 'no':\n",
    "        resampling_X = raw_X\n",
    "        resampling_y = raw_y\n",
    "    elif sampling_strategy == 'IncrementalSampling':\n",
    "        print(\"using IncrementalSampling\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling1(prototype_map,raw_X,raw_y,cluster_strategy=cluster_strategy)\n",
    "    # else:\n",
    "    #     resampling_X,resampling_y = data_sampling(raw_X,raw_y,sampling_strategy)\n",
    "    #     resampling_y.astype(float)\n",
    "    resampling_X = np.array(resampling_X)  # 将列表转换为单个NumPy数组\n",
    "    resampling_y = np.array(resampling_y)  # 将列表转换为单个NumPy数组\n",
    "\n",
    "    X_train_tensor = torch.tensor(resampling_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(resampling_y, dtype=torch.long)   # 标签张量\n",
    "    return X_train_tensor,y_train_tensor,prototype_map\n",
    "def read_test_data(test_data_path):\n",
    "    data = pd.read_csv(test_data_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    X = data_shuffled.iloc[:, :-1].values.astype(float)  # 特征\n",
    "    # print(X)\n",
    "    y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    y = y.astype(float)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X, dtype=torch.float32)  # 特征张量\n",
    "    y_test_tensor = torch.tensor(y, dtype=torch.long)   # 标签张量\n",
    "    return X_test_tensor,y_test_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 上诉代码没有共享模型\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import KDTree\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,recall_score, accuracy_score, roc_auc_score,cohen_kappa_score,confusion_matrix,precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.linalg import norm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, MeanShift ,SpectralClustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hdbscan\n",
    "import warnings\n",
    "# 定义MLP模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class IncrementalSampling(object):\n",
    "    def __init__(self,prototype_nums = 30,cluster_strategy = 'kmeans'):\n",
    "        self.incremental_data_map = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.k_value = prototype_nums\n",
    "        # 2024 06 07\n",
    "        self.cluster_strategy = cluster_strategy\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"把当前轮的数据，和类map中的数据进行合并\"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        # 考虑到有新类出现的情况\n",
    "        # 如果current_unique_labels有新label,直接扩充incremental_data_map\n",
    "        # 如果label 是incremental_data_map中已经有的，扩张incremental_data_map对应label中data的长度\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_data_map:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_data_map[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_data_map[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_data_map[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "    def cluster_data(self,data, num_clusters):\n",
    "        # ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm','hdbscan']\n",
    "        # kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "        cluster_centers = np.ndarray([])\n",
    "        if self.cluster_strategy == 'kmeans':\n",
    "            print(\"using  kmeans\")\n",
    "            with warnings.catch_warnings():\n",
    "            # KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "                # kmeans = KMeans(n_clusters=num_clusters,n_init='auto')\n",
    "                kmeans = KMeans(n_clusters=num_clusters)\n",
    "                kmeans.fit(data)\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "        elif self.cluster_strategy == 'spectral':\n",
    "            print(\"using spectual\")\n",
    "            with warnings.catch_warnings():\n",
    "                # 报错，小类样本太少，无法聚类\n",
    "                print('cluster : spectral',{num_clusters})\n",
    "                spectral = SpectralClustering(n_clusters=num_clusters, random_state=0, affinity='nearest_neighbors').fit(data)\n",
    "                labels_spectral = spectral.labels_\n",
    "                cluster_centers = np.array([data[labels_spectral == i].mean(axis=0) for i in range(num_clusters)])\n",
    "                print(len(labels_spectral))\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.lower() == 'hdbscan':\n",
    "            print(\"hdbscan\")\n",
    "            with warnings.catch_warnings():\n",
    "                min_cluster_size = 2\n",
    "                hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(data)\n",
    "                labels_hdbscan = hdb.labels_\n",
    "                cluster_centers = np.array([data[labels_hdbscan == i].mean(axis=0) for i in range(len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0))])\n",
    "        elif self.cluster_strategy.lower() == 'kmeans++' or self.cluster_strategy.lower() == 'kmeansplusplus' :\n",
    "            print(\"using kmeans++\")\n",
    "            with warnings.catch_warnings():\n",
    "                kmeans_plus_plus = KMeans(n_clusters=num_clusters, init='k-means++', random_state=0).fit(data)\n",
    "                cluster_centers = kmeans_plus_plus.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='kmedoids':\n",
    "            print(\"using medorid\")\n",
    "            with warnings.catch_warnings():\n",
    "                # 有几个空簇\n",
    "                print('cluster : spectral',{num_clusters})\n",
    "                kmedoids = KMedoids(n_clusters=num_clusters-10, random_state=0).fit(data)\n",
    "                labelxx = kmedoids.labels_\n",
    "                cluster_centers = kmedoids.cluster_centers_\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.upper() =='OPTICS':\n",
    "            print(\"using optics\")\n",
    "            with warnings.catch_warnings():\n",
    "                min_samples = 2\n",
    "                optics = OPTICS(min_samples=min_samples).fit(data)\n",
    "                labels_optics = optics.labels_\n",
    "                cluster_centers = np.array([data[labels_optics == i].mean(axis=0) for i in range(len(set(labels_optics)) - (1 if -1 in labels_optics else 0))])\n",
    "                print(len(cluster_centers))\n",
    "        elif self.cluster_strategy.lower() =='meanshift':\n",
    "            print(\"using meanshift\")\n",
    "            with warnings.catch_warnings():\n",
    "                bandwidth = 0.1\n",
    "                mean_shift = MeanShift(bandwidth=bandwidth).fit(data)\n",
    "                cluster_centers = mean_shift.cluster_centers_\n",
    "        elif self.cluster_strategy.lower() =='gmm':\n",
    "            print(\"using gmm\")\n",
    "            with warnings.catch_warnings():\n",
    "                gmm = GaussianMixture(n_components=num_clusters, random_state=0).fit(data)\n",
    "                cluster_centers = gmm.means_  # 高斯混合模型的质心是每个成分的均值\n",
    "        return  cluster_centers\n",
    "    def reCludter(self):\n",
    "        # incremental_data_map的大小进行压缩self.reCluster_instance_nums\n",
    "        new_cluster = {}\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) <= self.k_value:\n",
    "                # 全部保存\n",
    "                new_cluster[label] = data\n",
    "            else:\n",
    "                sampled_nums = self.k_value\n",
    "                clusters = self.cluster_data(data = data, num_clusters=sampled_nums)\n",
    "                new_cluster[label] = clusters\n",
    "        self.incremental_data_map = new_cluster\n",
    "    def compute_sampling_nums(self,combinde_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combinde_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def data_sampling(self,sampling_nums):\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label, data in self.incremental_data_map.items():\n",
    "            if len(data) > sampling_nums:\n",
    "                # 那就是下采样了,不放回采样\n",
    "                sampled_indices = random.sample(range(len(data)), sampling_nums)\n",
    "                sampled_data = [data[i] for i in sampled_indices]\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            elif len(data) == sampling_nums:\n",
    "                # 直接复制\n",
    "                resampling_data.extend(data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "            else:\n",
    "                # 上采样,保存原样本\n",
    "                resampling_data.extend(data)\n",
    "                # 随机有放回的找差额部分\n",
    "                sampled_data = random.choices(data, k=(sampling_nums-len(data)))\n",
    "                resampling_data.extend(sampled_data)\n",
    "                resampling_label.extend([label] * sampling_nums)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,incremental_prototype_map,data,label,sampling_strategy = \"OverSampling\"):\n",
    "        self.incremental_data_map = incremental_prototype_map\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_data_map)\n",
    "        if sampling_strategy.lower() == \"oversampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"downsampling\":\n",
    "            resampling_data,resampling_label = self.data_sampling(min_length)\n",
    "        else:\n",
    "            print(\"No sampling measures have been taken\")\n",
    "        self.reCludter()\n",
    "        return resampling_data,resampling_label,self.incremental_data_map\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "class Incremental_sampling2(object):\n",
    "    def __init__(self,save_prototype_nums = 30):\n",
    "        self.save_prototype_nums = save_prototype_nums\n",
    "        self.incremental_prototypes = {}\n",
    "        self.data  = np.array([])\n",
    "        self.label = np.array([])\n",
    "        self.combined_map = {}\n",
    "    def split_current_data_by_class(self):\n",
    "        \"\"\"把当前轮的数据按类别划分\"\"\"\n",
    "        # 获取唯一的类别标签\n",
    "        current_unique_labels = np.unique(self.label)\n",
    "        # 按类别分割当前轮获得的数据\n",
    "        data_by_class = {}  # map {\"label\":[the data belong to the label]}\n",
    "        for current_data_label in current_unique_labels:\n",
    "            indices = np.where(self.label == current_data_label)[0]\n",
    "            data_by_class[current_data_label] = self.data[indices]\n",
    "        return data_by_class, current_unique_labels\n",
    "    def data_combined(self):\n",
    "        \"\"\"\n",
    "        上一轮获得的数据原型和这一轮的新数据进行合并\n",
    "        1.首先，按不同类别把数据和原型进行分开\n",
    "        2.判断是不是新出现的类别的数据\n",
    "            2.1、新类别数据,在原型map中直接扩展一个新类的map,{'新类':新类的数据}\n",
    "            2.2、原先类别的数据,在对应类的数据上进行扩展，{'已有类':已有数据+新数据}\n",
    "        \"\"\"\n",
    "        current_data_map_by_class, current_unique_labels = self.split_current_data_by_class()\n",
    "        for new_data_label in current_unique_labels:\n",
    "            if new_data_label in self.incremental_prototypes:\n",
    "                # 增量map中已经有这个标签的数据了，那就扩充这个数据\n",
    "                self.incremental_prototypes[new_data_label] = np.concatenate(\n",
    "                    (self.incremental_prototypes[new_data_label], current_data_map_by_class[new_data_label])\n",
    "                )\n",
    "            else:\n",
    "                # 如果增量map中没有这个标签的的数据，就扩充增量map\n",
    "                self.incremental_prototypes[new_data_label] = current_data_map_by_class[new_data_label]\n",
    "        self.combined_map = self.incremental_prototypes\n",
    "    def cut_down_nearest_data_eu(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                # 计算成对距离\n",
    "                pairwise_distances = squareform(pdist(data, 'euclidean'))\n",
    "                np.fill_diagonal(pairwise_distances, np.inf)  # 将自身距离设置为无穷大，忽略自身距离\n",
    "\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 找到距离最小的一对\n",
    "                    min_dist_indices = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n",
    "                    # 保留一个样本，删除另一个\n",
    "                    data = np.delete(data, min_dist_indices[1], axis=0)\n",
    "                    # 从距离矩阵中删除对应的行和列\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=0)\n",
    "                    pairwise_distances = np.delete(pairwise_distances, min_dist_indices[1], axis=1)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data_kdtree(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            从离得最近的开始删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            # self.incremental_prototypes 在这之前已经和新数据合并了\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 建立 KD 树\n",
    "                    kdtree = KDTree(data)\n",
    "                    # 查询每个点的最近邻\n",
    "                    distances, indices = kdtree.query(data, k=2)  # k=2 因为第一个最近邻是点本身\n",
    "\n",
    "                    # 找到最近的两个点\n",
    "                    min_dist_idx = np.argmin(distances[:, 1])  # distances[:, 1] 是每个点的最近邻距离\n",
    "                    nearest_idx = indices[min_dist_idx, 1]\n",
    "\n",
    "                    # 删除其中一个点\n",
    "                    data = np.delete(data, nearest_idx, axis=0)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data_nn(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            使用批量删除\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                while len(data) > self.save_prototype_nums:\n",
    "                    # 建立 NearestNeighbors 模型\n",
    "                    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(data)\n",
    "                    distances, indices = nbrs.kneighbors(data)\n",
    "\n",
    "                    # 找到距离最小的一对\n",
    "                    min_dist_idx = np.argmin(distances[:, 1])\n",
    "                    nearest_idx = indices[min_dist_idx, 1]\n",
    "\n",
    "                    # 批量删除，尽量减少删除操作次数\n",
    "                    delete_indices = [min_dist_idx, nearest_idx]\n",
    "                    data = np.delete(data, delete_indices, axis=0)\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def cut_down_nearest_data(self):\n",
    "        \"\"\"\n",
    "        把每个类对应的原型中减去超出save_prototype_nums的样本\n",
    "        判断规则：\n",
    "            保留距离最远的样本\n",
    "        \"\"\"\n",
    "        for label in self.incremental_prototypes:\n",
    "            data = self.incremental_prototypes[label]\n",
    "            if len(data) > self.save_prototype_nums:\n",
    "                # 计算成对距离矩阵\n",
    "                pairwise_distances = squareform(pdist(data, 'euclidean'))\n",
    "\n",
    "                # 对距离矩阵进行排序，获取距离最远的样本索引\n",
    "                farthest_indices = np.argsort(-pairwise_distances.sum(axis=1))\n",
    "\n",
    "                # 保留距离最远的前save_prototype_nums个样本\n",
    "                keep_indices = farthest_indices[:self.save_prototype_nums]\n",
    "                data = data[keep_indices]\n",
    "\n",
    "                # 使用简化后的数据更新原型\n",
    "                self.incremental_prototypes[label] = data\n",
    "    def compute_sampling_nums(self,combined_map):\n",
    "        # initialize\n",
    "        min_length = float('inf')  # 初始化最小长度为正无穷大\n",
    "        max_length = 0  # 初始化最大长度为0\n",
    "        for label, data in combined_map.items():\n",
    "            data_length = len(data)\n",
    "            if data_length < min_length:\n",
    "                min_length = data_length\n",
    "            if data_length > max_length:\n",
    "                max_length = data_length\n",
    "        return min_length, max_length\n",
    "    def triplet_sampling(self, num_cluster, n_neighbors=5, randomOr=True, len_lim=True):\n",
    "        \"\"\"Triplets 数据采样\"\"\"\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        for label in self.incremental_prototypes:\n",
    "            data_min = self.incremental_prototypes[label]\n",
    "            if len(data_min) < num_cluster:\n",
    "                size = num_cluster - len(data_min)\n",
    "                weight = np.ones(len(data_min))\n",
    "                # 收集多数类样本\n",
    "                data_maj = np.vstack([self.incremental_prototypes[l] for l in self.incremental_prototypes if l != label])\n",
    "                gen_x_c, gen_y_c = self._sample_one(data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim)\n",
    "                gen_x += gen_x_c\n",
    "                gen_y += gen_y_c\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        for label in self.combined_map: # incremental_prototypes样本太少了，只是为了生成样本用的\n",
    "            data = self.incremental_prototypes[label]\n",
    "            resampling_data.append(data)\n",
    "            resampling_label.extend([label] * len(data))\n",
    "        resampling_data = np.vstack(resampling_data)\n",
    "        resampling_label = np.array(resampling_label)\n",
    "        if len(gen_x) > 0:\n",
    "            gen_x = np.vstack(gen_x)\n",
    "            gen_y = np.array(gen_y)\n",
    "            resampling_data = np.concatenate((resampling_data, gen_x), axis=0)\n",
    "            resampling_label = np.concatenate((resampling_label, gen_y), axis=0)\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def _sample_one(self, data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        min_idxs = np.arange(len(data_min))\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(data_maj)\n",
    "        _, indices = nbrs.kneighbors(data_min)\n",
    "\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = data_min[min_idxs[j]]\n",
    "            tp2 = data_maj[indices[j][:5]].mean(axis=0)\n",
    "            tp3_ord = np.random.randint(n_neighbors)\n",
    "            tp3 = data_maj[indices[j][tp3_ord]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(label)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            offset_norm = norm(offset)\n",
    "            if offset_norm == 0:\n",
    "                continue\n",
    "\n",
    "            tp1_tp2_norm = norm(tp1 - tp2)\n",
    "            if tp1_tp2_norm == 0:\n",
    "                continue\n",
    "\n",
    "            if len_lim: offset = offset * min(1, tp1_tp2_norm / offset_norm)\n",
    "            coef = np.random.rand() if randomOr else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(label)\n",
    "\n",
    "        return gen_x, gen_y\n",
    "\n",
    "    def random_sampling(self,num_cluster,sampling_strategy):\n",
    "        \"\"\"\n",
    "        这里需要对数据采样\n",
    "        首先遍历self.incremental_prototypes ,每个类\n",
    "        以及每个类的数据的长度\n",
    "        然后比较每个类的数据的长度和num_cluster之间的差距\n",
    "        差额部分使用triplets的核心算法对齐进行生成样本\n",
    "        \"\"\"\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        if sampling_strategy == \"ros-p\":\n",
    "            # 随机上采样，差额数据从原型数据中随机复制\n",
    "            for label, data in self.combined_map.items():\n",
    "                if len(data) < num_cluster:  # 需要上采样的数据的条件\n",
    "                    prototype_data = self.incremental_prototypes[label]\n",
    "                    sampling_nums = num_cluster - len(data)\n",
    "\n",
    "                    if len(prototype_data) < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(len(prototype_data)), min(needed, len(prototype_data)))\n",
    "                            sampled_data.extend([prototype_data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(len(prototype_data)), sampling_nums)\n",
    "                        sampled_data = [prototype_data[i] for i in sampled_indices]\n",
    "\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        elif sampling_strategy == \"ros-h\" :\n",
    "            # 随机上采样，从混合数据中采样差额数据\n",
    "            for label, data in self.combined_map.items():\n",
    "                data_len = len(data)\n",
    "                if data_len < num_cluster:\n",
    "                    # 那就是下采样了,不放回采样\n",
    "                    sampling_nums = num_cluster-data_len\n",
    "                    if data_len < sampling_nums:\n",
    "                        sampled_data = []\n",
    "                        while len(sampled_data) < sampling_nums:\n",
    "                            # 原型中样本还没有sampling_nums多时，直接复制原型中的数据\n",
    "                            needed = sampling_nums - len(sampled_data)\n",
    "                            sampled_indices = random.sample(range(data_len), min(needed, data_len))\n",
    "                            sampled_data.extend([data[i] for i in sampled_indices])\n",
    "                    else:\n",
    "                        sampled_indices = random.sample(range(data_len), sampling_nums)\n",
    "                        sampled_data = [data[i] for i in sampled_indices]\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "                    resampling_data.extend(sampled_data)\n",
    "                    resampling_label.extend([label] * sampling_nums)\n",
    "                elif len(data) == num_cluster:\n",
    "                    # 直接复制\n",
    "                    resampling_data.extend(data)\n",
    "                    resampling_label.extend([label] * len(data))\n",
    "        # 洗牌\n",
    "        combined_data = list(zip(resampling_data, resampling_label))\n",
    "        random.shuffle(combined_data)\n",
    "        resampling_data, resampling_label = zip(*combined_data)\n",
    "        return resampling_data,resampling_label\n",
    "    def fit(self,new_data,new_data_label,last_round_prototype,sampling_strategy='tpl'):\n",
    "        self.incremental_prototypes = last_round_prototype\n",
    "        self.data = new_data\n",
    "        self.label = new_data_label\n",
    "        self.data_combined()\n",
    "        resampling_data = []\n",
    "        resampling_label = []\n",
    "        min_length, max_length =self.compute_sampling_nums(self.incremental_prototypes)\n",
    "        if sampling_strategy.lower() == \"tpl\":\n",
    "            resampling_data,resampling_label = self.triplet_sampling(max_length)\n",
    "        elif sampling_strategy.lower() == \"ros-p\":\n",
    "            # rest data copied from prototype\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-p\")\n",
    "            print('use ros p')\n",
    "        elif sampling_strategy.lower() == \"ros-h\":\n",
    "            # rest data copied from hybrid data(combined data)\n",
    "            resampling_data,resampling_label = self.random_sampling(max_length,sampling_strategy = \"ros-h\")\n",
    "            print('use ros h')\n",
    "        return resampling_data,resampling_label,self.incremental_prototypes\n",
    "class Triplets(object):\n",
    "    def __init__(self, n_neighbors=5, random=True, len_lim=True, **kwargs):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.random = random\n",
    "        self.len_lim = len_lim\n",
    "\n",
    "    def fit_resample(self, x, y):\n",
    "        strategy = self._sample_strategy(y)\n",
    "        self.n_neighbors = max(self.n_neighbors, self.counts.max() // self.counts.min())\n",
    "\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        # 这里的代码平衡状态会报错\n",
    "        for c, size in enumerate(strategy):\n",
    "            if size == 0: continue\n",
    "            weight = self._weights(x, y, c)\n",
    "            gen_x_c, gen_y_c = self._sample_one(x, y, c, size, weight)\n",
    "            gen_x += gen_x_c\n",
    "            gen_y += gen_y_c\n",
    "\n",
    "        # 为了这个方法在平衡状态下不报错，我们特地在这里加了这段代码\n",
    "        # To prevent errors in this method when in a balanced state, we intentionally added this code block\n",
    "        if len(gen_x)==0:\n",
    "            return x,y\n",
    "        gen_x = np.vstack(gen_x)\n",
    "        gen_y = np.array(gen_y)\n",
    "        return np.concatenate((x, gen_x), axis=0), np.concatenate((y, gen_y), axis=0)\n",
    "\n",
    "    def _sample_strategy(self, y):\n",
    "        _, self.counts = np.unique(y, return_counts=True)\n",
    "        return max(self.counts) - self.counts\n",
    "\n",
    "    def _weights(self, x, y, c):\n",
    "        return np.ones(self.counts[c])\n",
    "\n",
    "    def _sample_one(self, x, y, c, size, weight):\n",
    "        gen_x = []\n",
    "        gen_y = []\n",
    "        if size == 0: return gen_x, gen_y\n",
    "\n",
    "        # get the indices of minority and majority instances\n",
    "        min_idxs = np.where(y == c)[0]\n",
    "        maj_idxs = np.where(y != c)[0]\n",
    "\n",
    "        # find nearest majority neighbors for each minority instance\n",
    "        nbrs = NearestNeighbors(n_neighbors=self.n_neighbors).fit(x[maj_idxs])\n",
    "        _, indices = nbrs.kneighbors(x[min_idxs])\n",
    "\n",
    "        # generate synthetic data\n",
    "        for j in np.random.choice(len(min_idxs), size, p=weight / weight.sum()):\n",
    "            tp1 = x[min_idxs[j]]\n",
    "            tp2 = x[maj_idxs[indices[j][:5]]].mean(axis=0)\n",
    "            # tp3_ord = np.random.randint(1, self.n_neighbors)\n",
    "            tp3_ord = np.random.randint(self.n_neighbors)\n",
    "            tp3 = x[maj_idxs[indices[j][tp3_ord]]]\n",
    "            if (tp2 == tp1).all():\n",
    "                gen_x.append(tp1)\n",
    "                gen_y.append(c)\n",
    "                continue\n",
    "\n",
    "            offset = tp3 - tp2\n",
    "            if self.len_lim: offset = offset * min(1, norm(tp1 - tp2) / norm(offset))\n",
    "            coef = np.random.rand() if self.random is True else 1.0\n",
    "            new_x = tp1 + coef * offset\n",
    "            gen_x.append(new_x)\n",
    "            gen_y.append(c)\n",
    "        return gen_x, gen_y\n",
    "\n",
    "def train_model_FedAvg_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "\n",
    "    return losses\n",
    "def train_model_FedProx_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    mu = 0.1\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model# copy.deepcopy(input_model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "    # not use deepcopy ,because model as parameter transport in this ,update model also update model\n",
    "    # current_local_model = cmodel\n",
    "    # model.train()\n",
    "    # for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        # FedProx\n",
    "        prox_term = 0.0\n",
    "        for p_i, param in enumerate(model.parameters()):\n",
    "                prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "        loss += prox_term\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())  # 将每次训练的损失值添加到列表中\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #\n",
    "    #         # fedprox\n",
    "    #         prox_term = 0.0\n",
    "    #         for p_i, param in enumerate(model.parameters()):\n",
    "    #             prox_term += (mu / 2) * torch.norm((param - global_weights[p_i])) ** 2\n",
    "    #         loss += prox_term\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Total Loss: {total_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "def train_model_FedNova_local(input_model,X_train_tensor, y_train_tensor, num_epochs):\n",
    "    # because last round trained global model replaced local model,\n",
    "    # that in this round the first local model is last round global model\n",
    "    losses =[]\n",
    "    model = input_model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    tau = 0\n",
    "    rho = 0.9\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        tau +=len(y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     total_loss = 0.0\n",
    "    #     for step,(x,y) in enumerate(zip(X_train_tensor,y_train_tensor)):\n",
    "    #         # current_local_model.train()\n",
    "    #         # model.train()\n",
    "    #         output = model(x) # current_local_model(x)\n",
    "    #         loss = criterion(output, y)\n",
    "    #         total_loss+=loss.item()\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         tau +=1\n",
    "    #     losses.append(total_loss)\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "    coeff = (tau - rho * (1 - pow(rho, tau)) / (1 - rho)) / (1 - rho)\n",
    "    state_dict = model.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_weights)\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.div(global_weights[key] - state_dict[key], coeff)\n",
    "\n",
    "    return losses, coeff, norm_grad,len(X_train_tensor)\n",
    "# 定义模型参数共享函数\n",
    "def share_params(model):\n",
    "    params = model.state_dict()\n",
    "    # return {k: v.clone().detach().requires_grad_(True) for k, v in params.items()}\n",
    "    return {k: v.clone().detach().requires_grad_(False) for k, v in params.items()}\n",
    "\n",
    "# # 定义模型参数聚合函数\n",
    "def aggregate_params(params_list):\n",
    "    aggregated_params = {}\n",
    "    for key in params_list[0].keys():\n",
    "        # 将参数转换为张量进行处理\n",
    "        params_tensors = [params[key].clone().detach().float() for params in params_list]\n",
    "        # 聚合参数\n",
    "        aggregated_params[key] = sum(params_tensors) / len(params_tensors)\n",
    "    return aggregated_params\n",
    "def test(global_model,X_test,y_test):\n",
    "    # 在全局模型上进行测试\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "        outputs = global_model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # 计算度量\n",
    "        predictions = predicted.numpy() # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        true_labels = y_test_tensor.numpy()  # 将张量转换为 NumPy 数组并去除零维数组\n",
    "        precision = precision_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        precision_micro = precision_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        precision_macro = precision_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        # recall\n",
    "        recalls = recall_score(true_labels,predictions,zero_division=0.0,average=None)\n",
    "        recalls_micro =recall_score(true_labels,predictions,zero_division=0.0,average='micro')\n",
    "        recalls_macro =recall_score(true_labels,predictions,zero_division=0.0,average='macro')\n",
    "        f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        kappa = cohen_kappa_score(true_labels,predictions)\n",
    "        conf_matrix = confusion_matrix(true_labels,predictions)\n",
    "        # 计算所有类别乘积的几何平均值作为 G-mean\n",
    "        g_mean_all= np.power(np.prod(recalls), 1 / len(recalls))\n",
    "        # AUC\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(true_labels)\n",
    "        true_labels_bin = lb.transform(true_labels)\n",
    "        predictions_bin = lb.transform(predictions)\n",
    "        auc = roc_auc_score(true_labels_bin, predictions_bin, average='weighted', multi_class='ovr')\n",
    "        metrics = {\n",
    "            'recall':recalls,\n",
    "            'recall_micro':recalls_micro,\n",
    "            'recall_macro':recalls_macro,\n",
    "            'precision':precision,\n",
    "            'precision_micro':precision_micro,\n",
    "            'precision_macro':precision_macro,\n",
    "            'f1_score':f1_scores,\n",
    "            'g_mean':g_mean_all,\n",
    "            'acc':acc,\n",
    "            'auc':auc,\n",
    "            'kappa':kappa,\n",
    "            'confusion_matrix':conf_matrix\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "def save_loss(loss_list,client_id,round_id,save_loss_path):\n",
    "    if not os.path.exists(save_loss_path):\n",
    "        os.makedirs(save_loss_path)\n",
    "    # 构建文件路径\n",
    "    file_path = os.path.join(save_loss_path, f\"client_{client_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 CSV 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # 将损失值添加到 DataFrame 中\n",
    "    column_name = f'round_{round_id}'\n",
    "    df[column_name] = loss_list\n",
    "\n",
    "    # 将 DataFrame 保存为 CSV 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "def save_model(global_model,round_id,save_model_path):\n",
    "    if not os.path.exists(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    model_path = os.path.join(save_model_path,f'round_{round_id}_gm.pt')\n",
    "    torch.save(global_model,model_path)\n",
    "\n",
    "def save_metrics(title, rounds, metrics, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    file_name = f\"{title}.csv\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "    # print(file_path)\n",
    "    recalls = metrics['recall']\n",
    "    class_nums = len(recalls)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件存在，加载现有的 Excel 文件为 DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 如果文件不存在，直接创建新的 DataFrame\n",
    "        columns = [\n",
    "        'rounds', 'accuracy', 'auc', 'kappa', 'g_mean', 'recall_micro', 'precision_micro',\n",
    "        'recall_macro', 'precision_macro'\n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for i in range(class_nums):  # 动态生成 f1-score 相关列名\n",
    "            columns.append(f'f1_score_{i}')\n",
    "            columns.append(f'recall_{i}')\n",
    "            columns.append(f'precession_{i}')\n",
    "\n",
    "    data = {\n",
    "        'rounds': rounds,\n",
    "        'accuracy': metrics['acc'],\n",
    "        'auc': metrics['auc'],\n",
    "        'kappa': metrics['kappa'],\n",
    "        'g_mean':metrics['g_mean'],\n",
    "        'recall_micro':metrics['recall_micro'],\n",
    "        'precision_micro':metrics['precision_micro'],\n",
    "        'recall_macro':metrics['recall_macro'],\n",
    "        'precision_macro':metrics['precision_macro']\n",
    "    }\n",
    "    # 添加每个类别的 F1-score、G-mean 和 Recall 到 data 中\n",
    "    for i in range(class_nums):  #类别数\n",
    "        data[f'recall_{i}'] = metrics['recall'][i]\n",
    "        data[f'precision_{i}'] = metrics['precision'][i]\n",
    "        data[f'f1_score_{i}'] = metrics['f1_score'][i]\n",
    "    # 创建新行并追加到 DataFrame\n",
    "    new_row = pd.DataFrame(data, index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE,KMeansSMOTE,SVMSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids,RandomUnderSampler,NearMiss,TomekLinks,EditedNearestNeighbours,RepeatedEditedNearestNeighbours,AllKNN,CondensedNearestNeighbour,OneSidedSelection,NeighbourhoodCleaningRule,InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "# SMOTE,ROS,SMOTENC,SMOTEN,ADASYN,BorderlineSMOTE1,BorderlineSMOTE2,KMeansSMOTE,SVMSMOTE\n",
    "# ClusterCentroids,RUS,NearMiss1,NearMiss2,NearMiss2,TomekLinks,ENN,RENN,AllKNN,CNN,OSS,NC,IHT\n",
    "# SMOTEENN,SMOTETomek\n",
    "def data_sampling(raw_X,raw_y,sampling_strategy):\n",
    "    if sampling_strategy.upper() == 'NO' :\n",
    "        return raw_X,raw_y\n",
    "    # overSampling\n",
    "    elif sampling_strategy.upper() == 'SMOTE': # overSampling\n",
    "        \"\"\"\n",
    "            1.对于样本x ,按照欧氏距离找到离其距离最近的K个近邻样本\n",
    "            2.确定采样比例，然后从K个近邻中选择x_n\n",
    "            3.公式 x_new = x + rand(0,1)*(x_n-x)\n",
    "        \"\"\"\n",
    "        smote = SMOTE( random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(raw_X,raw_y)\n",
    "        return X_resampled, y_resampled\n",
    "    elif sampling_strategy == \"RandomOverSampler\" or sampling_strategy.upper()=='ROS': # overSampling\n",
    "        ros = RandomOverSampler(random_state=1)\n",
    "        ros_data,ros_label = ros.fit_resample(raw_X,raw_y)\n",
    "        return ros_data,ros_label\n",
    "    elif sampling_strategy.upper() == 'SMOTENC':    # overSampling\n",
    "        smotenc = SMOTENC(random_state=1,categorical_features=[0])\n",
    "        smotenc_data,smotenc_label = smotenc.fit_resample(raw_X,raw_y)\n",
    "        return smotenc_data,smotenc_label\n",
    "    elif sampling_strategy.upper() == 'SMOTEN': # overSampling\n",
    "        smoten = SMOTEN(random_state=1)\n",
    "        smoten_data,smoten_label = smoten.fit_resample(raw_X,raw_y)\n",
    "        return smoten_data,smoten_label\n",
    "    elif sampling_strategy.upper() =='ADASYN':\n",
    "        adasyn = ADASYN(random_state=1)\n",
    "        adasyn_data,adasyn_label = adasyn.fit_resample(raw_X,raw_y)\n",
    "        return adasyn_data,adasyn_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE1' or sampling_strategy.upper()=='BSMOTE1':\n",
    "        bsmote1 = BorderlineSMOTE(kind='borderline-1',random_state=1)\n",
    "        bsmote1_data,bsmote1_label = bsmote1.fit_resample(raw_X,raw_y)\n",
    "        return bsmote1_data,bsmote1_label\n",
    "    elif sampling_strategy == 'BorderlineSMOTE2'or sampling_strategy.upper()=='BSMOTE2':\n",
    "        bsmote2 = BorderlineSMOTE(kind='borderline-2',random_state=1)\n",
    "        bsmote2_data,bsmote2_label = bsmote2.fit_resample(raw_X,raw_y)\n",
    "        return bsmote2_data,bsmote2_label\n",
    "    elif sampling_strategy == 'KMeansSMOTE' or sampling_strategy.upper() == 'KSMOTE':\n",
    "        kmeanssmote = KMeansSMOTE(random_state=1)\n",
    "        kmeanssmote_data,kmeanssmote_label = kmeanssmote.fit_resample(raw_X,raw_y)\n",
    "        return kmeanssmote_data,kmeanssmote_label\n",
    "    elif sampling_strategy == 'SVMSMOTE':\n",
    "        svmsmote = SVMSMOTE(random_state=1)\n",
    "        svmsmote_data,svmsmote_label = svmsmote.fit_resample(raw_X,raw_y)\n",
    "        return svmsmote_data,svmsmote_label\n",
    "    # downSampling\n",
    "    elif sampling_strategy == 'ClusterCentroids': # down-sampling,generate\n",
    "        clustercentroids = ClusterCentroids(random_state=1)\n",
    "        clustercentroids_data,clustercentroids_label = clustercentroids.fit_resample(raw_X,raw_y)\n",
    "        return clustercentroids_data,clustercentroids_label\n",
    "    elif sampling_strategy=='RandomUnderSampler' or sampling_strategy.upper()=='RUS':\n",
    "        rus = RandomUnderSampler(random_state=1)\n",
    "        rus_data,rus_label = rus.fit_resample(raw_X,raw_y)\n",
    "        return rus_data,rus_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS1':\n",
    "        # 在k个少数类别样本中，选择出与他们-平均距离最近的多数类样本-进行保存\n",
    "        nearmiss1 = NearMiss(version=1)\n",
    "        nearmiss1_data,nearmiss1_label = nearmiss1.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss1_data,nearmiss1_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS2':\n",
    "        # 选择K个距离最远的少数类别样本，然后根据这些样本选出的\"平均距离最近\"的样本进行保存\n",
    "        nearmiss2 = NearMiss(version=2)\n",
    "        nearmiss2_data,nearmiss2_label = nearmiss2.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss2_data,nearmiss2_label\n",
    "    elif sampling_strategy.upper() =='NEARMISS3':\n",
    "        # 1、对于每一个少数类别样本，保留其K个最近邻多数类样本；2、把到K个少数样本平均距离最大的多数类样本保存下来。\n",
    "        nearmiss3 = NearMiss(version=3)\n",
    "        nearmiss3_data,nearmiss3_label = nearmiss3.fit_resample(raw_X,raw_y)\n",
    "        return nearmiss3_data,nearmiss3_label\n",
    "    elif sampling_strategy == 'TomekLinks' or sampling_strategy.upper()=='TOMEK':\n",
    "        # 它需要计算每个样本之间的距离，然后把互为最近邻且类别不同的一对样本拿出来，根据需求的选择将这一对数据进行剔除 or 把多数类样本剔除\n",
    "        tomelink = TomekLinks(sampling_strategy='all')#sampling_strategy='all'表示全部删除，'auto'表示只删除多数类\n",
    "        tomelink_data,tomelink_label = tomelink.fit_resample(raw_X,raw_y)\n",
    "        return tomelink_data,tomelink_label\n",
    "    elif sampling_strategy == 'EditedNearestNeighbours' or sampling_strategy.upper() == 'ENN':\n",
    "        ENN = EditedNearestNeighbours()\n",
    "        ENN_data,ENN_label = ENN.fit_resample(raw_X,raw_y)\n",
    "        return ENN_data,ENN_label\n",
    "    elif sampling_strategy == 'RepeatedEditedNearestNeighbours' or sampling_strategy.upper() == 'RENN':\n",
    "        RENN = RepeatedEditedNearestNeighbours()\n",
    "        RENN_data,RENN_label = RENN.fit_resample(raw_X,raw_y)\n",
    "        return RENN_data,RENN_label\n",
    "    elif sampling_strategy =='AllKNN':\n",
    "        ## ENN的改进版本，和RepeatedEditedNearestNeighbours一样，会多次迭代ENN 算法，不同之处在于，他会每次增加KNN的K值\n",
    "        allknn = AllKNN()\n",
    "        allknn_data,allknn_label = allknn.fit_resample(raw_X,raw_y)\n",
    "        return allknn_data,allknn_label\n",
    "    elif sampling_strategy == 'CondensedNearestNeighbour'or sampling_strategy.upper() == 'CNN':\n",
    "        ## 如果有样本无法和其他多数类样本聚类到一起，那么说明它极有可能是边界的样本，所以将这些样本加入到集合中\n",
    "        CNN = CondensedNearestNeighbour(random_state=1)\n",
    "        CNN_data,CNN_label = CNN.fit_resample(raw_X,raw_y)\n",
    "        return CNN_data,CNN_label\n",
    "    elif sampling_strategy == 'OneSidedSelection' or sampling_strategy.upper() == 'OSS':\n",
    "        # OneSidedSelection = tomekLinks + CondensedNearestNeighbour,先使用自杀式的方式把大类数据中的其他值剔除，然后再使用CondensedNearestNeighbour的下采样\n",
    "        OSS = OneSidedSelection(random_state=1)\n",
    "        OSS_data,OSS_label = OSS.fit_resample(raw_X,raw_y)\n",
    "        return OSS_data,OSS_label\n",
    "    elif sampling_strategy == 'NeighbourhoodCleaningRule'or sampling_strategy.upper() == 'NC':\n",
    "        # 若在大类的K-近邻中，少数类占多数，那就剔除这个多数类别的样本\n",
    "        NC = NeighbourhoodCleaningRule()\n",
    "        NC_data,NC_label = NC.fit_resample(raw_X,raw_y)\n",
    "        return NC_data,NC_label\n",
    "    elif sampling_strategy == 'InstanceHardnessThreshold' or sampling_strategy.upper() == 'IHT':\n",
    "        # 默认算法是随机森林，通过分类算法给出样本阈值来剔除部分样本，（阈值较低的可以剔除）,慢\n",
    "        IHT = InstanceHardnessThreshold(random_state=1)\n",
    "        IHT_data,IHT_label = IHT.fit_resample(raw_X,raw_y)\n",
    "        return IHT_data,IHT_label\n",
    "    # hibird\n",
    "    elif sampling_strategy.upper() =='SMOTEENN':\n",
    "        se = SMOTEENN(random_state=1)\n",
    "        se_data,se_label = se.fit_resample(raw_X,raw_y)\n",
    "        return se_data,se_label\n",
    "    elif sampling_strategy.upper() =='SMOTETOMEK':\n",
    "        st = SMOTETomek(random_state=1)\n",
    "        st_data,st_label = st.fit_resample(raw_X,raw_y)\n",
    "        return st_data,st_label\n",
    "    elif sampling_strategy == 'Triplets':\n",
    "        print(\" Triplets sampling\")\n",
    "        tpl = Triplets()\n",
    "        tpl_data,tpl_label = tpl.fit_resample(raw_X,raw_y)\n",
    "        return tpl_data,tpl_label\n",
    "    else :\n",
    "        print(\"skipped all the sampling strategy,but return the raw data and label\")\n",
    "        return raw_X,raw_y\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "def inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy):\n",
    "    class_nums = len(np.unique(raw_y))\n",
    "    print(\"class nums\" ,class_nums)\n",
    "    if cluster_strategy == 'ros-p' or cluster_strategy == 'ros-h' or cluster_strategy=='tpl':\n",
    "        isap = Incremental_sampling2()\n",
    "        resampling_data,resampling_label,prototype_map = isap.fit(new_data=raw_X,new_data_label=raw_y,last_round_prototype=prototype_map,sampling_strategy=cluster_strategy)\n",
    "    else :\n",
    "        isap = IncrementalSampling(cluster_strategy=cluster_strategy)\n",
    "        resampling_data,resampling_label,prototype_map = isap.fit(incremental_prototype_map=prototype_map,data=raw_X,label=raw_y)\n",
    "    return resampling_data,resampling_label,prototype_map\n",
    "# ['kmeans','spectral','kmeans++','kmedoids','OPTICS','meanshift','gmm']\n",
    "def read_data_return_tensor(dataset_path, round_id, client_id, sampling_strategy='no',prototype_map = {},cluster_strategy='kmeans'):\n",
    "    folder_path = os.path.join(dataset_path, f'client_{client_id}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    open_file_path = os.path.join(folder_path, f'round_{round_id}.csv')\n",
    "    data = pd.read_csv(open_file_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    raw_X = data_shuffled.iloc[:, :-1].values  # 特征\n",
    "    raw_y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    raw_y = raw_y.astype(float)\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    # print(len(raw_X),type(raw_X))\n",
    "    if sampling_strategy == 'IncrementalSampling':\n",
    "        print(\"using IncrementalSampling\")\n",
    "        resampling_X,resampling_y,prototype_map = inremental_sampling(prototype_map,raw_X,raw_y,cluster_strategy=cluster_strategy)\n",
    "    else:\n",
    "        resampling_X,resampling_y = data_sampling(raw_X,raw_y,sampling_strategy)\n",
    "        resampling_y.astype(float)\n",
    "    resampling_X = np.array(resampling_X)  # 将列表转换为单个NumPy数组\n",
    "    resampling_y = np.array(resampling_y)  # 将列表转换为单个NumPy数组\n",
    "\n",
    "    X_train_tensor = torch.tensor(resampling_X, dtype=torch.float32)  # 特征张量\n",
    "    y_train_tensor = torch.tensor(resampling_y, dtype=torch.long)   # 标签张量\n",
    "    return X_train_tensor,y_train_tensor,prototype_map\n",
    "def read_test_data(test_data_path):\n",
    "    data = pd.read_csv(test_data_path, header=0)\n",
    "    # 排除标题行并打乱数据\n",
    "    data_shuffled = shuffle(data)  # 排除标题行并打乱数据\n",
    "\n",
    "    # 提取特征和目标变量\n",
    "    X = data_shuffled.iloc[:, :-1].values.astype(float)  # 特征\n",
    "    # print(X)\n",
    "    y = data_shuffled.iloc[:, -1].values   # 目标变量\n",
    "    y = y.astype(float)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X, dtype=torch.float32)  # 特征张量\n",
    "    y_test_tensor = torch.tensor(y, dtype=torch.long)   # 标签张量\n",
    "    return X_test_tensor,y_test_tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/200], Loss: 0.1024\n",
      "prox_term :  0.002545970492064953\n",
      "Test Accuracy: 96.15%\n",
      "Test Accuracy: 96.17%\n",
      "gme acc: {'recall': array([0.92820013, 0.99464943, 1.        , 1.        , 0.98876404]), 'recall_micro': 0.961692685250508, 'recall_macro': 0.9823227211195524, 'precision': array([0.99985742, 0.99994262, 0.99923983, 0.98644775, 0.13355408]), 'precision_micro': 0.961692685250508, 'precision_macro': 0.823808341286185, 'f1_score': array([0.96269718, 0.997289  , 0.99961977, 0.99317764, 0.23532272]), 'g_mean': 0.9819307617888088, 'acc': 0.961692685250508, 'auc': 0.9806872075277322, 'kappa': 0.9346497151304031, 'confusion_matrix': array([[77139,     4,     0,     0,  5963],\n",
      "       [    0, 69711,     6,    52,   317],\n",
      "       [    0,     0,  7887,     0,     0],\n",
      "       [    0,     0,     0,  3785,     0],\n",
      "       [   11,     0,     0,     0,   968]], dtype=int64)}\n",
      "round186\n",
      "client0\n",
      "using IncrementalSampling\n",
      "class nums 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 352\u001B[0m\n\u001B[0;32m    350\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m    351\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m--> 352\u001B[0m \u001B[43mrunFedReweight\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamplingName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mIncrementalSampling\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msettingName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettingname\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcluster_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_strategy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    353\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()  \u001B[38;5;66;03m# 记录当前迭代的结束时间\u001B[39;00m\n\u001B[0;32m    354\u001B[0m execution_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time  \u001B[38;5;66;03m# 计算当前迭代的执行时间\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 174\u001B[0m, in \u001B[0;36mrunFedReweight\u001B[1;34m(samplingName, settingName, cluster_strategy)\u001B[0m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_clients):\n\u001B[0;32m    173\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 174\u001B[0m     X_train_local, y_train_local, prototype_map_r \u001B[38;5;241m=\u001B[39m \u001B[43mread_data_return_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_data_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mround_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclient_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43msampling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampling_strategy_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprototype_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_prototype_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcluster_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_strategy\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     client_prototype_map[i] \u001B[38;5;241m=\u001B[39m prototype_map_r\n\u001B[0;32m    179\u001B[0m     local_params, prox_term, losses \u001B[38;5;241m=\u001B[39m train_model_FedReweight_local(\n\u001B[0;32m    180\u001B[0m         clients_models[i], X_train_local, y_train_local, num_epochs\u001B[38;5;241m=\u001B[39mnum_epochs\n\u001B[0;32m    181\u001B[0m     )\n",
      "Cell \u001B[1;32mIn[1], line 984\u001B[0m, in \u001B[0;36mread_data_return_tensor\u001B[1;34m(dataset_path, round_id, client_id, sampling_strategy, prototype_map, cluster_strategy)\u001B[0m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sampling_strategy \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIncrementalSampling\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    983\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musing IncrementalSampling\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 984\u001B[0m     resampling_X,resampling_y,prototype_map \u001B[38;5;241m=\u001B[39m \u001B[43minremental_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprototype_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43mraw_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43mraw_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcluster_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_strategy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    986\u001B[0m     resampling_X,resampling_y \u001B[38;5;241m=\u001B[39m data_sampling(raw_X,raw_y,sampling_strategy)\n",
      "Cell \u001B[1;32mIn[1], line 961\u001B[0m, in \u001B[0;36minremental_sampling\u001B[1;34m(prototype_map, raw_X, raw_y, cluster_strategy)\u001B[0m\n\u001B[0;32m    959\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cluster_strategy \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mros-p\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m cluster_strategy \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mros-h\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m cluster_strategy\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpl\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    960\u001B[0m     isap \u001B[38;5;241m=\u001B[39m Incremental_sampling2()\n\u001B[1;32m--> 961\u001B[0m     resampling_data,resampling_label,prototype_map \u001B[38;5;241m=\u001B[39m \u001B[43misap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraw_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnew_data_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraw_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlast_round_prototype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprototype_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43msampling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_strategy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m :\n\u001B[0;32m    963\u001B[0m     isap \u001B[38;5;241m=\u001B[39m IncrementalSampling(cluster_strategy\u001B[38;5;241m=\u001B[39mcluster_strategy)\n",
      "Cell \u001B[1;32mIn[1], line 502\u001B[0m, in \u001B[0;36mIncremental_sampling2.fit\u001B[1;34m(self, new_data, new_data_label, last_round_prototype, sampling_strategy)\u001B[0m\n\u001B[0;32m    500\u001B[0m min_length, max_length \u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_sampling_nums(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mincremental_prototypes)\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sampling_strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtpl\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 502\u001B[0m     resampling_data,resampling_label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtriplet_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m sampling_strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mros-p\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;66;03m# rest data copied from prototype\u001B[39;00m\n\u001B[0;32m    505\u001B[0m     resampling_data,resampling_label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_sampling(max_length,sampling_strategy \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mros-p\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 371\u001B[0m, in \u001B[0;36mIncremental_sampling2.triplet_sampling\u001B[1;34m(self, num_cluster, n_neighbors, randomOr, len_lim)\u001B[0m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;66;03m# 收集多数类样本\u001B[39;00m\n\u001B[0;32m    370\u001B[0m data_maj \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mincremental_prototypes[l] \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mincremental_prototypes \u001B[38;5;28;01mif\u001B[39;00m l \u001B[38;5;241m!=\u001B[39m label])\n\u001B[1;32m--> 371\u001B[0m gen_x_c, gen_y_c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_maj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandomOr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlen_lim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    372\u001B[0m gen_x \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gen_x_c\n\u001B[0;32m    373\u001B[0m gen_y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gen_y_c\n",
      "Cell \u001B[1;32mIn[1], line 399\u001B[0m, in \u001B[0;36mIncremental_sampling2._sample_one\u001B[1;34m(self, data_min, data_maj, label, size, weight, n_neighbors, randomOr, len_lim)\u001B[0m\n\u001B[0;32m    397\u001B[0m min_idxs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(data_min))\n\u001B[0;32m    398\u001B[0m nbrs \u001B[38;5;241m=\u001B[39m NearestNeighbors(n_neighbors\u001B[38;5;241m=\u001B[39mn_neighbors)\u001B[38;5;241m.\u001B[39mfit(data_maj)\n\u001B[1;32m--> 399\u001B[0m _, indices \u001B[38;5;241m=\u001B[39m \u001B[43mnbrs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_min\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mlen\u001B[39m(min_idxs), size, p\u001B[38;5;241m=\u001B[39mweight \u001B[38;5;241m/\u001B[39m weight\u001B[38;5;241m.\u001B[39msum()):\n\u001B[0;32m    402\u001B[0m     tp1 \u001B[38;5;241m=\u001B[39m data_min[min_idxs[j]]\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py:814\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m issparse(X):\n\u001B[0;32m    809\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    810\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m does not work with sparse matrices. Densify the data, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    811\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor set algorithm=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrute\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    812\u001B[0m             \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_method\n\u001B[0;32m    813\u001B[0m         )\n\u001B[1;32m--> 814\u001B[0m     chunked_results \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_tree_query_parallel_helper\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgen_even_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minternal: _fit_method not recognized\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\parallel.py:1085\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1076\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1077\u001B[0m     \u001B[38;5;66;03m# Only set self._iterating to True if at least a batch\u001B[39;00m\n\u001B[0;32m   1078\u001B[0m     \u001B[38;5;66;03m# was dispatched. In particular this covers the edge\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1082\u001B[0m     \u001B[38;5;66;03m# was very quick and its callback already dispatched all the\u001B[39;00m\n\u001B[0;32m   1083\u001B[0m     \u001B[38;5;66;03m# remaining jobs.\u001B[39;00m\n\u001B[0;32m   1084\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1085\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1086\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1088\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 117\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py:623\u001B[0m, in \u001B[0;36m_tree_query_parallel_helper\u001B[1;34m(tree, *args, **kwargs)\u001B[0m\n\u001B[0;32m    617\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_tree_query_parallel_helper\u001B[39m(tree, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    618\u001B[0m     \u001B[38;5;124;03m\"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors.\u001B[39;00m\n\u001B[0;32m    619\u001B[0m \n\u001B[0;32m    620\u001B[0m \u001B[38;5;124;03m    The Cython method tree.query is not directly picklable by cloudpickle\u001B[39;00m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    under PyPy.\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def aggregate_fednova(local_params_list,gm):\n",
    "    # (share_params(clients_models[i]),coeff, norm_grad, data_len) as input\n",
    "    total_data_len = sum(data_len for _, _, _, data_len in local_params_list)\n",
    "    global_model_state = gm.state_dict()\n",
    "    nova_model_state = copy.deepcopy(global_model_state)\n",
    "    # avg_loss = 0\n",
    "    coeff = 0.0\n",
    "    for clientID,(client_model,client_coeff,client_norm_grad,client_local_data_len) in enumerate(local_params_list):\n",
    "        coeff = coeff + client_coeff*client_local_data_len/total_data_len\n",
    "        for key in client_model.state_dict():\n",
    "            if clientID == 0:\n",
    "                nova_model_state[key] = client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "            else:\n",
    "                nova_model_state[key] =nova_model_state[key]+ client_norm_grad[key] * client_local_data_len/total_data_len\n",
    "        # avg_loss = avg_loss + cl\n",
    "    for key in global_model_state:\n",
    "        global_model_state[key] -= coeff*nova_model_state[key]\n",
    "\n",
    "    return global_model_state\n",
    "pp = []\n",
    "def runFedNova(samplingName,settingName,cluster_strategy):\n",
    "    sampling_strategy_name = samplingName\n",
    "    num_clients = 10\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    base_path = 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName  \n",
    "    sampling_strategy_name = samplingName\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "\n",
    "    algorithm = 'FedNova_kvalue30_cluster'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # 在每个客户端训练本地模型\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local,y_train_local,prototype_map_r = read_data_return_tensor(read_data_path,round_id=update,client_id=i,sampling_strategy=sampling_strategy_name,prototype_map=client_prototype_map[i],cluster_strategy=cluster_strategy)\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            # 训练本地模型并获取损失值\n",
    "            losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,y_train_local,num_epochs=num_epochs)\n",
    "            # y_train_tensor = torch.sub(y_train_tensor, 1)\n",
    "            # losses, coeff, norm_grad, data_len = train_model_FedNova_local(clients_models[i],X_train_local,torch.sub(y_train_local, 1),num_epochs=num_epochs)\n",
    "            # save_loss(loss_list=losses,client_id=i,round_id=update,save_loss_path=save_loss_path)\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]),X_test_tensor,y_test_tensor)\n",
    "            local_params_list.append((copy.deepcopy(clients_models[i]),coeff, norm_grad, data_len))\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics,save_folder = save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fednova(local_params_list,gm = copy.deepcopy(global_model))\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm),update,save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm,X_test_tensor,y_test_tensor) # torch.sub(y_train_tensor, 1)\n",
    "        # me = test(gm,X_test_tensor,torch.sub(y_test_tensor, 1))\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me,save_folder = save_metrics_path)\n",
    "        print(\"gme acc:\" ,me)\n",
    "\n",
    "def train_model_FedReweight_local(input_model, X_train_tensor, y_train_tensor, num_epochs):\n",
    "    losses = []\n",
    "    model = copy.deepcopy(input_model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    global_weights = copy.deepcopy(list(model.parameters()))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    prox_term = 0.0\n",
    "    for p_i, param in enumerate(model.parameters()):\n",
    "        prox_term += torch.norm(param - global_weights[p_i]) ** 2\n",
    "    print(\"prox_term : \",prox_term.item())\n",
    "    return copy.deepcopy(model.state_dict()), prox_term.item(), losses\n",
    "def aggregate_fedReweight(local_params_list, prox_terms):\n",
    "    \"\"\"\n",
    "    聚合本地模型参数。\n",
    "\n",
    "    参数:\n",
    "    local_params_list: List[Dict[str, torch.Tensor]]\n",
    "        每个元素是一个本地模型参数字典。\n",
    "    prox_terms: List[float]\n",
    "        每个本地模型的prox_term列表。\n",
    "\n",
    "    返回:\n",
    "    global_model_params: Dict[str, torch.Tensor]\n",
    "        聚合后的全局模型参数字典。\n",
    "    \"\"\"\n",
    "    # 初始化全局模型参数\n",
    "    global_model_params = {key: torch.zeros_like(param) for key, param in local_params_list[0].items()}\n",
    "\n",
    "    # 计算权重的倒数\n",
    "    inv_prox_terms = [1.0 / (term + 1e-10) for term in prox_terms]\n",
    "\n",
    "    # 归一化权重的倒数\n",
    "    total_inv_weight = sum(inv_prox_terms)\n",
    "    normalized_weights = [inv_weight / total_inv_weight for inv_weight in inv_prox_terms]\n",
    "\n",
    "    # 聚合模型参数\n",
    "    for key in global_model_params.keys():\n",
    "        for local_params, weight in zip(local_params_list, normalized_weights):\n",
    "            global_model_params[key] += weight * local_params[key]\n",
    "\n",
    "    return global_model_params\n",
    "def runFedReweight(samplingName, settingName, cluster_strategy):\n",
    "    sampling_strategy_name = samplingName\n",
    "    num_clients = 10\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    base_path = 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName  \n",
    "    sampling_strategy_name = samplingName\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "    # E:/FedStream/metrics/pokerhand_five_Sampling/FedNova_kvalue30/pokerhand_client/IncrementalSampling_epoch200/client_0_metrics.csv\n",
    "    algorithm = 'FedRewighted_kvalue30_cluster'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round{update}\")\n",
    "        local_params_list = []\n",
    "        prox_terms = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"client{i}\")\n",
    "            X_train_local, y_train_local, prototype_map_r = read_data_return_tensor(\n",
    "                read_data_path, round_id=update, client_id=i,\n",
    "                sampling_strategy=sampling_strategy_name, prototype_map=client_prototype_map[i], cluster_strategy=cluster_strategy\n",
    "            )\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "            local_params, prox_term, losses = train_model_FedReweight_local(\n",
    "                clients_models[i], X_train_local, y_train_local, num_epochs=num_epochs\n",
    "            )\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]), X_test_tensor, y_test_tensor)\n",
    "            local_params_list.append(local_params)\n",
    "            prox_terms.append(prox_term)\n",
    "            save_metrics(f\"client_{i}_metrics\", update, local_metrics, save_metrics_path)\n",
    "\n",
    "        aggregated_params = aggregate_fedReweight(local_params_list, prox_terms)\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(gm, update, save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        global_metrics = test(gm, X_test_tensor, y_test_tensor)\n",
    "        save_metrics(\"global_back\", update, global_metrics, save_metrics_path)\n",
    "        print(\"gme acc:\", global_metrics)\n",
    "\n",
    "def train_model_FedScaFFold_local(input_model, X_train_tensor, y_train_tensor, num_epochs, c_global, c_local, lr=0.01):\n",
    "    model = input_model\n",
    "    global_weights = copy.deepcopy(input_model.state_dict())\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # 初始化控制变量差异\n",
    "    if not c_local:\n",
    "        c_local = [torch.zeros_like(param) for param in model.parameters()]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "\n",
    "        # 计算并加上控制变量差异\n",
    "        c_diff = [c_g - c_l for c_g, c_l in zip(c_global, c_local)]\n",
    "        for param, c_d in zip(model.parameters(), c_diff):\n",
    "            param.grad += c_d.data\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # 计算 y_delta（模型参数的变化量）\n",
    "    y_delta = [param.data - global_weights[name].data for name, param in model.named_parameters()]\n",
    "\n",
    "    # 更新本地控制变量\n",
    "    coef = 1 / (num_epochs * lr)\n",
    "    c_local = [c_l - c_g - coef * delta for c_l, c_g, delta in zip(c_local, c_global, y_delta)]\n",
    "\n",
    "    return model.state_dict(), y_delta, c_local\n",
    "# 定义服务器端聚合函数\n",
    "def scaffold_aggregator(local_params):\n",
    "    global_params = local_params[0][0]\n",
    "    global_c = local_params[0][2]\n",
    "    num_clients = len(local_params)\n",
    "\n",
    "    # 初始化全局y_delta和c_delta\n",
    "    avg_y_delta = [torch.zeros_like(param) for param in global_params.values()]\n",
    "    avg_c_delta = [torch.zeros_like(c) for c in global_c]\n",
    "\n",
    "    for params, y_delta, c_local in local_params:\n",
    "        for i, delta in enumerate(y_delta):\n",
    "            avg_y_delta[i] += delta / num_clients\n",
    "        for i, c_delta in enumerate(c_local):\n",
    "            avg_c_delta[i] += c_delta / num_clients\n",
    "\n",
    "    # 更新全局模型参数\n",
    "    for (name, param), delta in zip(global_params.items(), avg_y_delta):\n",
    "        param.data += delta\n",
    "\n",
    "    # 更新全局控制变量\n",
    "    for i, delta in enumerate(avg_c_delta):\n",
    "        global_c[i] += delta\n",
    "\n",
    "    return global_params, global_c\n",
    "\n",
    "# 运行SCAFFOLD算法\n",
    "def runFedScaFFold(samplingName, settingName,cluster_strategy):\n",
    "    sampling_strategy_name = samplingName\n",
    "    num_clients = 10\n",
    "    input_size = 10\n",
    "    hidden_size = 100\n",
    "    output_size = 5\n",
    "    global_model = MLP(input_size, hidden_size, output_size)\n",
    "    client_prototype_map = [{} for _ in range(num_clients)]\n",
    "    clients_models = [MLP(input_size, hidden_size, output_size) for _ in range(num_clients)]\n",
    "\n",
    "    c_global= [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "    c_locals = [[] for _ in range(num_clients)]\n",
    "\n",
    "    \n",
    "    num_epochs = 200\n",
    "    num_global_updates = 195\n",
    "    base_path = 'E:/FedStream/'\n",
    "    dataset_name = 'pokerhand_five'\n",
    "    setting_name = settingName  \n",
    "    sampling_strategy_name = samplingName\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "\n",
    "    algorithm = 'FedScofflod_kvalue30'\n",
    "    experiment_times = 'epoch200'\n",
    "    save_loss_path = f'{base_path}/loss/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_model_path = f'{base_path}/models/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    save_metrics_path  = f'{base_path}/metrics/{dataset_name}_Sampling/{algorithm}/{setting_name}/{sampling_strategy_name}/{cluster_strategy}'\n",
    "    # E:/FedStream/real_data_set/realdataset0427/covertypeNorm/covertypeNorm_client\n",
    "    read_data_path = f'E:/FedStream/real_data_set/realdataset0427/{dataset_name}/{setting_name}/'\n",
    "    # 读取测试集CSV文件并转换为PyTorch张量\n",
    "    test_path = 'E:/FedStream/real_data_set/realdataset0427/pokerhand_five/test.csv'\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "    X_test_tensor,y_test_tensor = read_test_data(test_path)\n",
    "\n",
    "    for update in range(num_global_updates):\n",
    "        print(f\"round {update}\")\n",
    "        local_params_list = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"client {i}\")\n",
    "            X_train_local, y_train_local, prototype_map_r = read_data_return_tensor(\n",
    "                read_data_path, round_id=update, client_id=i,\n",
    "                sampling_strategy=sampling_strategy_name,\n",
    "                prototype_map=client_prototype_map[i],cluster_strategy=cluster_strategy\n",
    "            )\n",
    "            client_prototype_map[i] = prototype_map_r\n",
    "\n",
    "            model_weights, y_delta, updated_c_local = train_model_FedScaFFold_local(\n",
    "                clients_models[i], X_train_local, y_train_local, num_epochs=num_epochs,\n",
    "                c_global=c_global, c_local=c_locals[i], lr=0.01\n",
    "            )\n",
    "            local_params_list.append((model_weights, y_delta, updated_c_local))\n",
    "            c_locals[i] = updated_c_local\n",
    "\n",
    "            local_metrics = test(copy.deepcopy(clients_models[i]), X_test_tensor, y_test_tensor)\n",
    "            save_metrics(title=f\"client_{i}_metrics\", rounds=update, metrics=local_metrics, save_folder=save_metrics_path)\n",
    "\n",
    "        # 聚合本地模型参数到全局模型\n",
    "        aggregated_params, updated_c_global = scaffold_aggregator(local_params_list)\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "        c_global = updated_c_global\n",
    "\n",
    "        # 在每轮结束后发送全局模型参数给客户端\n",
    "        gm = copy.deepcopy(global_model)\n",
    "        save_model(copy.deepcopy(gm), update, save_model_path)\n",
    "        for client_model in clients_models:\n",
    "            client_model.load_state_dict(gm.state_dict())\n",
    "\n",
    "        me = test(gm, X_test_tensor, y_test_tensor)\n",
    "        save_metrics(title=\"global_back\", rounds=update, metrics=me, save_folder=save_metrics_path)\n",
    "        print(me)\n",
    "import time\n",
    "\n",
    "dataset_list =['pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client','pokerhand_client']\n",
    "cluster_strategy_list = ['tpl']#'OPTICS','kmeans','gmm','kmeans++']# ['meanshift','ros-p']\n",
    "\n",
    "total_start_time = time.time()  # 记录整个循环的开始时间\n",
    "ex_time_list = []\n",
    "import gc\n",
    "for j, settingname in enumerate(dataset_list):\n",
    "    for i, cluster_strategy in enumerate(cluster_strategy_list):\n",
    "        start_time = time.time()  # 记录当前迭代的开始时间\n",
    "        runFedScaFFold(samplingName='IncrementalSampling', settingName=settingname,cluster_strategy=cluster_strategy)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        gc.collect()\n",
    "        time.sleep(10)\n",
    "        runFedNova(samplingName='IncrementalSampling', settingName=settingname,cluster_strategy=cluster_strategy)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        gc.collect()\n",
    "        time.sleep(10)\n",
    "        runFedReweight(samplingName='IncrementalSampling', settingName=settingname,cluster_strategy=cluster_strategy)\n",
    "        end_time = time.time()  # 记录当前迭代的结束时间\n",
    "        execution_time = end_time - start_time  # 计算当前迭代的执行时间\n",
    "        ex_time_list.append(execution_time)\n",
    "        gc.collect()\n",
    "        time.sleep(10)\n",
    "\n",
    "total_end_time = time.time()  # 记录整个循环的结束时间\n",
    "total_execution_time = total_end_time - total_start_time  # 计算整个循环的执行时间\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")\n",
    "print(ex_time_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 为了方便地控制数据流中的类不平衡何时以及如何发生，我们根据 FL\\cite{yang2023impact} 中对概念漂移的分类，建立了 9 个实验场景，并在 5 个时间漂移特征（严重性、速度、复发性、频率、可预测性）和 4 个空间漂移特征（覆盖性、同步性、方向性、相关性）上进行了改变。\n",
    "# 表~ref{tab:xx}总结了我们如何控制这些特征以及相应的设置。\n",
    "#\n",
    "# 为了更好地研究这些人工场景，我们根据多数类与少数类的不平衡比 (IR) 设计了六种等级的类不平衡。平衡（50/50，B）表示在客户端之间以及每个客户端的每轮通信中获得的数据是平衡的。低（75/25，L）表示类不平衡程度较低。中（90/10，M）表示中等程度的类不平衡。高（95/5，H）表示不平衡程度高。极高（98/2，VH；2/98，VHR）表示班级失衡程度非常严重。\n",
    "# 我们在两个真实世界的数据集上评估了我们的方法： 电力和扑克牌。电力》是一个二元分类数据集，类比约为 3:4。我们使用了 20% 的数据进行测试，并将剩余的 80% 的数据分为 100 个样本块，按顺序分配给每个客户端，以保留时间信息。在每一轮中，每个客户端数据的类别比例可能会略有不同。\n",
    "#\n",
    "# 扑克手》是一个多类数据集，最初包含 10 个类别。由于每轮通信中的少数类别样本数量不足，我们将数据集重组为扑克手牌（5 类）。类 0、1 和 2 被保留，类 3、6 和 7 被合并为新的第三类，其余类被合并为第四类。这种重组确保每个客户端在每一轮都能从最小的类中获得至少两个样本。\n",
    "#\n",
    "# 我们的实验有 10 个客户端参与了 FL 训练，每个客户端在 100 轮训练中每轮接收 100 个样本。对于 “电力 ”数据集，客户端参与了 35 轮，每轮 100 个样本。在 “扑克+手 ”数据集中，每个客户端每轮收到 338 个样本，涉及 10 个客户端的 5 个类别。我们对每个场景重复训练和评估五次，比较平均性能。\n",
    "#\n",
    "# 我们使用 MLP 算法进行局部和全局模型训练，并使用 FedNova 作为聚合方法。我们将增量采样与现有的四种方法进行了比较： 随机欠采样 (RUS)、压缩近邻 (CNN)、随机过度采样 (ROS) 和三重采样。基线是无采样情况。我们的实验是在配备 16GB 内存的英特尔第七代酷睿 i5 处理器上进行的，使用的是带有 CUDA 11.6 的 PyTorch 1.12.0 和英伟达 GEFORCE GTX 1050Ti。\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
